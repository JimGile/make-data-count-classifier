{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:01:50.979427Z",
     "iopub.status.busy": "2025-07-09T17:01:50.978613Z",
     "iopub.status.idle": "2025-07-09T17:01:50.983002Z",
     "shell.execute_reply": "2025-07-09T17:01:50.982155Z",
     "shell.execute_reply.started": "2025-07-09T17:01:50.979393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install lxml\n",
    "#!pip install PyMuPDF\n",
    "# Uncomment and run this line once to download the model\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!pip install bitsandbytes\n",
    "#!pip install flash_attn\n",
    "#!pip install xformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:02:06.039087Z",
     "iopub.status.busy": "2025-07-09T17:02:06.038447Z",
     "iopub.status.idle": "2025-07-09T17:02:33.963760Z",
     "shell.execute_reply": "2025-07-09T17:02:33.963116Z",
     "shell.execute_reply.started": "2025-07-09T17:02:06.039061Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- 0. Environment Setup & Offline Preparation ---\n",
    "\n",
    "# Standard Imports\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import fitz # PyMuPDF is imported as 'fitz'\n",
    "import lxml.etree as etree\n",
    "from lxml.etree import _Element as Element # Type hinting for lxml.etree.Element\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from transformers.training_args import TrainingArguments\n",
    "import torch\n",
    "import kagglehub\n",
    "import spacy\n",
    "import json\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Dict, Union\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = \"cuda\" if torch and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:02:42.703618Z",
     "iopub.status.busy": "2025-07-09T17:02:42.702757Z",
     "iopub.status.idle": "2025-07-09T17:02:43.821920Z",
     "shell.execute_reply": "2025-07-09T17:02:43.821360Z",
     "shell.execute_reply.started": "2025-07-09T17:02:42.703592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define constants for file paths and model configurations\n",
    "BASE_INPUT_DIR = './kaggle/input/make-data-count-finding-data-references'\n",
    "ARTICLE_TRAIN_DIR = os.path.join(BASE_INPUT_DIR, 'train')\n",
    "ARTICLE_TEST_DIR = os.path.join(BASE_INPUT_DIR, 'test')\n",
    "\n",
    "# Define directories for articles in train and test sets\n",
    "LABELED_TRAINING_DATA_CSV_PATH = os.path.join(BASE_INPUT_DIR, 'train_labels.csv')\n",
    "\n",
    "# Define the base model path\n",
    "#MODEL_PATH = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "MODEL_PATH = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/1.7b\")\n",
    "#MODEL_PATH = kagglehub.model_download(\"google/gemma-3n/transformers/gemma-3n-e2b-it\")\n",
    "#T5GEMMA_PATH = kagglehub.model_download(\"google/t5gemma/transformers/t5gemma-2b-2b-ul2-it\")\n",
    "\n",
    "# Output directory for the fine-tuned model and results\n",
    "BASE_OUTPUT_DIR = \"./kaggle/working\"\n",
    "SUBMISSION_CSV_PATH = os.path.join(BASE_OUTPUT_DIR, \"submission.csv\")\n",
    "\n",
    "# Load a spaCy model (e.g., 'en_core_web_sm')\n",
    "# python -m spacy download en_core_web_sm \n",
    "try:\n",
    "    NLP_SPACY = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    !python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DatasetCitation:\n",
    "    dataset_ids: List[str]\n",
    "    citation_context: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ArticleData:\n",
    "    article_id: str = \"\"\n",
    "    article_doi: str = \"\"\n",
    "    title: str = \"\"\n",
    "    author: str = \"\"\n",
    "    abstract: str = \"\"\n",
    "    # datasets: List[str] = field(default_factory=list)\n",
    "    data_availability: str = \"\"\n",
    "    other_dataset_citations: List[DatasetCitation] = field(default_factory=list)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Custom initialization\n",
    "        if self.article_id and not self.article_doi:\n",
    "            # If article_id is provided but not article_doi, set article_doi\n",
    "            self.article_doi = self.article_id.replace(\"_\", \"/\").lower()\n",
    "\n",
    "    def add_dataset_citation(self, dataset_citation: DatasetCitation):\n",
    "        \"\"\"Adds a dataset citation to the article.\"\"\"\n",
    "        self.other_dataset_citations.append(dataset_citation)\n",
    "        \n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), separators=(',', ':'))\n",
    "\n",
    "    def has_data(self) -> bool:\n",
    "        \"\"\"Returns True if there is any data availability or dataset citation.\"\"\"\n",
    "        return bool(self.data_availability or self.other_dataset_citations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.1234/example/article'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = ArticleData(article_id=\"10.1234_example_article\")\n",
    "article.article_doi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common dataset identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:02:54.212719Z",
     "iopub.status.busy": "2025-07-09T17:02:54.212465Z",
     "iopub.status.idle": "2025-07-09T17:02:54.227291Z",
     "shell.execute_reply": "2025-07-09T17:02:54.226636Z",
     "shell.execute_reply.started": "2025-07-09T17:02:54.212702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- 2. Information Extraction (IE) - Dataset Identification ---\n",
    "NON_STD_UNICODE_DASHES = re.compile(r'[\\u2010\\u2011\\u2012\\u2013\\u2014]')\n",
    "NON_STD_UNICODE_TICKS = re.compile(r'[\\u201c\\u201d]')\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input text by removing non-standard unicode dashes and extra whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Replace all non-standard unicode dashes with '-'\n",
    "    text = text.replace('\\u200b', '').replace('-\\n', '-').replace('_\\n', '_').replace('/\\n', '/')\n",
    "    text = NON_STD_UNICODE_DASHES.sub('-', text)\n",
    "    text = NON_STD_UNICODE_TICKS.sub(\"'\", text)\n",
    "    # Remove extra whitespace\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Regex patterns for common dataset identifiers\n",
    "# DOI_PATTERN = r'10\\.\\d{4,5}/[-._;()/:A-Za-z0-9\\u002D\\u2010\\u2011\\u2012\\u2013\\u2014\\u2015]+'\tDOI_PATTERN\n",
    "# DOI_PATTERN = r'10\\.\\s?\\d{4,5}\\/[-._()<>;\\/:A-Za-z0-9]+\\s?(?:(?![A-Z]+)(?!\\d{1,3}\\.))+[-._()<>;\\/:A-Za-z0-9]+'\n",
    "#DOI_PATTERN = r'\\bhttps://doi.org/10\\.\\d{4,5}\\/[-._\\/:A-Za-z0-9]+'\n",
    "DOI_PATTERN = r'\\b10\\.\\d{4,5}\\/[-._\\/:A-Za-z0-9]+'\n",
    "EPI_PATTERN = r'\\bEPI[-_A-Z0-9]{2,}'\n",
    "SAM_PATTERN = r'\\bSAMN[0-9]{2,}'          # SAMN07159041\n",
    "IPR_PATTERN = r'\\bIPR[0-9]{2,}'\n",
    "CHE_PATTERN = r'\\bCHEMBL[0-9]{2,}'\n",
    "PRJ_PATTERN = r'\\bPRJ[A-Z0-9]{2,}'\n",
    "E_G_PATTERN = r'\\bE-[A-Z]{4}-[0-9]{2,}'   # E-GEOD-19722 or E-PROT-100\n",
    "ENS_PATTERN = r'\\bENS[A-Z]{4}[0-9]{2,}'\n",
    "CVC_PATTERN = r'\\bCVCL_[A-Z0-9]{2,}'\n",
    "EMP_PATTERN = r'\\bEMPIAR-[0-9]{2,}'\n",
    "PXD_PATTERN = r'\\bPXD[0-9]{2,}'\n",
    "HPA_PATTERN = r'\\bHPA[0-9]{2,}'\n",
    "SRR_PATTERN = r'\\bSRR[0-9]{2,}'\n",
    "GSE_PATTERN = r'\\b(GSE|GSM|GDS|GPL)\\d{4,6}\\b' # Example for GEO accession numbers (e.g., GSE12345, GSM12345)\n",
    "GNB_PATTERN = r'\\b[A-Z]{1,2}\\d{5,6}\\b' # GenBank accession numbers (e.g., AB123456, AF000001)\n",
    "CAB_PATTERN = r'\\bCAB[0-9]{2,}'\n",
    "\n",
    "# Combine all patterns into a list\n",
    "DATASET_ID_PATTERNS = [\n",
    "    DOI_PATTERN,\n",
    "    EPI_PATTERN,\n",
    "    SAM_PATTERN,\n",
    "    IPR_PATTERN,\n",
    "    CHE_PATTERN,\n",
    "    PRJ_PATTERN,\n",
    "    E_G_PATTERN,\n",
    "    ENS_PATTERN,\n",
    "    CVC_PATTERN,\n",
    "    EMP_PATTERN,\n",
    "    PXD_PATTERN,\n",
    "    HPA_PATTERN,\n",
    "    SRR_PATTERN,\n",
    "    GSE_PATTERN,\n",
    "    GNB_PATTERN,\n",
    "    CAB_PATTERN,\n",
    "]\n",
    "\n",
    "# Compile all patterns for efficiency\n",
    "COMPILED_DATASET_ID_REGEXES = [re.compile(p) for p in DATASET_ID_PATTERNS]\n",
    "\n",
    "# Data related keywords to look for in the text\n",
    "# These keywords help to ensure that the text is relevant to datasets\n",
    "DATA_RELATED_KEYWORDS = ['data release', 'data associated', 'data availability', 'data access', 'download', 'program data', 'the data', 'dataset', 'database', 'repository', 'data source', 'data access', 'archive', 'arch.', 'digital']\n",
    "\n",
    "def is_text_data_related(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in DATA_RELATED_KEYWORDS)\n",
    "\n",
    "def text_has_dataset_id(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given text contains any dataset identifier.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to check for dataset identifiers.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if any dataset identifier is found, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    occurrences_with_context: list[str] = []\n",
    "    for regex in COMPILED_DATASET_ID_REGEXES:\n",
    "        if regex.search(text):\n",
    "            text_lower = text.lower()\n",
    "            # Check for specific keywords in the text\n",
    "            if any(keyword in text_lower for keyword in DATA_RELATED_KEYWORDS):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def extract_dataset_ids(text: str, context_chars: int = 250) -> list[dict[str, list[str] | str]]:\n",
    "    \"\"\"\n",
    "    Extract dataset identifiers with context from the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to search for dataset identifiers.\n",
    "        context_chars (int): Number of characters to include before and after the match for context.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: A list of extracted dataset identifiers with context.\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    is_small_context = len(text) < context_chars * 2\n",
    "    dataset_ids: list[str] = []\n",
    "    occurrences_with_context: list[dict[str, list[str] | str]] = []\n",
    "    if is_text_data_related(text):\n",
    "        for regex in COMPILED_DATASET_ID_REGEXES:\n",
    "            matches = regex.finditer(text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                dataset_id = text[match.start() : match.end()]\n",
    "                if is_small_context:\n",
    "                    dataset_ids.append(dataset_id)\n",
    "                else:\n",
    "                    citation_context = text[max(0, match.start() - context_chars): match.end() + context_chars ]\n",
    "                    citation_context = citation_context.replace('\\n', '').replace('[', '').replace(']', '')\n",
    "                    citation_context = re.sub(r'\\s+', ' ', citation_context).strip()\n",
    "                    if is_text_data_related(citation_context):\n",
    "                        occurrences_with_context.append({\"dataset_ids\": [dataset_id], \"citation_context\": citation_context})\n",
    "                        #occurrences_with_context.append(\"{\" + f'\"dataset_ids\": {[dataset_id]}, citation_context: \"{citation_context}\"' + \"}\")\n",
    "        if dataset_ids:\n",
    "            occurrences_with_context.append({\"dataset_ids\": dataset_ids, \"citation_context\": text})\n",
    "    \n",
    "    # # If no occurrences found, return an empty string\n",
    "    # # Otherwise, join the occurrences with a specific separator\n",
    "    # if not occurrences_with_context:\n",
    "    #     return \"\"\n",
    "    # return json.dumps(occurrences_with_context, separators=(',', ':'))\n",
    "    return occurrences_with_context\n",
    "\n",
    "# Use NLP to get sentences from the given text\n",
    "\n",
    "def get_sentences_from_text(text: str, nlp=NLP_SPACY) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Replace all non-standard unicode dashes with '-'\n",
    "    text = clean_text(text)\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    doc_spacy = nlp(text)\n",
    "    return \" \".join([sent.text for sent in doc_spacy.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:03:03.719542Z",
     "iopub.status.busy": "2025-07-09T17:03:03.719087Z",
     "iopub.status.idle": "2025-07-09T17:03:03.750581Z",
     "shell.execute_reply": "2025-07-09T17:03:03.749817Z",
     "shell.execute_reply.started": "2025-07-09T17:03:03.719518Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: Seventy-eight pleural effusions. CT scans acquired from The Cancer Imaging Archive 'NSCLC Radiomics' data collection. All expert-vetted segmentations are publicly available in NIfTI format through The Cancer Imaging Archive at https://doi.org/10.7937/tcia.2020.6c7y-gq39\n"
     ]
    }
   ],
   "source": [
    "s = \"Seventy\\u2010eight pleural effusions. CT scans acquired from The Cancer \\nImaging Archive \\u201cNSCLC Radiomics\\u201d data collection. All expert\\u2010vetted segmentations are publicly available in NIfTI format through The Cancer Imaging Archive at  https://doi.org/10.7937/tcia.2020.6c7y\\u2010gq39\"\n",
    "print(f\"Cleaned text: {get_sentences_from_text(s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML Element Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:03:11.196158Z",
     "iopub.status.busy": "2025-07-09T17:03:11.195838Z",
     "iopub.status.idle": "2025-07-09T17:03:11.208160Z",
     "shell.execute_reply": "2025-07-09T17:03:11.207446Z",
     "shell.execute_reply.started": "2025-07-09T17:03:11.196135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_element_text(element: Element | None) -> str:\n",
    "    if element is not None:\n",
    "        # Use itertext() to get all text content from the <p> tag and its descendants\n",
    "        # and join them into a single string.\n",
    "        all_text = \" \".join(element.itertext(tag=None)).replace('\\u200b', '').strip()\n",
    "        return all_text[:2000]\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def extract_next_sibling_text(elements: list[Element] | None, sibling_xpath: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from the next sibling of the given XML element.\n",
    "    \n",
    "    Args:\n",
    "        element (Element | None): The XML element whose next sibling's text is to be extracted.\n",
    "        sibling_xpath (str): The XPath expression to find the next sibling element. (eg. \"following-sibling::passage[1]\")\n",
    "        \n",
    "    Returns:\n",
    "        str: A string containing the text from the next sibling element, or an empty string if no sibling exists.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if the provided elements list is None or empty\n",
    "    if not elements:\n",
    "        return \"\"\n",
    "    \n",
    "    # Assuming there's only one such element, take the first one found\n",
    "    # and find the element immediately following based on the given sibling_xpath.\n",
    "    first_element = elements[0]\n",
    "    sibling_elements = first_element.xpath(sibling_xpath)\n",
    "\n",
    "    if not sibling_elements:\n",
    "        # print(\"DEBUG: No following <passage> element found.\") # Uncomment for debugging\n",
    "        return \"\"\n",
    "    \n",
    "    next_sibling = sibling_elements[0]\n",
    "    if next_sibling is None:\n",
    "        return \"\"\n",
    "    \n",
    "    return extract_element_text(next_sibling)\n",
    "\n",
    "def extract_elements_text(elements: list[Element] | None, sep: str = \" \") -> str:\n",
    "    elements_text = []\n",
    "    if elements is None:\n",
    "        return \"\"\n",
    "    \n",
    "    for element in elements:\n",
    "        text = extract_element_text(element)\n",
    "        if text:\n",
    "            elements_text.append(text)\n",
    "\n",
    "    return sep.join(elements_text).strip()\n",
    "\n",
    "def extract_elements_text_from_xpath_list(root: Element | None, xpath_list: list[str], ns: dict[str, str] | None = None) -> str:\n",
    "    elements_text = \"\"\n",
    "    if root is None or not xpath_list:\n",
    "        return \"\"\n",
    "    \n",
    "    for xpath in xpath_list:\n",
    "        element = root.find(xpath, namespaces=ns)\n",
    "        elements_text += extract_element_text(element)\n",
    "    return elements_text\n",
    "\n",
    "def extract_text_from_elements_within_element(element: Element | None, child_xpaths: list[str] = [], ns: dict[str, str] | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from elements within a given XML element that match the specified tag names.\n",
    "    \n",
    "    Args:\n",
    "        element (Element | None): The XML element to search within.\n",
    "        tag_names (list[str]): A list of tag names to search for.\n",
    "        \n",
    "    Returns:\n",
    "        str: A string containing the extracted text from the matching elements.\n",
    "    \"\"\"\n",
    "    if element is None:\n",
    "        return \"\"\n",
    "    \n",
    "    if not child_xpaths:\n",
    "        # If no child tag names are provided, return the text of the element itself\n",
    "        return extract_element_text(element)\n",
    "    \n",
    "    extracted_text = []\n",
    "    for xpath in child_xpaths:\n",
    "        for child in element.findall(xpath, namespaces=ns):\n",
    "            text = extract_element_text(child)\n",
    "            if text:\n",
    "                extracted_text.append(text)\n",
    "    \n",
    "    return \"|\".join(extracted_text)\n",
    "\n",
    "def extract_data_related_elements_text(elements: list[Element] | None, child_xpaths: list[str] = [], ns: dict[str, str] | None = None) -> list[dict[str, list[str] | str]]:\n",
    "    elements_text = []\n",
    "    if elements is None:\n",
    "        return elements_text\n",
    "    \n",
    "    for element in elements:\n",
    "        text = extract_dataset_ids(extract_text_from_elements_within_element(element, child_xpaths, ns))\n",
    "        if text:\n",
    "            elements_text.extend(text)\n",
    "\n",
    "    return elements_text\n",
    "\n",
    "def extract_data_related_elements_text_from_xpath_list(root: Element | None, xpath_list: list[str], ns: dict[str, str] | None = None) -> list[dict[str, list[str] | str]]:\n",
    "    \"\"\"\n",
    "    Extracts text from elements in the XML tree that match the provided XPath expressions.\n",
    "    \n",
    "    Args:\n",
    "        root (Element | None): The root element of the XML tree.\n",
    "        xpath_list (list[str]): A list of XPath expressions to search for elements.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: A list of extracted text from the matching elements.\n",
    "    \"\"\"\n",
    "    elements_text = []\n",
    "    if root is None or not xpath_list:\n",
    "        return elements_text\n",
    "    \n",
    "    for xpath in xpath_list:\n",
    "        primary_xpath, *child_xpath_text = xpath.split('||')\n",
    "        child_xpaths = child_xpath_text[0].split(',') if child_xpath_text else []\n",
    "        elements = root.findall(primary_xpath, namespaces=ns)\n",
    "        if elements:\n",
    "            elements_text.extend(extract_data_related_elements_text(elements, child_xpaths, ns))\n",
    "    return elements_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF File Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:03:21.067901Z",
     "iopub.status.busy": "2025-07-09T17:03:21.067299Z",
     "iopub.status.idle": "2025-07-09T17:03:21.083320Z",
     "shell.execute_reply": "2025-07-09T17:03:21.082502Z",
     "shell.execute_reply.started": "2025-07-09T17:03:21.067878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_author_names(full_text: str, nlp=NLP_SPACY) -> str:\n",
    "    \"\"\"\n",
    "    Extracts potential author names from the beginning of a research article's text\n",
    "    using spaCy's Named Entity Recognition. It attempts to isolate the author section\n",
    "    and applies heuristics to filter out non-author entities.\n",
    "\n",
    "    Args:\n",
    "        full_text (str): The complete text content of the research article,\n",
    "                         typically extracted from a PDF.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of unique strings, each representing a potential author name,\n",
    "                   sorted alphabetically. Returns an empty list if no authors are found.\n",
    "    \"\"\"\n",
    "    if not full_text or not full_text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    full_text = full_text.replace('1\\n,', ',').replace('1,', ',').replace('\\u2019', \"'\")\n",
    "\n",
    "    # 1. Isolate the potential author section\n",
    "    # Authors are typically at the very beginning, before the abstract or introduction.\n",
    "    # We'll search for common section headers to define the end of the author block.\n",
    "    # Using regex for case-insensitive search and handling various newline/spacing.\n",
    "    header_patterns = [\n",
    "        r\"\\n\\s*Abstract\\s*\\n\",\n",
    "        r\"\\n\\s*Introduction\\s*\\n\",\n",
    "        r\"\\n\\s*Summary\\s*\\n\",\n",
    "        r\"\\n\\s*Keywords\\s*\\n\",\n",
    "        r\"\\n\\s*Graphical Abstract\\s*\\n\",\n",
    "        r\"\\n\\s*1\\.\\s*Introduction\\s*\\n\", # Common for numbered sections\n",
    "        r\"\\n\\s*DOI:\\s*\\n\" # Sometimes DOI appears before abstract\n",
    "    ]\n",
    "\n",
    "    author_section_end_index = len(full_text)\n",
    "    for pattern in header_patterns:\n",
    "        match = re.search(pattern, full_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            # Take text up to the start of the found header\n",
    "            author_section_end_index = min(author_section_end_index, match.start())\n",
    "            break\n",
    "    \n",
    "    # As a fallback or if no header is found early, limit the search to the first\n",
    "    # 2500 characters. This prevents processing the entire document for authors.\n",
    "    author_section_text = full_text[:min(author_section_end_index, 2500)]\n",
    "\n",
    "    if not author_section_text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    # 2. Process the isolated author section with spaCy\n",
    "    doc = nlp(author_section_text)\n",
    "\n",
    "    # 3. Extract PERSON entities and apply initial filtering\n",
    "    potential_authors: list[str] = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            name = ent.text.strip()\n",
    "            # Basic filtering to reduce false positives:\n",
    "            # - Exclude very short strings (e.g., single letters, common conjunctions)\n",
    "            # - Exclude common stop words (e.g., \"The\", \"And\")\n",
    "            # - Exclude all-uppercase strings that might be acronyms (e.g., \"WHO\", \"NASA\")\n",
    "            # - Ensure it contains at least one space (e.g., \"John Doe\") or is a capitalized\n",
    "            #   single word that's longer than 2 characters (e.g., \"Smith\").\n",
    "            if (len(name) > 1 and\n",
    "                name.lower() not in nlp.Defaults.stop_words and\n",
    "                not name.isupper() and\n",
    "                (' ' in name or (name[0].isupper() and len(name) > 2))):\n",
    "                \n",
    "                potential_authors.append(name)\n",
    "\n",
    "    # 4. Apply more advanced heuristics to filter out non-author names\n",
    "    # This step is crucial for accuracy and often requires tuning.\n",
    "    filtered_authors = []\n",
    "    for author in potential_authors:\n",
    "        # Heuristic 1: Filter out names that contain common affiliation keywords.\n",
    "        # This is a simple check; more robust solutions might use spaCy's dependency\n",
    "        # parsing to check if a PERSON entity is part of an ORG entity.\n",
    "        affiliation_keywords = [\"univ\", \"observ\", \"institute\", \"department\", \"center\", \"lab\",\n",
    "                                \"hospital\", \"college\", \"school\", \"inc.\", \"ltd.\", \"company\",\n",
    "                                \"corp.\", \"group\", \"foundation\", \"research\"]\n",
    "        if any(keyword in author.lower() for keyword in affiliation_keywords):\n",
    "            continue # Skip if it looks like an affiliation\n",
    "\n",
    "        # Heuristic 2: Filter out names that contain email patterns or ORCID patterns.\n",
    "        if '@' in author or re.search(r'\\b\\d{4}-\\d{4}-\\d{4}-\\d{3}[\\dX]\\b', author):\n",
    "            continue # Skip if it contains an email or ORCID\n",
    "\n",
    "        # Heuristic 3: Filter out names that are likely just initials or very short.\n",
    "        # This is partially covered by initial filtering, but can be refined.\n",
    "        # E.g., \"J. D.\" might be an author, but \"J.\" alone is unlikely.\n",
    "        if len(author.split()) == 1 and len(author) <= 2 and author.isupper():\n",
    "            continue # Skip single-letter or two-letter uppercase (e.g., \"JD\")\n",
    "\n",
    "        filtered_authors.append(author)\n",
    "\n",
    "    # Convert to list and sort for consistent output\n",
    "    return filtered_authors[0] if filtered_authors else \"\"\n",
    "\n",
    "def extract_pdf_doc_text(pdf_doc: fitz.Document)  -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    \"\"\"\n",
    "    Extracts all text from a PDF document using PyMuPDF.\n",
    "    \n",
    "    Args:\n",
    "        pdf_doc (fitz.Document): The PDF document to extract text from.\n",
    "        \n",
    "    Returns:\n",
    "        str: A JSON string of the article_dict containing specific elements extracted from the PDF.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the article dictionary with empty strings\n",
    "    article_dict = {\n",
    "        'title': '',\n",
    "        'author': '',\n",
    "        'abstract': '',\n",
    "        'data_availability': '',\n",
    "        'other_dataset_citations': []\n",
    "    }\n",
    "\n",
    "    # Initialize variables for text extraction\n",
    "    p1 = None  # Placeholder for the first page text\n",
    "    other_dataset_citations = []\n",
    "    for page in pdf_doc:\n",
    "        # Extract text from the page\n",
    "        textpage = page.get_textpage()\n",
    "        if page.number == 0:\n",
    "            p1_txt = textpage.extractTEXT()\n",
    "            p1 = get_sentences_from_text(p1_txt)\n",
    "            p1 = p1[:int(len(p1)/2)]\n",
    "            article_dict['author'] = extract_author_names(p1_txt, nlp=NLP_SPACY)\n",
    "\n",
    "        # Extract text from all blocks that have an abstract or dataset id's\n",
    "        blocks = textpage.extractBLOCKS()\n",
    "        for block in blocks:\n",
    "            block_text = get_sentences_from_text(block[4])\n",
    "            block_text_lower = block_text.lower()\n",
    "            if page.number == 0 and len(block_text) > 100 and \"abstract\" in block_text_lower:\n",
    "                # Add the abstract block text to the article dictionary\n",
    "                article_dict['abstract'] = block_text\n",
    "            elif \"data availability\" in block_text_lower or \"data accessibility\" in block_text_lower or \"acknowledgments\" in block_text_lower:\n",
    "                # Add the data availability block text to the article dictionary\n",
    "                article_dict['data_availability'] = block_text\n",
    "            else:\n",
    "                context_chars = min(250, len(block_text))  # Use a minimum\n",
    "                dataset_ids_found = extract_dataset_ids(block_text, context_chars)  # Extract dataset IDs from the block text\n",
    "                if dataset_ids_found:\n",
    "                    # print(f\"DEBUG: Found dataset IDs in block: {dataset_ids_found}\")  # Debugging output\n",
    "                    # print(f\"DEBUG: block_text: {block_text}\")  # Debugging output\n",
    "                    if len(article_dict['data_availability']) > 0 and len(article_dict['data_availability']) < 25:\n",
    "                        # If data availability text is only a few characters, append the next block text to it\n",
    "                        # This is a heuristic to ensure that we capture relevant dataset IDs\n",
    "                        article_dict['data_availability'] = block_text\n",
    "                    else:\n",
    "                        # Append the dataset IDs found in the block to the other_dataset_citations\n",
    "                        other_dataset_citations.extend(dataset_ids_found)\n",
    "\n",
    "    article_dict['other_dataset_citations'] = other_dataset_citations if other_dataset_citations else []\n",
    "    \n",
    "    # If an abstract was not found, use the first page text as the abstract\n",
    "    if not article_dict['abstract'] and p1:\n",
    "        article_dict['abstract'] = p1\n",
    "\n",
    "    # Return the article dictionary as a JSON string\n",
    "    return article_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML File Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:03:31.368239Z",
     "iopub.status.busy": "2025-07-09T17:03:31.367681Z",
     "iopub.status.idle": "2025-07-09T17:03:31.381602Z",
     "shell.execute_reply": "2025-07-09T17:03:31.380854Z",
     "shell.execute_reply.started": "2025-07-09T17:03:31.368217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_xml_text_jats(root: Element) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Find the title, abstract, and data availablity info for Journal Archiving and Interchange DTD (JATS)\n",
    "    # The \".//\" ensures it searches anywhere in the document, not just direct children of root.\n",
    "    ns = None  # No namespaces for JATS\n",
    "\n",
    "    xpath_title = \".//article-title\"\n",
    "    xpath_authors_1 = \".//contrib-group/contrib[@contrib-type='author']/name\"\n",
    "    xpath_authors_2 = \".//biblstruct/analytic/author[@role='corresp']/persname\"\n",
    "    author = extract_element_text(root.find(xpath_authors_1, namespaces=ns))\n",
    "    if not author:\n",
    "        author = extract_element_text(root.find(xpath_authors_2, namespaces=ns))\n",
    "    xpath_abstract = \".//abstract\"\n",
    "    xpath_data_avails = [\".//notes[@notes-type='data-availability']\", \".//sec[@sec-type='data-availability']\"]\n",
    "    xpath_citations = [\".//element-citation||.article-title,.source,.pub-id\", \".//mixed-citation\"]  # List of XPath expressions for citations\n",
    "\n",
    "    return {\n",
    "        'title': extract_element_text(root.find(xpath_title, ns)),\n",
    "        'author': author,\n",
    "        'abstract': get_sentences_from_text(extract_element_text(root.find(xpath_abstract, ns))),\n",
    "        'data_availability': extract_elements_text_from_xpath_list(root, xpath_data_avails, ns=ns),\n",
    "        'other_dataset_citations': extract_data_related_elements_text_from_xpath_list(root, xpath_citations, ns=ns),\n",
    "    }\n",
    "\n",
    "def extract_xml_text_tei(root: Element) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Find the title, abstract, and data availability info for Text Encoding Initiative (TEI)\n",
    "    # Set the namespace for TEI\n",
    "    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    xpath_title = \".//tei:title\"\n",
    "    xpath_authors = \".//tei:sourcedesc/tei:biblstruct/tei:analytic/tei:author/tei:persname\"\n",
    "    xpath_abstract = \".//tei:abstract\"\n",
    "    xpath_data_avail = \"\" #\".//tei:biblstruct\"\n",
    "    xpath_citations = [\".//tei:biblstruct||.//tei:title,.//tei:idno,.//tei:notes\"]  # List of XPath expressions for citations\n",
    "        \n",
    "    return {\n",
    "        'title': extract_element_text(root.find(xpath_title, namespaces=ns)),\n",
    "        'author': extract_element_text(root.find(xpath_authors, namespaces=ns)),\n",
    "        'abstract': get_sentences_from_text(extract_element_text(root.find(xpath_abstract, namespaces=ns))),\n",
    "        'data_availability': xpath_data_avail,  # No direct extraction for TEI data_availability\n",
    "        'other_dataset_citations': extract_data_related_elements_text_from_xpath_list(root, xpath_citations, ns=ns),\n",
    "    }\n",
    "\n",
    "def extract_xml_text_wiley(root: Element) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Find the title, abstract, and data availability info for Wiley XML format\n",
    "    # Set the namespace for Wiley\n",
    "    ns = {'ns': 'http://www.wiley.com/namespaces/wiley'}\n",
    "\n",
    "    xpath_title = \".//ns:publicationMeta[@level='part']/ns:titleGroup\"    #<publicationMeta level=\"part\"><titleGroup><title type=\"main\">\n",
    "    xpath_authors = \".//selfCitationGroup/citation[@type='self']/author\"\n",
    "    xpath_abstract = \".//ns:abstract[@type='main']\"  #<abstract type=\"main\"\n",
    "    xpath_data_avail = \".//ns:section[@type='dataAvailability']\"  #<section numbered=\"no\" type=\"dataAvailability\"\n",
    "    xpath_citations = [\".//ns:citation||.//ns:articleTitle,.//ns:journalTitle,.//ns:url\"]  # List of XPath expressions for citations\n",
    "        \n",
    "    return {\n",
    "        'title': extract_elements_text(root.findall(xpath_title, namespaces=ns)),\n",
    "        'author': extract_element_text(root.find(xpath_authors, namespaces=ns)),\n",
    "        'abstract': get_sentences_from_text(extract_element_text(root.find(xpath_abstract, namespaces=ns))),\n",
    "        'data_availability': extract_element_text(root.find(xpath_data_avail, namespaces=ns)),\n",
    "        'other_dataset_citations': extract_data_related_elements_text_from_xpath_list(root, xpath_citations, ns=ns),\n",
    "    }\n",
    "\n",
    "def extract_xml_text_biorxiv(root: Element) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Find the title, abstract, and data availability info for BioRxiv XML format\n",
    "    # Set the namespace for BioRxiv\n",
    "    ns = {'biorxiv': 'http://www.biorxiv.org'}\n",
    "\n",
    "    xpath_title = \".//biorxiv:title\"\n",
    "    xpath_authors = \".//biorxiv:contrib[@contrib-type='author']/biorxiv:name\"\n",
    "    xpath_abstract = \".//biorxiv:abstract\"\n",
    "    xpath_data_avail = \".//biorxiv:sec[@sec-type='data-availability']\"  #<sec sec-type=\"data-availability\"\n",
    "    xpath_citations = [\".//biorxiv:biblio||.//biorxiv:title,.//biorxiv:source,.//biorxiv:pub-id\"]  # List of XPath expressions for citations\n",
    "        \n",
    "    return {\n",
    "        'title': extract_element_text(root.find(xpath_title, namespaces=ns)),\n",
    "        'author': extract_element_text(root.find(xpath_authors, namespaces=ns)),\n",
    "        'abstract': get_sentences_from_text(extract_element_text(root.find(xpath_abstract, namespaces=ns))),\n",
    "        'data_availability': extract_element_text(root.find(xpath_data_avail, namespaces=ns)),\n",
    "        'other_dataset_citations': extract_data_related_elements_text_from_xpath_list(root, xpath_citations, ns=ns),\n",
    "    }\n",
    "\n",
    "def extract_xml_text_bioc(root: Element) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Find the title, abstract, and data availability info for BioC-API XML format\n",
    "    ns = None  # No namespaces for BioC\n",
    "\n",
    "    xpath_title = \"string(.//passage[infon[@key='section_type' and text()='TITLE']]/text)\"\n",
    "    xpath_authors = \"string(.//infon[@key='name_0'] | .//infon[@key='name_1'])\"\n",
    "    xpath_abstract = \"string(.//passage[infon[@key='section_type' and text()='ABSTRACT']]/text)\"\n",
    "    xpath_data_avail = \".//passage[text[text()='DATA ACCESSIBILITY:']]\"\n",
    "    xpath_data_avail_sibling = \"following-sibling::passage[1]\"\n",
    "    xpath_citations = []\n",
    "        \n",
    "    return {\n",
    "        'title': root.xpath(xpath_title, namespaces=ns),\n",
    "        'author': root.xpath(xpath_authors, namespaces=ns).strip().replace('surname:', '').replace(';given-names:', ' '),\n",
    "        'abstract': get_sentences_from_text(root.xpath(xpath_abstract, namespaces=ns)[:2000]),  # Limit to 2000 characters\n",
    "        'data_availability': extract_next_sibling_text(root.xpath(xpath_data_avail, namespaces=ns), xpath_data_avail_sibling),\n",
    "        'other_dataset_citations': xpath_citations,\n",
    "    }\n",
    "\n",
    "def extract_xml_text_taxonx(root: Element) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Find the title, abstract, and data availability info for TaxonX format\n",
    "    ns = None  # No namespaces for Taxonomic Treatment Publishing DTD\n",
    "\n",
    "    xpath_title = \"string(.//article-meta/title-group/article-title)\"\n",
    "    xpath_authors = \"\"\n",
    "    xpath_abstract = \"string(.//article-meta/abstract)\"\n",
    "    xpath_data_avail = \"\"\n",
    "    xpath_citations = []\n",
    "        \n",
    "    return {\n",
    "        'title': root.xpath(xpath_title, namespaces=ns),\n",
    "        'author': xpath_authors,\n",
    "        'abstract': get_sentences_from_text(root.xpath(xpath_abstract, namespaces=ns)[:2000]),  # Limit to 2000 characters\n",
    "        'data_availability': xpath_data_avail,  # No direct extraction for TaxonX data_availability\n",
    "        'other_dataset_citations': xpath_citations,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:04:16.011843Z",
     "iopub.status.busy": "2025-07-09T17:04:16.011561Z",
     "iopub.status.idle": "2025-07-09T17:04:16.033082Z",
     "shell.execute_reply": "2025-07-09T17:04:16.032458Z",
     "shell.execute_reply.started": "2025-07-09T17:04:16.011824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Dictionary mapping XML types to their respective extraction functions\n",
    "XML_TYPE_EXTRACTORS = {\n",
    "    'jats': extract_xml_text_jats,\n",
    "    'tei': extract_xml_text_tei,\n",
    "    'wiley': extract_xml_text_wiley,\n",
    "    'bioc': extract_xml_text_bioc,\n",
    "    'taxonx': extract_xml_text_taxonx,\n",
    "}\n",
    "\n",
    "# --- Data Loading ---\n",
    "def get_file_extension(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the file extension of the given file path.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "        \n",
    "    Returns:\n",
    "        str: The file extension, or an empty string if no extension is found.\n",
    "    \"\"\"\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "    return ext.lower() if ext else \"\"\n",
    "\n",
    "def read_first_line_of_xml(file_path: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Reads and returns the first line of an XML file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "\n",
    "    Returns:\n",
    "        str | None: The first line of the file, stripped of leading/trailing whitespace,\n",
    "                    or None if the file cannot be read or is empty.\n",
    "    \"\"\"\n",
    "    if not file_path and not os.path.exists(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline().replace('<?xml version=\"1.0\" encoding=\"UTF-8\"?>', '').replace('<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>', '').strip()\n",
    "            # If the first line is empty, read the next line\n",
    "            if not first_line:\n",
    "                first_line = f.readline()\n",
    "            return first_line.strip()[:90] if first_line else None\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
    "                first_line = f.readline().replace('<?xml version=\"1.0\" encoding=\"UTF-8\"?>', '').replace('<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>', '').strip()\n",
    "                if not first_line:\n",
    "                    first_line = f.readline()\n",
    "                return first_line.strip()[:90] if first_line else None\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file '{file_path}': {e}\")\n",
    "        return None\n",
    "    \n",
    "def identify_xml_type(first_line: str) -> str:\n",
    "    \"\"\"\n",
    "    Identifies the XML type based on the first line of the XML file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "        \n",
    "    Returns:\n",
    "        str: The identified XML type ('jats', 'tei', 'wiley', 'bioc', or 'unknown').\n",
    "    \"\"\"\n",
    "    if not first_line:\n",
    "        return \"unknown\"\n",
    "    first_line_lower = first_line.lower()\n",
    "    # Check for specific patterns in the first line\n",
    "    if 'journal archiving and interchange dtd' in first_line_lower:\n",
    "        return \"jats\"\n",
    "    elif 'xmlns=\"http://www.tei-c.org/ns/1.0\"' in first_line_lower:\n",
    "        return \"tei\"\n",
    "    elif 'xmlns=\"http://www.wiley.com/namespaces/wiley\"' in first_line_lower:\n",
    "        return \"wiley\"\n",
    "    elif 'bioc.dtd' in first_line_lower or 'bioc-api' in first_line_lower:\n",
    "        return \"bioc\"\n",
    "    elif 'taxonomic treatment publishing dtd' in first_line_lower:\n",
    "        return \"taxonx\"\n",
    "    \n",
    "    return \"unknown\"    \n",
    "\n",
    "def get_xml_type(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Determines the XML type of a file based on its first line.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "        \n",
    "    Returns:\n",
    "        str: The identified XML type ('jats', 'tei', 'wiley', 'bioc', 'taxonx', or 'unknown').\n",
    "    \"\"\"\n",
    "    first_line = \"\"\n",
    "    if \".xml\" == get_file_extension(file_path):\n",
    "        # If the file is an XML file, read the first line and identify the type\n",
    "        first_line = read_first_line_of_xml(file_path)\n",
    "    return identify_xml_type(first_line) if first_line else \"unknown\"\n",
    "\n",
    "def load_file_paths(dataset_type_dir: str) -> pd.DataFrame: \n",
    "    pdf_path = os.path.join(dataset_type_dir, 'PDF')\n",
    "    xml_path = os.path.join(dataset_type_dir, 'XML')\n",
    "    dataset_type = os.path.basename(dataset_type_dir)\n",
    "    pdf_files = [f for f in os.listdir(pdf_path) if f.endswith('.pdf')]\n",
    "    xml_files = [f for f in os.listdir(xml_path) if f.endswith('.xml')]\n",
    "    df_pdf = pd.DataFrame({\n",
    "        'article_id': [f.replace('.pdf', '') for f in pdf_files],\n",
    "        'pdf_file_path': [os.path.join(pdf_path, f) for f in pdf_files]\n",
    "    })\n",
    "    df_xml = pd.DataFrame({\n",
    "        'article_id': [f.replace('.xml', '') for f in xml_files],\n",
    "        'xml_file_path': [os.path.join(xml_path, f) for f in xml_files]\n",
    "    })\n",
    "    merge_df = pd.merge(df_pdf, df_xml, on='article_id', how='outer', suffixes=('_pdf', '_xml'), validate=\"one_to_many\")\n",
    "    merge_df['dataset_type'] = dataset_type\n",
    "    return merge_df\n",
    "\n",
    "def extract_pdf_text(file_path: str, xml_type: str | None = None) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    \"\"\"Extracts all text from a PDF file using PyMuPDF.\"\"\"\n",
    "    article_dict = {}\n",
    "    if file_path and os.path.exists(file_path):\n",
    "        try:\n",
    "            with fitz.open(file_path) as doc:\n",
    "                article_dict = extract_pdf_doc_text(doc)  # Extract text from the PDF document\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"PDF file not found: {file_path}\")\n",
    "    \n",
    "    return article_dict\n",
    "\n",
    "def extract_xml_text(file_path: str, xml_type: str) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    \"\"\"Reads and extracts text from an XML file based on the specified XML type.\n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "        xml_type (str): The type of XML format (e.g., 'jats', 'tei', 'wiley', 'bioc', 'taxonx').\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted text from the XML file.\n",
    "    \"\"\"\n",
    "    # Initialize the article dictionary\n",
    "    article_dict = {}\n",
    "    if file_path and os.path.exists(file_path):\n",
    "        # Disable external entity resolution for security\n",
    "        parser = etree.XMLParser(resolve_entities=False, no_network=True)\n",
    "        try:\n",
    "            tree = etree.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "            # Use the appropriate extraction function based on the xml_type\n",
    "            extract_function = XML_TYPE_EXTRACTORS.get(xml_type, extract_xml_text_jats)  \n",
    "            article_dict = extract_function(root)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading XML {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"XML file not found: {file_path}\")    \n",
    "    return article_dict\n",
    "\n",
    "def dedupe_other_dataset_citations(article_dict: dict[str, str | list[dict[str, list[str] | str]]]) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    \"\"\"\n",
    "    Deduplicates dataset IDs in the article dictionary.\n",
    "    \n",
    "    Args:\n",
    "        article_dict (dict): The article dictionary containing dataset IDs.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The updated article dictionary with deduplicated dataset IDs.\n",
    "    \"\"\"\n",
    "    unique_dataset_ids = set()\n",
    "    unique_dataset_id_citations = []\n",
    "    if 'other_dataset_citations' in article_dict and isinstance(article_dict['other_dataset_citations'], list):\n",
    "        dataset_citations = article_dict['other_dataset_citations']\n",
    "        for citation_dict in dataset_citations:\n",
    "            dataset_ids = citation_dict['dataset_ids']\n",
    "            if isinstance(dataset_ids, list):\n",
    "                if len(dataset_ids) == 1:\n",
    "                    dataset_id = dataset_ids[0]\n",
    "                    if dataset_id not in article_dict['data_availability'] and dataset_id not in unique_dataset_ids:\n",
    "                        unique_dataset_ids.add(dataset_id)\n",
    "                        unique_dataset_id_citations.append(citation_dict)\n",
    "                else:\n",
    "                    unique_dataset_id_citations.append(citation_dict)\n",
    "        article_dict['other_dataset_citations'] = unique_dataset_id_citations\n",
    "    \n",
    "    return article_dict \n",
    "\n",
    "def process_unsupported_file(file_path: str, xml_type: str | None = None) -> dict:\n",
    "    return {\n",
    "        'title': f\"Unsupported file type for: {file_path}\",\n",
    "        'data_availability': \"\",\n",
    "        'other_dataset_citations': [],\n",
    "    }\n",
    "\n",
    "# Dictionary mapping file extensions to loading functions\n",
    "FILE_EXTRACTORS = {\n",
    "    '.xml': extract_xml_text,\n",
    "    '.pdf': extract_pdf_text,\n",
    "}\n",
    "\n",
    "def extract_article_dict(file_path: str) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Get the file extension (e.g., '.xml', '.pdf')\n",
    "    file_extension = get_file_extension(file_path)\n",
    "\n",
    "    # Get the XML type if the file is an XML file\n",
    "    xml_type = get_xml_type(file_path)\n",
    "\n",
    "    # Get the appropriate function from the dictionary,\n",
    "    # or fall back to a default 'unsupported' function if not found.\n",
    "    extract_function = FILE_EXTRACTORS.get(file_extension, process_unsupported_file)\n",
    "\n",
    "    # Call the selected function\n",
    "    article_dict = extract_function(file_path, xml_type=xml_type)\n",
    "    article_dict = dedupe_other_dataset_citations(article_dict)\n",
    "    \n",
    "    return article_dict\n",
    "\n",
    "def extract_article_text(arg: str| dict) -> str:\n",
    "    \"\"\"\n",
    "    Overloaded function: Accepts either a file_path (str) or an article_dict (dict).\n",
    "    Returns the article text as a JSON string.\n",
    "    \"\"\"\n",
    "    if isinstance(arg, dict):\n",
    "        # If it's already a dict, just serialize it\n",
    "        return json.dumps(arg, separators=(',', ':'))\n",
    "    elif isinstance(arg, str):\n",
    "        # If it's a file path, process as before\n",
    "        file_path = arg\n",
    "        file_extension = get_file_extension(file_path)\n",
    "        xml_type = get_xml_type(file_path)\n",
    "        extract_function = FILE_EXTRACTORS.get(file_extension, process_unsupported_file)\n",
    "        article_dict = extract_function(file_path, xml_type=xml_type)\n",
    "        article_dict = dedupe_other_dataset_citations(article_dict)\n",
    "        text_content = json.dumps(article_dict, separators=(',', ':'))\n",
    "        print(f\"Extracted text from {file_path}. Length: {len(text_content)} characters, xml_type: {xml_type}\")\n",
    "        return text_content\n",
    "    else:\n",
    "        raise TypeError(\"extract_article_text expects a file path (str) or article_dict (dict)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:04:41.462794Z",
     "iopub.status.busy": "2025-07-09T17:04:41.462490Z",
     "iopub.status.idle": "2025-07-09T17:04:41.547621Z",
     "shell.execute_reply": "2025-07-09T17:04:41.547101Z",
     "shell.execute_reply.started": "2025-07-09T17:04:41.462771Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2704"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'{\"title\":\"PleThora: Pleural effusion and thoracic cavity segmentations in diseased lungs for benchmarking chest CT processing pipelines\",\"author\":\"Kiser Kendall J.\",\"abstract\":\"This manuscript describes a dataset of thoracic cavity segmentations and discrete pleural effusion segmentations we have annotated on 402 computed tomography (CT) scans acquired from patients with non-small cell lung cancer. The segmentation of these anatomic regions precedes fundamental tasks in image analysis pipelines such as lung structure segmentation, lesion detection, and radiomics feature extraction. Bilateral thoracic cavity volumes and pleural effusion volumes were manually segmented on CT scans acquired from The Cancer Imaging Archive \\'NSCLC Radiomics\\' data collection. Four hundred and two thoracic segmentations were first generated automatically by a U-Net based algorithm trained on chest CTs without cancer, manually corrected by a medical student to include the complete thoracic cavity (normal, pathologic, and atelectatic lung parenchyma, lung hilum, pleural effusion, fibrosis, nodules, tumor, and other anatomic anomalies), and revised by a radiation oncologist or a radiologist. Seventy-eight pleural effusions were manually segmented by a medical student and revised by a radiologist or radiation oncologist. Interobserver agreement between the radiation oncologist and radiologist corrections was acceptable. All expert-vetted segmentations are publicly available in NIfTI format through The Cancer Imaging Archive at https://doi.org/10.7937/tcia.2020.6c7y-gq39 . Tabular data detailing clinical and technical metadata linked to segmentation cases are also available. Thoracic cavity segmentations will be valuable for developing image analysis pipelines on pathologic lungs - where current automated algorithms struggle most. In conjunction with gross tumor volume segmentations already available from \\'NSCLC Radiomics,\\' pleural effusion segmentations may be valuable for investigating radiomics profile differences between effusion and primary tumor or training algorithms to discriminate between them.\",\"data_availability\":\"\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.7937/K9/TCIA.2015.PF0M9REI\"],\"citation_context\":\"Aerts HJWL , Wee L , Rios Velazquez E , et al. Data from NSCLC-Radiomics [Dataset] . In: The Cancer Imaging Archive ; 2019 10.7937/K9/TCIA.2015.PF0M9REI\"},{\"dataset_ids\":[\"10.7937/tcia.2020.6c7y-gq39\"],\"citation_context\":\"Kiser KJ , Ahmed S , Stieb SM , et al. Data from the thoracic volume and pleural effusion segmentations in diseased lungs for benchmarking chest CT processing pipelines [Dataset] . In: The Cancer Imaging Archive ; 2020 10.7937/tcia.2020.6c7y-gq39\"}]}'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test extracting text from various PDF and XML files\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1002_2017jc013030.pdf')\n",
    "pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1002_mp.14424.xml')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1007_s00259-022-06053-8.xml')    # jats\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1007_s00382-022-06361-7.xml')    # tei\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1111_1365-2435.13431.xml')       # wiley\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1111_mec.16977.xml')             # bioc\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.3897_zoologia.35.e23481.xml')    # taxonx\n",
    "article_text = extract_article_text(extract_article_dict(pdf_file_path))\n",
    "display(len(article_text))\n",
    "article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:04:55.042360Z",
     "iopub.status.busy": "2025-07-09T17:04:55.041531Z",
     "iopub.status.idle": "2025-07-09T17:04:55.219521Z",
     "shell.execute_reply": "2025-07-09T17:04:55.218957Z",
     "shell.execute_reply.started": "2025-07-09T17:04:55.042331Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Base dir: ./kaggle/input/make-data-count-finding-data-references\\\\test'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files paths shape: (30, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "834b6d91-dd07-4e90-9c99-bb25ba046def",
       "rows": [
        [
         "24",
         "10.1002_ejoc.202000916",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejoc.202000916.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ejoc.202000916.xml",
         "test",
         "jats"
        ],
        [
         "18",
         "10.1002_ece3.961",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.961.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.961.xml",
         "test",
         "jats"
        ],
        [
         "13",
         "10.1002_ece3.5260",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.5260.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.5260.xml",
         "test",
         "jats"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>xml_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.1002_ejoc.202000916</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>jats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.1002_ece3.961</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>jats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.1002_ece3.5260</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>jats</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                article_id                                      pdf_file_path  \\\n",
       "24  10.1002_ejoc.202000916  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "18        10.1002_ece3.961  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "13       10.1002_ece3.5260  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                        xml_file_path dataset_type xml_type  \n",
       "24  ./kaggle/input/make-data-count-finding-data-re...         test     jats  \n",
       "18  ./kaggle/input/make-data-count-finding-data-re...         test     jats  \n",
       "13  ./kaggle/input/make-data-count-finding-data-re...         test     jats  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the base file dir for the articles to be processed\n",
    "# base_file_dir = ARTICLE_TEST_DIR \\\n",
    "#     if os.getenv('KAGGLE_IS_COMPETITION_RERUN') \\\n",
    "#     else ARTICLE_TRAIN_DIR\n",
    "\n",
    "base_file_dir = ARTICLE_TEST_DIR\n",
    "\n",
    "display(f\"Base dir: {base_file_dir}\")\n",
    "\n",
    "# Load the PDF and XML file paths from the base_file_dir\n",
    "file_paths_df = load_file_paths(base_file_dir)\n",
    "file_paths_df['xml_file_path'] = file_paths_df['xml_file_path'].fillna('')\n",
    "\n",
    "# Get the xml type for each file based on the first line of the XML file\n",
    "file_paths_df['xml_type'] = file_paths_df['xml_file_path'].apply(get_xml_type)\n",
    "\n",
    "file_paths_df.to_csv(os.path.join(BASE_OUTPUT_DIR, 'file_paths.csv'), index=False)\n",
    "\n",
    "print(f\"Files paths shape: {file_paths_df.shape}\")\n",
    "display(file_paths_df.sample(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Qwen evaluation model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:05:01.302796Z",
     "iopub.status.busy": "2025-07-09T17:05:01.302252Z",
     "iopub.status.idle": "2025-07-09T17:05:01.312286Z",
     "shell.execute_reply": "2025-07-09T17:05:01.311602Z",
     "shell.execute_reply.started": "2025-07-09T17:05:01.302775Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- QwenModelEval Class ---\n",
    "#max_new_tokens=32768\n",
    "# class QwenModelEval:\n",
    "#     def __init__(self, model_name, sys_prompt, enable_thinking=False, max_new_tokens=100, max_input_length=8200): # <--- Increased max_new_tokens slightly for safety with greedy\n",
    "#         print(f\"Loading Qwen model and tokenizer from: {model_name}\")\n",
    "#         self.model_name = model_name\n",
    "#         self.sys_prompt = sys_prompt\n",
    "#         self.enable_thinking = enable_thinking  # Enable or disable thinking mode\n",
    "#         self.max_new_tokens = max_new_tokens  # Set the maximum number of new tokens to generate\n",
    "#         self.max_input_length = max_input_length  # Set the maximum input length for the model\n",
    "#         self.bnb_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=torch.float16,\n",
    "#             bnb_4bit_use_double_quant=True,\n",
    "#             llm_int8_enable_fp32_cpu_offload=True\n",
    "#         )\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "#         if self.tokenizer.pad_token is None:\n",
    "#             self.tokenizer.pad_token = self.tokenizer.eos_token        \n",
    "#         self.model = AutoModelForCausalLM.from_pretrained(\n",
    "#             self.model_name,\n",
    "#             quantization_config=self.bnb_config,\n",
    "#             device_map=\"auto\",\n",
    "#             torch_dtype=torch.float16,\n",
    "#             attn_implementation=\"sdpa\",\n",
    "#             trust_remote_code=True\n",
    "#         )\n",
    "#         self.model.eval()\n",
    "#         print(self.model.hf_device_map)\n",
    "\n",
    "#     def generate_response(self, user_input):  \n",
    "#         inputs = self._get_inputs(user_input)\n",
    "        \n",
    "#         with torch.no_grad(): \n",
    "#             generation_args = {\n",
    "#                 \"max_new_tokens\": self.max_new_tokens,\n",
    "#                 \"pad_token_id\": self.tokenizer.eos_token_id,\n",
    "#                 \"eos_token_id\": self.tokenizer.convert_tokens_to_ids(\"<|im_end|>\"),\n",
    "#                 # \"num_beams\": 1, # num_beams is only relevant for beam search, not greedy or simple sampling\n",
    "#             }\n",
    "\n",
    "#             if self.enable_thinking:\n",
    "#                 # Model card's recommended \"thinking mode\" parameters\n",
    "#                 generation_args[\"do_sample\"] = True\n",
    "#                 generation_args[\"temperature\"] = 0.6\n",
    "#                 generation_args[\"top_p\"] = 0.95\n",
    "#                 generation_args[\"top_k\"] = 20\n",
    "#                 generation_args[\"min_p\"] = 0\n",
    "#             else:\n",
    "#                 # Forcing greedy decoding for concise, classification-focused output\n",
    "#                 generation_args[\"do_sample\"] = False # <--- CRITICAL CHANGE: Force Greedy Decoding\n",
    "#                 # Remove sampling parameters as they are not used with do_sample=False\n",
    "#                 # generation_args[\"temperature\"] = 0.1\n",
    "#                 # generation_args[\"top_p\"] = 0.7\n",
    "#                 # generation_args[\"top_k\"] = 5\n",
    "#                 # generation_args[\"min_p\"] = 0\n",
    "\n",
    "#             generated_ids = self.model.generate(\n",
    "#                 **inputs,\n",
    "#                 **generation_args,\n",
    "#             )\n",
    "\n",
    "#         input_len = inputs['input_ids'].shape[1]\n",
    "#         generated_only_ids = generated_ids[0][input_len:]\n",
    "#         decoded_generated_text = self.tokenizer.decode(generated_only_ids, skip_special_tokens=False)\n",
    "        \n",
    "#         print(\"\\n--- RAW DECODED OUTPUT (GENERATED ONLY) ---\")\n",
    "#         print(decoded_generated_text)\n",
    "#         print(\"--------------------------------------------\\n\")\n",
    "\n",
    "#         return self._parse_response(decoded_generated_text)\n",
    "    \n",
    "#     def _get_inputs(self, user_input):\n",
    "#         user_input = user_input[:self.max_input_length]\n",
    "#         messages = [\n",
    "#             {\"role\": \"system\", \"content\": self.sys_prompt},\n",
    "#             {\"role\": \"user\", \"content\": user_input}\n",
    "#         ]\n",
    "#         text = self.tokenizer.apply_chat_template(\n",
    "#             messages,\n",
    "#             tokenize=False,\n",
    "#             add_generation_prompt=True,\n",
    "#         )\n",
    "#         return self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "    \n",
    "#     def _parse_response(self, generated_text: str):\n",
    "#         thinking_content = \"\"\n",
    "#         raw_response = generated_text\n",
    "\n",
    "#         think_match = re.search(r'<think>(.*?)(?=\\[|\\<\\|im_end\\|\\>|$)', generated_text, re.DOTALL)\n",
    "#         if think_match:\n",
    "#             thinking_content = think_match.group(1).strip()\n",
    "#             raw_response = generated_text[generated_text.find(think_match.group(0)) + len(think_match.group(0)):]\n",
    "#             raw_response = raw_response.strip()\n",
    "\n",
    "#         response = self._parse_json(raw_response)\n",
    "        \n",
    "#         print(f\"Parsed response: {response}\")\n",
    "#         print(f\"Extracted thinking content: {thinking_content}\")\n",
    "#         return response, thinking_content\n",
    "    \n",
    "#     def _parse_json(self, raw_response: str) -> list[dict[str,str]]:\n",
    "#         cleaned = raw_response.strip()\n",
    "#         if cleaned.startswith(\"```json\"):\n",
    "#             cleaned = cleaned[len(\"```json\"):].strip()\n",
    "#         if cleaned.endswith(\"```\"):\n",
    "#             cleaned = cleaned[:-3].strip()\n",
    "\n",
    "#         json_match = re.search(r'\\[.*?\\]', cleaned, re.DOTALL)\n",
    "#         if json_match:\n",
    "#             json_string = json_match.group(0)\n",
    "#             try:\n",
    "#                 parsed_json = json.loads(json_string)\n",
    "#                 if isinstance(parsed_json, list) and all(isinstance(item, dict) for item in parsed_json):\n",
    "#                     return parsed_json\n",
    "#                 else:\n",
    "#                     print(f\"Warning: Parsed JSON is not in expected list of dicts format: {parsed_json}\")\n",
    "#                     return []\n",
    "#             except json.JSONDecodeError as e:\n",
    "#                 print(f\"JSON decoding error: {e}\")\n",
    "#                 print(f\"Problematic JSON string: {json_string}\")\n",
    "#                 return []\n",
    "#         else:\n",
    "#             print(\"No JSON array found in generated output.\")\n",
    "#             return []\n",
    "\n",
    "\n",
    "class QwenModelEval:\n",
    "    def __init__(self, model_name, sys_prompt, enable_thinking=True, max_new_tokens=256, max_input_length=8200):\n",
    "        print(f\"Loading Qwen model and tokenizer from: {model_name}\")\n",
    "        self.model_name = model_name\n",
    "        self.sys_prompt = sys_prompt\n",
    "        self.enable_thinking = enable_thinking  # Enable or disable thinking mode\n",
    "        self.max_new_tokens = max_new_tokens  # Set the maximum number of new tokens to generate\n",
    "        self.max_input_length = max_input_length  # Set the maximum input length for the model\n",
    "        self.bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\", # \"fp4\" or \"nf4\"\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            llm_int8_enable_fp32_cpu_offload=True # <--- ADD THIS LINE\n",
    "        )\n",
    "        # Load the tokenizer and ensure pad_token_id is set for generation if not already in tokenizer config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token        \n",
    "        # Load model with quantization\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            quantization_config=self.bnb_config,\n",
    "            device_map=\"auto\", # Automatically maps model layers to available devices\n",
    "            torch_dtype=torch.float16, # Match compute_dtype if using 4-bit\n",
    "            attn_implementation=\"sdpa\", # Use SDPA for better performance\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.model.eval() # Set the model to evaluation mode here.\n",
    "        print(self.model.hf_device_map)\n",
    "\n",
    "    def generate_response(self, user_input):  \n",
    "        inputs = self._get_inputs(user_input)\n",
    "        # Disable gradient calculation during inference\n",
    "        # Generate the response using the model\n",
    "        with torch.no_grad(): \n",
    "            generated_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.convert_tokens_to_ids(\"<|im_end|>\"),\n",
    "                do_sample=False,  # CRUCIAL: Always enable sampling as per model card\n",
    "                # num_beams=1,     # Keep at 1 for standard sampling (not beam search)\n",
    "                # temperature=0.6 if self.enable_thinking else 0.7,\n",
    "                # top_p=0.95 if self.enable_thinking else 0.8,\n",
    "                # top_k=20,\n",
    "                # min_p=0,\n",
    "            )\n",
    "\n",
    "        # Get the length of the input tokens\n",
    "        input_len = inputs['input_ids'].shape[1]\n",
    "        \n",
    "        # Slice generated_ids to get only the newly generated tokens\n",
    "        # generated_ids[0] is the first (and only) sequence in the batch\n",
    "        # [input_len:] slices from the end of the input tokens onwards\n",
    "        generated_only_ids = generated_ids[0][input_len:]\n",
    "        \n",
    "        # Decode only the generated part (including special tokens like <think>)\n",
    "        decoded_generated_text = self.tokenizer.decode(generated_only_ids, skip_special_tokens=False)\n",
    "        \n",
    "        print(\"\\n--- RAW DECODED OUTPUT (GENERATED ONLY) ---\")\n",
    "        print(decoded_generated_text)\n",
    "        print(\"--------------------------------------------\\n\")\n",
    "\n",
    "        # Pass only the generated text to the parsing method\n",
    "        return self._parse_response(inputs, generated_ids)\n",
    "    \n",
    "    def _get_inputs(self, user_input):\n",
    "        \"\"\"Prepare the input for the model based on user input.\"\"\"\n",
    "        # Trim the user input to a maximum length for better performance\n",
    "        user_input = user_input[:self.max_input_length]  # Limit input length to 4096 characters\n",
    "        # Create the messages for the chat template\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=self.enable_thinking\n",
    "        )\n",
    "        return self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "    \n",
    "    def _parse_response(self, inputs, generated_ids):\n",
    "        # Extract the output IDs from the generated IDs\n",
    "        output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist()\n",
    "        try:\n",
    "            index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        thinking_content = self.tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "        raw_response = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "        response = self._parse_json(raw_response)\n",
    "        print(f\"Parsed response: {response}\")\n",
    "        return response, thinking_content\n",
    "    \n",
    "    def _parse_json(self, raw_response: str) -> list[dict[str,str]]:\n",
    "        # Remove code block markers and leading/trailing whitespace\n",
    "        cleaned = raw_response.strip()\n",
    "        if cleaned.startswith(\"```json\"):\n",
    "            cleaned = cleaned[len(\"```json\"):].strip()\n",
    "        if cleaned.endswith(\"```\"):\n",
    "            cleaned = cleaned[:-3].strip()\n",
    "\n",
    "        # Now parse as JSON\n",
    "        try:\n",
    "            return json.loads(cleaned)\n",
    "        except json.JSONDecodeError as e:\n",
    "            return []        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:05:09.611813Z",
     "iopub.status.busy": "2025-07-09T17:05:09.611537Z",
     "iopub.status.idle": "2025-07-09T17:05:09.620447Z",
     "shell.execute_reply": "2025-07-09T17:05:09.619734Z",
     "shell.execute_reply.started": "2025-07-09T17:05:09.611792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define the one-shot reasoning and task prompt\n",
    "# This prompt is designed to guide the model through a structured reasoning process\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are an advanced AI research assistant specialized in identifying and classifying datasets used within academic research papers.\n",
    "Your primary goal is to accurately extract and categorize dataset identifiers (dataset_ids) from provided paper sections.\n",
    "\n",
    "---\n",
    "\n",
    "### Input Data Structure\n",
    "\n",
    "You will receive a JSON string representing key sections of an academic paper, structured as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"title\": \"Title of the paper\",\n",
    "    \"author\": \"The primary author of the paper, e.g., 'Author A'\",\n",
    "    \"abstract\": \"Abstract of the paper\",\n",
    "    \"data_availability\": \"Data availability information\",\n",
    "    \"other_dataset_citations\": [\n",
    "        {\"dataset_ids\": [\"10.12345/12345\"], \"citation_context\": \"Dataset citation context 1\"},\n",
    "        {\"dataset_ids\": [\"10.1234/xxxx.1x1x-xx11\", \"EPI_ISL_12345678\"], \"citation_context\": \"Dataset citation context 2\"},\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Guidance on Input Sections:**\n",
    "*   **`title`**: Provides general context for the paper's topic.\n",
    "*   **`author`**: The primary author of the paper. This is crucial for determining if the dataset's *raw data* was *originally generated by this author*.\n",
    "*   **`abstract`**: **CRITICAL** for understanding the research scope and, most importantly, for determining if a dataset's *raw data* was *originally generated by the authors of *this* paper*.\n",
    "*   **`data_availability`**: This section provides information on datasets. Its content must be evaluated to determine if the data was *originally generated by the authors of this paper* (Primary) or *acquired from an existing source* (Secondary).\n",
    "*   **`other_dataset_citations`**: A list of potential dataset citations. The `citation_context` is vital to confirm if a `dataset_id` truly refers to a dataset and to aid in classifying its origin (Primary or Secondary).\n",
    "\n",
    "---\n",
    "\n",
    "### Core Objective & Critical Exclusion\n",
    "\n",
    "Your overarching objective is to identify and classify **only valid, data-related `dataset_id`s**.\n",
    "\n",
    "**CRITICAL EXCLUSION**: You **MUST NOT** extract any `dataset_id`s that refer to other academic papers, articles, or the paper itself. Focus strictly on identifiers for *datasets* only found within the `abstract`, `data_availability` and `other_dataset_citations` sections. DO NOT make up any dataset_ids.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Definitions\n",
    "\n",
    "*   **`dataset_id`**: A unique, persistent identifier for a dataset. There are two main types:\n",
    "\n",
    "    1.  **Digital Object Identifier (DOI)**:\n",
    "        *   DOI's are used to identify both academic papers/articles and datasets. Your goal is to find DOI's that are related to data/datasets NOT to papers/articles.\n",
    "        *   **Format**: `[prefix]/[suffix]`. The prefix always starts with \"10.\" and is followed by a 4 or 5 digit number. The suffix can contain letters, numbers, and special characters.\n",
    "        *   DO NOT look for Accession IDs within the suffix of a DOI.\n",
    "        *   May or may not start with \"https://doi.org/\" or \"doi:\".\n",
    "        *   **NOTE ON EXTRACTION**: A DOI may appear as a standalone string (e.g., `10.1234/abc`), or embedded within a URL (e.g., `https://doi.org/10.1234/abc`), or within a sentence. In all cases, **extract the `10.xxxx/yyyy` string as a potential `dataset_id`** and proceed with validation rules.\n",
    "        *   **IMPORTANT DOI VALIDATION RULE**:\n",
    "            *   A DOI is a `dataset_id` ONLY if the surrounding `citation_context` or `data_availability` section clearly indicates it refers to a dataset, data repository, data archive, or similar data-specific entity.\n",
    "            *   Only identify DOIs that are explicitly used as `dataset_id`s.\n",
    "            *   **DO NOT extract DOIs for academic papers/articles.**\n",
    "            *   **If a DOI is presented as a reference to a paper, article, or publication (e.g., \"as described in [DOI]\", \"cited in [DOI]\", \"see [DOI] for details on the method\"), it is NOT a dataset_id.**\n",
    "\n",
    "    2.  **Accession ID**:\n",
    "        *   DO NOT look for Accession IDs within a DOI. If a dataset_id has a DOI format, no portion of the DOI should be identified as an Accession ID.\n",
    "        *   They always start with two or more alpha characters, including underscores (\"_\"), followed by three or more digits.\n",
    "        *   These identifiers are often used in biological databases, chemical databases, or other scientific data repositories.\n",
    "        *   *Examples*: `\"EPI_ISL_12345678\"` (e.g., a genomic sequence dataset), `\"IPR000264\"` (e.g., a protein family identifier), `\"SAMN07159041\"` (e.g., a biological sample record), `\"CHEMBL1782574\"` (e.g., a chemical compound entry)\n",
    "\n",
    "*   **Distinction: Data Source vs. Specific Data Product (dataset_id):**\n",
    "    *   A **Data Source** is a repository or collection where data is stored (e.g., The Cancer Imaging Archive).\n",
    "    *   A **Specific Data Product** is the actual dataset or data item being referenced by a `dataset_id` (e.g., \"images\", \"scans\", \"segmentations\", \"raw data\", etc.).\n",
    "    *   **CRITICAL**: The classification (Primary/Secondary) applies to the *Specific Data Product* associated with the `dataset_id`, not the general data source it might have come from.\n",
    "\n",
    "*   **Dataset Type Classification**: **This is the MOST CRITICAL distinction. For each `dataset_id`, ask: Was the *specific data product* it refers to *CREATED* by the authors of *this paper*?**\n",
    "\n",
    "    *   **Primary**: The `dataset_id` refers to **NEW DATA** (e.g., measurements, annotations, segmentations, etc.) that was **ORIGINALLY GENERATED, COLLECTED, PROCESSED, or CREATED by the AUTHORS OF *THIS SPECIFIC PAPER*** as a direct output of their research. This is the *novel contribution* of the paper.\n",
    "        *   *Keywords for Primary*: \"generated\", \"sequenced\", \"collected\", \"created\", \"produced\", \"developed\", \"annotated\", \"our data\".\n",
    "        *   **CRITICAL CLARIFICATION**: If a `dataset_id` refers to a novel data product (like annotations or segmentations) that was *created by the authors* (Primary) but was *derived from* or *applied to* existing, external data (Secondary input), the `dataset_id` for the *novel data product* is still **Primary**. The act of making this novel data product publicly available does not change its origin.\n",
    "        *   *Example*: For an example DOI of `10.7937/tcia.2020.6c7y-gq39`, if the context states \"a dataset of thoracic cavity segmentations and discrete pleural effusion segmentations **we have annotated**... All expert-vetted segmentations are publicly available... at https://doi.org/10.7937/tcia.2020.6c7y-gq39\". The \"we have annotated\" indicates original creation of the *segmentations* by the authors, making this **Primary**.\n",
    "\n",
    "    *   **Secondary**: The `dataset_id` refers to **EXISTING DATA** that was **ACQUIRED, DERIVED, USED, REUSED, or RE-ANALYZED from EXISTING RECORDS or PREVIOUSLY PUBLISHED DATASETS** and that was *not originally generated by the authors of this specific paper*. This is *input data* that the authors *used*, but did not create.\n",
    "        *   *Keywords for Secondary*: \"previously published\", \"existing\", \"external\", \"re-analyzed\", \"obtained from\", \"acquired from\", \"derived from\", \"sourced from\", \"data from [external source]\".\n",
    "        *   **IMPORTANT**: If the data was *not created by the authors of this paper*, it is **Secondary**.\n",
    "        *   *Example*: For an example DOI of `10.7937/K9/TCIA.2015.PF0M9REI`, if the context states \"CT scans **acquired from The Cancer Imaging Archive 'NSCLC Radiomics' data collection**\". This indicates the raw CT scans were acquired and used by the authors, but not created by them, making this **Secondary**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Classification Logic Flow (for each identified `dataset_id`):\n",
    "\n",
    "To classify a `dataset_id` as Primary or Secondary, follow these steps strictly:\n",
    "\n",
    "1.  **For the specific `dataset_id` being evaluated, identify the *data product* it refers to** by examining the `abstract` and associated `citation_context`. (e.g., \"images\", \"scans\", \"segmentations\", \"genomic sequences\", \"raw data\", etc.).\n",
    "2.  **STEP 1: Is this *specific data product* a NEW CREATION by the authors of *this paper*?**\n",
    "    *   Look for phrases like \"we have annotated\", \"generated by us\", \"created by the authors\", \"our data\", \"produced in this study\", or descriptions of *original data collection/creation* by the authors.\n",
    "        *   **IF YES**: Classify as **Primary**.\n",
    "        *   *Example*: If the abstract states things like \"a dataset of thoracic cavity segmentations and discrete pleural effusion segmentations **we have annotated**... All expert-vetted segmentations are publicly available at ...\". The \"we have annotated\" indicates original creation of the *segmentations* by the authors, making this **Primary**.\n",
    "        *   **REMEMBER**: Even if this *Primary* data product was derived from *Secondary* input data, it is still **Primary** because the *data product itself* is a novel creation of these authors.\n",
    "3.  **STEP 2: Is this *specific data product* EXISTING DATA ACQUIRED from an *external source*?**\n",
    "    *   If the `dataset_id` was *not* classified as Primary in Step 1, then look for phrases like \"acquired from\", \"obtained from\", \"derived from\", or \"previously published\".\n",
    "        *   **IF YES**: Classify as **Secondary**.\n",
    "        *   *Example*: If the abstract states things like \"scans **acquired from** The Cancer Imaging Archive 'NSCLC Radiomics' data collection\". The **acquired from** indicates the raw CT scans were acquired and used by the authors, but not created by them, making this **Secondary**.\n",
    "4.  **STEP 3: Fallback Rule:**\n",
    "    *   If, after applying Step 1 and Step 2, the origin of the *specific data product* remains truly ambiguous, then default to \"Primary\".\n",
    "\n",
    "---\n",
    "\n",
    "### Tasks: Step-by-Step Instructions\n",
    "\n",
    "Follow these three tasks in order:\n",
    "\n",
    "**SHORT-CIRCUIT RULE:**\n",
    "**IF** the `data_availability` section is an empty string (`\"\"`) **AND** the `other_dataset_citations` section is an empty list (`[]`), **THEN** immediately **skip all other tasks** and proceed directly to **Task 3** to output the \"Missing\" JSON structure. Do not perform any further analysis or reasoning.\n",
    "\n",
    "**Task 1: Identify Valid Dataset IDs**\n",
    "\n",
    "1.  **Search Priority**:\n",
    "    *   **IF** `data_availability` is NOT an empty string (`\"\"`), search its text first.\n",
    "    *   **THEN**, **IF** `other_dataset_citations` is NOT an empty list (`[]`), search its text.\n",
    "    *   **IMPORTANT**: If `data_availability` is empty, proceed directly to search `other_dataset_citations`.\n",
    "2.  **Validation and Extraction**: For each potential `dataset_id` (DOI or Accession ID) found *within the text* of `data_availability` or `citation_context`, confirm it is truly data-related and **extract the identifier string**.\n",
    "    *   **For DOIs**: Strictly apply the **IMPORTANT DOI VALIDATION RULE** defined above. If it refers to a publication, **DO NOT** extract it.\n",
    "    *   **For all IDs**: Look for surrounding terms like \"data release\", \"data availability\", \"dataset\", \"database\", \"repository\", \"data source\", \"data access\", or \"data archive\" within the `data_availability` section or the `citation_context`.\n",
    "    *   *Example of Extraction from `data_availability`*: If `data_availability` contains \"Data are available at Dryad Digital Repository at: https://doi.org/10.5061/dryad.zw3r22854 . ...\", then `10.5061/dryad.zw3r22854` is a valid `dataset_id` to extract.\n",
    "3.  **Deduplication**: If the same `dataset_id` is found multiple times, **only process the first instance encountered**.\n",
    "4.  **Conditional Proceeding**:\n",
    "    *   If **no valid `dataset_id`s are found** after searching both sections, **skip directly to Task 3** and output the \"Missing\" JSON structure.\n",
    "    *   If one or more valid `dataset_id`s are found, proceed to Task 2.\n",
    "\n",
    "**Task 2: Classify Dataset Types**\n",
    "\n",
    "1.  For each valid `dataset_id` identified in Task 1, classify its type as either \"Primary\" or \"Secondary\".\n",
    "2.  **STRICTLY APPLY THE \"CLASSIFICATION LOGIC FLOW\" ABOVE for each `dataset_id`.** Use the `abstract` section and the `citation_context` to determine if the *specific data product* associated with the `dataset_id` was *originally generated by the authors of *this* paper* (Primary) or *acquired/reused from an existing source* (Secondary).\n",
    "3.  Apply the \"Key Definitions\" for Primary and Secondary types, paying close attention to the associated keywords and the provided examples.\n",
    "4.  Remember the \"Fallback Rule\": Default to \"Primary\" if the classification remains truly ambiguous regarding the *original generation* of the raw data.\n",
    "\n",
    "**Task 3: Format and Return Results**\n",
    "\n",
    "**CRITICAL: Your entire response MUST ONLY be the JSON array. Do NOT include any conversational text, explanations, reasoning steps, or internal thoughts (like <think> tags).**\n",
    "\n",
    "Return your final results as a JSON array of objects.\n",
    "\n",
    "1.  **Scenario A: No Valid Datasets Found**\n",
    "    If Task 1 resulted in no valid `dataset_id`s, return a single JSON object with the following structure:\n",
    "    ```json\n",
    "    [\n",
    "        {\n",
    "            \"dataset_id\": \"Missing\",\n",
    "            \"type\": \"Missing\"\n",
    "        }\n",
    "    ]\n",
    "    ```\n",
    "2.  **Scenario B: One or More Valid Datasets Found**\n",
    "    If Task 1 identified one or more valid `dataset_id`s, return every valid dataset found in a JSON array of objects, where each object has the following structure:\n",
    "    ```json\n",
    "    [\n",
    "        {\n",
    "            \"dataset_id\": \"example_id_1\",\n",
    "            \"type\": \"Primary\"\n",
    "        },\n",
    "        {\n",
    "            \"dataset_id\": \"example_id_2\",\n",
    "            \"type\": \"Secondary\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    ```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Qwen Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:34:21.490445Z",
     "iopub.status.busy": "2025-07-09T17:34:21.489652Z",
     "iopub.status.idle": "2025-07-09T17:34:23.782115Z",
     "shell.execute_reply": "2025-07-09T17:34:23.781370Z",
     "shell.execute_reply.started": "2025-07-09T17:34:21.490419Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen model and tokenizer from: C:\\Users\\jim\\.cache\\kagglehub\\models\\qwen-lm\\qwen-3\\transformers\\1.7b\\1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea94ff03f2784deda018b40e548ee08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensor.item() cannot be called on meta tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Instantiate the QwenModelEval class with the model path and system prompt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m inference_model = \u001b[43mQwenModelEval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSYS_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_thinking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_input_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 149\u001b[39m, in \u001b[36mQwenModelEval.__init__\u001b[39m\u001b[34m(self, model_name, sys_prompt, enable_thinking, max_new_tokens, max_input_length)\u001b[39m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28mself\u001b[39m.tokenizer.pad_token = \u001b[38;5;28mself\u001b[39m.tokenizer.eos_token        \n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# Load model with quantization\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[38;5;28mself\u001b[39m.model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Automatically maps model layers to available devices\u001b[39;49;00m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Match compute_dtype if using 4-bit\u001b[39;49;00m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msdpa\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use SDPA for better performance\u001b[39;49;00m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    156\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[38;5;28mself\u001b[39m.model.eval() \u001b[38;5;66;03m# Set the model to evaluation mode here.\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m.model.hf_device_map)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4668\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4665\u001b[39m         device_map_kwargs[\u001b[33m\"\u001b[39m\u001b[33moffload_buffers\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   4667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_deepspeed_zero3_enabled():\n\u001b[32m-> \u001b[39m\u001b[32m4668\u001b[39m         \u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdevice_map_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4671\u001b[39m     hf_quantizer.postprocess_model(model, config=config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\accelerate\\big_modeling.py:426\u001b[39m, in \u001b[36mdispatch_model\u001b[39m\u001b[34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[39m\n\u001b[32m    421\u001b[39m         tied_params_map[data_ptr] = {}\n\u001b[32m    423\u001b[39m         \u001b[38;5;66;03m# Note: To handle the disk offloading case, we can not simply use weights_map[param_name].data_ptr() as the reference pointer,\u001b[39;00m\n\u001b[32m    424\u001b[39m         \u001b[38;5;66;03m# as we have no guarantee that safetensors' `file.get_tensor()` will always give the same pointer.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m426\u001b[39m \u001b[43mattach_align_device_hook_on_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffload\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# warn if there is any params on the meta device\u001b[39;00m\n\u001b[32m    438\u001b[39m offloaded_devices_str = \u001b[33m\"\u001b[39m\u001b[33m and \u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    439\u001b[39m     [device \u001b[38;5;28;01mfor\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(device_map.values()) \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m    440\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:676\u001b[39m, in \u001b[36mattach_align_device_hook_on_blocks\u001b[39m\u001b[34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m child_name, child \u001b[38;5;129;01min\u001b[39;00m module.named_children():\n\u001b[32m    675\u001b[39m     child_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchild_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(module_name) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m child_name\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m     \u001b[43mattach_align_device_hook_on_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:676\u001b[39m, in \u001b[36mattach_align_device_hook_on_blocks\u001b[39m\u001b[34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m child_name, child \u001b[38;5;129;01min\u001b[39;00m module.named_children():\n\u001b[32m    675\u001b[39m     child_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchild_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(module_name) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m child_name\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m     \u001b[43mattach_align_device_hook_on_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:676\u001b[39m, in \u001b[36mattach_align_device_hook_on_blocks\u001b[39m\u001b[34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[39m\n\u001b[32m    674\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m child_name, child \u001b[38;5;129;01min\u001b[39;00m module.named_children():\n\u001b[32m    675\u001b[39m     child_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchild_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(module_name) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m child_name\n\u001b[32m--> \u001b[39m\u001b[32m676\u001b[39m     \u001b[43mattach_align_device_hook_on_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    680\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_buffers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchild_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:658\u001b[39m, in \u001b[36mattach_align_device_hook_on_blocks\u001b[39m\u001b[34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[39m\n\u001b[32m    651\u001b[39m         hook = AlignDevicesHook(\n\u001b[32m    652\u001b[39m             execution_device=execution_device[module_name],\n\u001b[32m    653\u001b[39m             io_same_device=(module_name == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    654\u001b[39m             skip_keys=skip_keys,\n\u001b[32m    655\u001b[39m             tied_params_map=tied_params_map,\n\u001b[32m    656\u001b[39m         )\n\u001b[32m    657\u001b[39m         add_hook_to_module(module, hook)\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m     \u001b[43mattach_execution_device_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m module_name == \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    666\u001b[39m     hook = AlignDevicesHook(\n\u001b[32m    667\u001b[39m         execution_device=execution_device.get(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    668\u001b[39m         io_same_device=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    669\u001b[39m         skip_keys=skip_keys,\n\u001b[32m    670\u001b[39m         tied_params_map=tied_params_map,\n\u001b[32m    671\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:451\u001b[39m, in \u001b[36mattach_execution_device_hook\u001b[39m\u001b[34m(module, execution_device, skip_keys, preload_module_classes, tied_params_map)\u001b[39m\n\u001b[32m    448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m module.children():\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[43mattach_execution_device_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecution_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_module_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtied_params_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\accelerate\\hooks.py:440\u001b[39m, in \u001b[36mattach_execution_device_hook\u001b[39m\u001b[34m(module, execution_device, skip_keys, preload_module_classes, tied_params_map)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mattach_execution_device_hook\u001b[39m(\n\u001b[32m    413\u001b[39m     module: torch.nn.Module,\n\u001b[32m    414\u001b[39m     execution_device: Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m, torch.device],\n\u001b[32m   (...)\u001b[39m\u001b[32m    417\u001b[39m     tied_params_map: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mdict\u001b[39m[torch.device, torch.Tensor]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    418\u001b[39m ):\n\u001b[32m    419\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    420\u001b[39m \u001b[33;03m    Recursively attaches `AlignDevicesHook` to all submodules of a given model to make sure they have the right\u001b[39;00m\n\u001b[32m    421\u001b[39m \u001b[33;03m    execution device\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    438\u001b[39m \u001b[33;03m            instead of duplicating memory.\u001b[39;00m\n\u001b[32m    439\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33m_hf_hook\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m) > \u001b[32m0\u001b[39m:\n\u001b[32m    441\u001b[39m         add_hook_to_module(\n\u001b[32m    442\u001b[39m             module,\n\u001b[32m    443\u001b[39m             AlignDevicesHook(execution_device, skip_keys=skip_keys, tied_params_map=tied_params_map),\n\u001b[32m    444\u001b[39m         )\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# Break the recursion if we get to a preload module.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2228\u001b[39m, in \u001b[36mModule.state_dict\u001b[39m\u001b[34m(self, destination, prefix, keep_vars, *args)\u001b[39m\n\u001b[32m   2226\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules.items():\n\u001b[32m   2227\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2228\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2229\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2230\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2231\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2232\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2233\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict_hooks.values():\n\u001b[32m   2234\u001b[39m     hook_result = hook(\u001b[38;5;28mself\u001b[39m, destination, prefix, local_metadata)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2225\u001b[39m, in \u001b[36mModule.state_dict\u001b[39m\u001b[34m(self, destination, prefix, keep_vars, *args)\u001b[39m\n\u001b[32m   2223\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict_pre_hooks.values():\n\u001b[32m   2224\u001b[39m     hook(\u001b[38;5;28mself\u001b[39m, prefix, keep_vars)\n\u001b[32m-> \u001b[39m\u001b[32m2225\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save_to_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2226\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._modules.items():\n\u001b[32m   2227\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:470\u001b[39m, in \u001b[36mLinear4bit._save_to_state_dict\u001b[39m\u001b[34m(self, destination, prefix, keep_vars)\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;28msuper\u001b[39m()._save_to_state_dict(destination, prefix, keep_vars)  \u001b[38;5;66;03m# saving weight and bias\u001b[39;00m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.weight, \u001b[33m\"\u001b[39m\u001b[33mquant_state\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquant_state\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.items():\n\u001b[32m    471\u001b[39m         destination[prefix + \u001b[33m\"\u001b[39m\u001b[33mweight.\u001b[39m\u001b[33m\"\u001b[39m + k] = v \u001b[38;5;28;01mif\u001b[39;00m keep_vars \u001b[38;5;28;01melse\u001b[39;00m v.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\bitsandbytes\\functional.py:677\u001b[39m, in \u001b[36mQuantState.as_dict\u001b[39m\u001b[34m(self, packed)\u001b[39m\n\u001b[32m    662\u001b[39m qs_dict = {\n\u001b[32m    663\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mquant_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.quant_type,\n\u001b[32m    664\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mabsmax\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.absmax,\n\u001b[32m   (...)\u001b[39m\u001b[32m    668\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m.shape),\n\u001b[32m    669\u001b[39m }\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nested:\n\u001b[32m    671\u001b[39m     qs_dict.update(\n\u001b[32m    672\u001b[39m         {\n\u001b[32m    673\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnested_absmax\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.state2.absmax,\n\u001b[32m    674\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnested_blocksize\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.state2.blocksize,\n\u001b[32m    675\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnested_quant_map\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.state2.code.clone(),  \u001b[38;5;66;03m# un-shared to avoid restoring it after shared tensors are removed by safetensors\u001b[39;00m\n\u001b[32m    676\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnested_dtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.state2.dtype).strip(\u001b[33m\"\u001b[39m\u001b[33mtorch.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnested_offset\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    678\u001b[39m         },\n\u001b[32m    679\u001b[39m     )\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m packed:\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m qs_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\torch\\_meta_registrations.py:7088\u001b[39m, in \u001b[36mmeta_local_scalar_dense\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   7086\u001b[39m \u001b[38;5;129m@register_meta\u001b[39m(aten._local_scalar_dense)\n\u001b[32m   7087\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmeta_local_scalar_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m: Tensor):\n\u001b[32m-> \u001b[39m\u001b[32m7088\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTensor.item() cannot be called on meta tensors\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Tensor.item() cannot be called on meta tensors"
     ]
    }
   ],
   "source": [
    "# Instantiate the QwenModelEval class with the model path and system prompt\n",
    "inference_model = QwenModelEval(MODEL_PATH, sys_prompt=SYS_PROMPT, enable_thinking=False, max_new_tokens=100, max_input_length=8200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:35:13.430070Z",
     "iopub.status.busy": "2025-07-09T17:35:13.429778Z",
     "iopub.status.idle": "2025-07-09T17:35:13.436210Z",
     "shell.execute_reply": "2025-07-09T17:35:13.435612Z",
     "shell.execute_reply.started": "2025-07-09T17:35:13.430048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_articles(file_paths_df: pd.DataFrame, model) -> pd.DataFrame:\n",
    "    results = []\n",
    "    # tqdm(os.listdir(pdf_directory), total=len(os.listdir(pdf_directory)))\n",
    "    for i, row in tqdm(file_paths_df.iterrows(), total=len(file_paths_df)):\n",
    "        article_id = row['article_id']\n",
    "        text_type = \"XML\"\n",
    "        article_dict = extract_article_dict(row['xml_file_path'])\n",
    "        if not article_dict['data_availability'] and not article_dict['other_dataset_citations']:\n",
    "            text_type = \"PDF\"\n",
    "            article_dict = extract_article_dict(row['pdf_file_path'])\n",
    "\n",
    "        user_input = \"\"\n",
    "        response = [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
    "        thinking_content = \"\"\n",
    "        \n",
    "        # Only process articles that have data_availability and/or other_dataset_citations\n",
    "        if article_dict['data_availability'] or article_dict['other_dataset_citations']:\n",
    "            # Prepare the user input for the model\n",
    "            user_input = f\"Text Content: {extract_article_text(article_dict)}\\n\"\n",
    "            print(f\"Processing article {i}/{len(file_paths_df)}: {article_id}, type: {text_type}, input: {len(user_input)}\")\n",
    "            # Generate response from the model\n",
    "            response, thinking_content = model.generate_response(user_input)\n",
    "\n",
    "        results.append({\n",
    "            'article_id': article_id,\n",
    "            'text_type': text_type,\n",
    "            'llm_input': user_input,\n",
    "            'llm_response': response,\n",
    "            'llm_thinking_content': thinking_content\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(by=[\"article_id\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:35:17.339049Z",
     "iopub.status.busy": "2025-07-09T17:35:17.338716Z",
     "iopub.status.idle": "2025-07-09T17:35:17.349597Z",
     "shell.execute_reply": "2025-07-09T17:35:17.348962Z",
     "shell.execute_reply.started": "2025-07-09T17:35:17.339025Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File paths DataFrame shape: (30, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "1e2ed845-b042-410e-9ac3-aa868f4e2dbc",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_2017jc013030.xml",
         "test",
         "tei"
        ],
        [
         "1",
         "10.1002_anie.201916483",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.201916483.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.201916483.xml",
         "test",
         "jats"
        ],
        [
         "2",
         "10.1002_anie.202005531",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202005531.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.202005531.xml",
         "test",
         "bioc"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>xml_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>tei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>jats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>bioc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                                      pdf_file_path  \\\n",
       "0    10.1002_2017jc013030  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "1  10.1002_anie.201916483  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "2  10.1002_anie.202005531  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                       xml_file_path dataset_type xml_type  \n",
       "0  ./kaggle/input/make-data-count-finding-data-re...         test      tei  \n",
       "1  ./kaggle/input/make-data-count-finding-data-re...         test     jats  \n",
       "2  ./kaggle/input/make-data-count-finding-data-re...         test     bioc  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Load the file paths DataFrame from the CSV file\n",
    "# file_paths_df = pd.read_csv(os.path.join(BASE_OUTPUT_DIR, 'file_paths.csv'))\n",
    "# # Fill NaN values in the 'xml_type' and 'xml_text' columns with empty strings\n",
    "# file_paths_df['xml_file_path'] = file_paths_df['xml_file_path'].fillna('')\n",
    "# file_paths_df['xml_text'] = file_paths_df['xml_text'].fillna('')\n",
    "# Display the first few rows of the file paths DataFrame\n",
    "print(f\"File paths DataFrame shape: {file_paths_df.shape}\")\n",
    "display(file_paths_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T21:24:15.583840Z",
     "iopub.status.busy": "2025-07-08T21:24:15.583522Z",
     "iopub.status.idle": "2025-07-08T21:24:15.586902Z",
     "shell.execute_reply": "2025-07-08T21:24:15.586342Z",
     "shell.execute_reply.started": "2025-07-08T21:24:15.583819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# sample_file_paths_df = file_paths_df.copy().sample(3, random_state=42).reset_index(drop=True)\n",
    "# sample_file_paths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:35:27.033057Z",
     "iopub.status.busy": "2025-07-09T17:35:27.032475Z",
     "iopub.status.idle": "2025-07-09T17:38:40.615728Z",
     "shell.execute_reply": "2025-07-09T17:38:40.614959Z",
     "shell.execute_reply.started": "2025-07-09T17:35:27.033028Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8fcb6f79b84de19ca4396ada494ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article 0/30: 10.1002_2017jc013030, type: XML, input: 1441\n",
      "\n",
      "--- RAW DECODED OUTPUT (GENERATED ONLY) ---\n",
      "<think>\n",
      "Okay, let's tackle this problem step by step. The user provided a JSON structure with a paper's details, and I need to extract and classify dataset IDs based on the given instructions. \n",
      "\n",
      "First, I'll check the input data. The title is about assessing variability in the relationship between particulate backscattering coefficient and chlorophyll a concentration using a global Biogeochemical-Argo database. The author is Marie Barbieux. The abstract mentions the BGC-Argo\n",
      "--------------------------------------------\n",
      "\n",
      "No JSON array found in generated output.\n",
      "Parsed response: []\n",
      "Extracted thinking content: Okay, let's tackle this problem step by step. The user provided a JSON structure with a paper's details, and I need to extract and classify dataset IDs based on the given instructions. \n",
      "\n",
      "First, I'll check the input data. The title is about assessing variability in the relationship between particulate backscattering coefficient and chlorophyll a concentration using a global Biogeochemical-Argo database. The author is Marie Barbieux. The abstract mentions the BGC-Argo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article 1/30: 10.1002_anie.201916483, type: PDF, input: 1340\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m processed_articles_df = \u001b[43mevaluate_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Save processed_articles_df to CSV\u001b[39;00m\n\u001b[32m      3\u001b[39m processed_articles_df.to_csv(os.path.join(BASE_OUTPUT_DIR, \u001b[33m'\u001b[39m\u001b[33msample_evaluated_articles.csv\u001b[39m\u001b[33m'\u001b[39m), index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mevaluate_articles\u001b[39m\u001b[34m(file_paths_df, model)\u001b[39m\n\u001b[32m     20\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing article \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(file_paths_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(user_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m         \u001b[38;5;66;03m# Generate response from the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         response, thinking_content = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     results.append({\n\u001b[32m     25\u001b[39m         \u001b[33m'\u001b[39m\u001b[33marticle_id\u001b[39m\u001b[33m'\u001b[39m: article_id,\n\u001b[32m     26\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtext_type\u001b[39m\u001b[33m'\u001b[39m: text_type,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mllm_thinking_content\u001b[39m\u001b[33m'\u001b[39m: thinking_content\n\u001b[32m     30\u001b[39m     })\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(results).sort_values(by=[\u001b[33m\"\u001b[39m\u001b[33marticle_id\u001b[39m\u001b[33m\"\u001b[39m]).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mQwenModelEval.generate_response\u001b[39m\u001b[34m(self, user_input)\u001b[39m\n\u001b[32m     52\u001b[39m         generation_args[\u001b[33m\"\u001b[39m\u001b[33mdo_sample\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;66;03m# <--- CRITICAL CHANGE: Force Greedy Decoding\u001b[39;00m\n\u001b[32m     53\u001b[39m         \u001b[38;5;66;03m# Remove sampling parameters as they are not used with do_sample=False\u001b[39;00m\n\u001b[32m     54\u001b[39m         \u001b[38;5;66;03m# generation_args[\"temperature\"] = 0.1\u001b[39;00m\n\u001b[32m     55\u001b[39m         \u001b[38;5;66;03m# generation_args[\"top_p\"] = 0.7\u001b[39;00m\n\u001b[32m     56\u001b[39m         \u001b[38;5;66;03m# generation_args[\"top_k\"] = 5\u001b[39;00m\n\u001b[32m     57\u001b[39m         \u001b[38;5;66;03m# generation_args[\"min_p\"] = 0\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     generated_ids = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m input_len = inputs[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m].shape[\u001b[32m1\u001b[39m]\n\u001b[32m     65\u001b[39m generated_only_ids = generated_ids[\u001b[32m0\u001b[39m][input_len:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2597\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2589\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2590\u001b[39m         input_ids=input_ids,\n\u001b[32m   2591\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2592\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2593\u001b[39m         **model_kwargs,\n\u001b[32m   2594\u001b[39m     )\n\u001b[32m   2596\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2597\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2604\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2607\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2609\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2610\u001b[39m         input_ids=input_ids,\n\u001b[32m   2611\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2612\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2613\u001b[39m         **model_kwargs,\n\u001b[32m   2614\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:3560\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3558\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3560\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3562\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3563\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3564\u001b[39m     outputs,\n\u001b[32m   3565\u001b[39m     model_kwargs,\n\u001b[32m   3566\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3567\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:969\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    966\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    971\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:746\u001b[39m, in \u001b[36mQwen3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    744\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n\u001b[32m    745\u001b[39m slice_indices = \u001b[38;5;28mslice\u001b[39m(-logits_to_keep, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits_to_keep, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m logits_to_keep\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    748\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Data\\Projects\\make-data-count-classifier\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "processed_articles_df = evaluate_articles(file_paths_df, inference_model)\n",
    "# Save processed_articles_df to CSV\n",
    "processed_articles_df.to_csv(os.path.join(BASE_OUTPUT_DIR, 'sample_evaluated_articles.csv'), index=False)\n",
    "print(f\"Processed articles DataFrame shape: {processed_articles_df.shape}\")\n",
    "processed_articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:38:58.797394Z",
     "iopub.status.busy": "2025-07-09T17:38:58.796686Z",
     "iopub.status.idle": "2025-07-09T17:38:58.806168Z",
     "shell.execute_reply": "2025-07-09T17:38:58.805399Z",
     "shell.execute_reply.started": "2025-07-09T17:38:58.797368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_dataset_id(dataset_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Formats the dataset_id by removing any leading/trailing whitespace and ensuring it is a string.\n",
    "    \n",
    "    Args:\n",
    "        dataset_id (str): The dataset identifier to format.\n",
    "        \n",
    "    Returns:\n",
    "        str: The formatted dataset identifier.\n",
    "    \"\"\"\n",
    "    if dataset_id and dataset_id.startswith(\"10.\") and len(dataset_id) > 10:\n",
    "        # If the dataset_id starts with \"10.\" and is longer than 10 characters, it's likely a DOI\n",
    "        dataset_id = \"https://doi.org/\" + dataset_id.lower().strip()\n",
    "    return dataset_id\n",
    "\n",
    "# Create a DataFrame to hold the evaluation results by expaning the 'llm_response' column\n",
    "def expand_evaluation_results(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expands the evaluation results DataFrame by extracting dataset_id and type from the 'llm_response' column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing evaluation results.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with expanded dataset_id and type columns.\n",
    "    \"\"\"\n",
    "    expanded_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        article_id = row['article_id']\n",
    "        article_doi = article_id.replace('_', '/')\n",
    "        datasets = row['llm_response']\n",
    "        missing_dataset = {\n",
    "            'article_id': article_id,\n",
    "            'dataset_id': 'Missing',\n",
    "            'type': 'Missing',\n",
    "        }\n",
    "\n",
    "        if datasets:\n",
    "            for dataset in datasets:\n",
    "                dataset_id = dataset.get('dataset_id', 'Missing')\n",
    "                # Skip if the dataset_id is the same as the article DOI\n",
    "                if dataset_id == article_doi:\n",
    "                    # If the dataset_id is the same as the article DOI add it as Missing\n",
    "                    expanded_rows.append(missing_dataset)\n",
    "                else:\n",
    "                    expanded_rows.append({\n",
    "                        'article_id': article_id,\n",
    "                        'dataset_id': dataset.get('dataset_id', 'Missing'),\n",
    "                        'type': dataset.get('type', 'Missing'),\n",
    "                    })\n",
    "        else:\n",
    "            # If no datasets were found, add a row with 'Missing' values\n",
    "            expanded_rows.append(missing_dataset)\n",
    "    \n",
    "    # Create a DataFrame from the expanded rows\n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "    expanded_df['dataset_id'] = expanded_df['dataset_id'].apply(format_dataset_id)  # Format dataset_id\n",
    "    expanded_df['type'] = expanded_df['type'].str.strip().str.capitalize()  # Ensure type is capitalized and stripped of whitespace\n",
    "    expanded_df = expanded_df.sort_values(by=[\"article_id\", \"dataset_id\", \"type\"], ascending=True).drop_duplicates(subset=['article_id', 'dataset_id'], keep=\"first\").reset_index(drop=True)\n",
    "    \n",
    "    return expanded_df\n",
    "\n",
    "def prepare_for_submission(expanded_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the expanded DataFrame for submission by ensuring the correct columns and formatting.\n",
    "    \n",
    "    Args:\n",
    "        expanded_df (pd.DataFrame): The DataFrame containing expanded dataset information.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame ready for submission with 'article_id', 'dataset_id', and 'type' columns.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame has the correct columns\n",
    "    submission_df = expanded_df[['article_id', 'dataset_id', 'type']].copy()\n",
    "    # Rename columns to match the expected format\n",
    "    submission_df.columns = ['article_id', 'dataset_id', 'type']\n",
    "\n",
    "    # Remove rows where type is 'Missing' and reset index\n",
    "    submission_df = submission_df[submission_df['type'] != 'Missing'].reset_index(drop=True)\n",
    "    submission_df['row_id'] = range(len(submission_df))\n",
    "\n",
    "    # Reorder columns to match the submission format\n",
    "    submission_df = submission_df[['row_id', 'article_id', 'dataset_id', 'type']]\n",
    "    \n",
    "    return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:39:07.879450Z",
     "iopub.status.busy": "2025-07-09T17:39:07.879178Z",
     "iopub.status.idle": "2025-07-09T17:39:07.895828Z",
     "shell.execute_reply": "2025-07-09T17:39:07.894999Z",
     "shell.execute_reply.started": "2025-07-09T17:39:07.879430Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Eval DataFrame shape: (40, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "028ce6e8-a99c-40da-868a-4e16d30a01cf",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "https://doi.org/10.17882/47142",
         "Primary"
        ],
        [
         "1",
         "10.1002_2017jc013030",
         "https://doi.org/10.17882/49388",
         "Primary"
        ],
        [
         "2",
         "10.1002_2017jc013030",
         "https://doi.org/10.5194/essd-2017-58",
         "Primary"
        ],
        [
         "3",
         "10.1002_anie.201916483",
         "Missing",
         "Missing"
        ],
        [
         "4",
         "10.1002_anie.202005531",
         "Missing",
         "Missing"
        ],
        [
         "5",
         "10.1002_anie.202007717",
         "Missing",
         "Missing"
        ],
        [
         "6",
         "10.1002_chem.201902131",
         "Missing",
         "Missing"
        ],
        [
         "7",
         "10.1002_chem.201903120",
         "Missing",
         "Missing"
        ],
        [
         "8",
         "10.1002_chem.202000235",
         "Missing",
         "Missing"
        ],
        [
         "9",
         "10.1002_chem.202001412",
         "Missing",
         "Missing"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/47142</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/49388</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.5194/essd-2017-58</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1002_anie.202007717</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.1002_chem.201902131</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.1002_chem.201903120</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.1002_chem.202000235</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.1002_chem.202001412</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                            dataset_id     type\n",
       "0    10.1002_2017jc013030        https://doi.org/10.17882/47142  Primary\n",
       "1    10.1002_2017jc013030        https://doi.org/10.17882/49388  Primary\n",
       "2    10.1002_2017jc013030  https://doi.org/10.5194/essd-2017-58  Primary\n",
       "3  10.1002_anie.201916483                               Missing  Missing\n",
       "4  10.1002_anie.202005531                               Missing  Missing\n",
       "5  10.1002_anie.202007717                               Missing  Missing\n",
       "6  10.1002_chem.201902131                               Missing  Missing\n",
       "7  10.1002_chem.201903120                               Missing  Missing\n",
       "8  10.1002_chem.202000235                               Missing  Missing\n",
       "9  10.1002_chem.202001412                               Missing  Missing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expanded_df = expand_evaluation_results(processed_articles_df)\n",
    "expanded_df.to_csv(os.path.join(BASE_OUTPUT_DIR, 'expanded_eval_results.csv'), index=False)\n",
    "print(f\"Expanded Eval DataFrame shape: {expanded_df.shape}\")\n",
    "display(expanded_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:39:20.044565Z",
     "iopub.status.busy": "2025-07-09T17:39:20.044004Z",
     "iopub.status.idle": "2025-07-09T17:39:20.054079Z",
     "shell.execute_reply": "2025-07-09T17:39:20.053459Z",
     "shell.execute_reply.started": "2025-07-09T17:39:20.044541Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e831c7b9-c7c1-45d2-b4fa-870c3a637318",
       "rows": [
        [
         "Secondary",
         "17"
        ],
        [
         "Primary",
         "8"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "type\n",
       "Secondary    17\n",
       "Primary       8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "submission_df = prepare_for_submission(expanded_df)\n",
    "submission_df.to_csv(SUBMISSION_CSV_PATH, index=False)\n",
    "\n",
    "submission_df[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-09T17:39:35.949520Z",
     "iopub.status.busy": "2025-07-09T17:39:35.948813Z",
     "iopub.status.idle": "2025-07-09T17:39:35.964369Z",
     "shell.execute_reply": "2025-07-09T17:39:35.963661Z",
     "shell.execute_reply.started": "2025-07-09T17:39:35.949497Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 5\n",
      "FP: 20\n",
      "FN: 9\n",
      "F1 Score: 0.256\n"
     ]
    }
   ],
   "source": [
    "def f1_score(tp, fp, fn):\n",
    "    return 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n",
    "    \n",
    "    \n",
    "# if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "pred_df = submission_df.copy()\n",
    "label_df = pd.read_csv(\"./kaggle/input/make-data-count-finding-data-references/sample_submission.csv\")\n",
    "label_df = label_df[label_df['type'] != 'Missing'].reset_index(drop=True)\n",
    "\n",
    "hits_df = label_df.merge(pred_df, on=[\"article_id\", \"dataset_id\", \"type\"])\n",
    "\n",
    "tp = hits_df.shape[0]\n",
    "fp = pred_df.shape[0] - tp\n",
    "fn = label_df.shape[0] - tp\n",
    "\n",
    "print(\"TP:\", tp)\n",
    "print(\"FP:\", fp)\n",
    "print(\"FN:\", fn)\n",
    "print(\"F1 Score:\", round(f1_score(tp, fp, fn), 3))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12656064,
     "sourceId": 82370,
     "sourceType": "competition"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301506,
     "sourceId": 363124,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301511,
     "sourceId": 363131,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
