{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13738e80",
   "metadata": {},
   "source": [
    "# Simple EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0288edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Imports\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import lxml.etree as etree\n",
    "from lxml.etree import _Element as Element # Type hinting for lxml.etree.Element\n",
    "import fitz # PyMuPDF for PDF processing\n",
    "import spacy\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import List, Set, Dict, Union\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f49b7b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for file paths and model configurations\n",
    "BASE_INPUT_DIR = './kaggle/input/make-data-count-finding-data-references'\n",
    "ARTICLE_TRAIN_DIR = os.path.join(BASE_INPUT_DIR, 'train')\n",
    "ARTICLE_TEST_DIR = os.path.join(BASE_INPUT_DIR, 'test')\n",
    "\n",
    "# Define directories for articles in train and test sets\n",
    "LABELED_TRAINING_DATA_CSV_PATH = os.path.join(BASE_INPUT_DIR, 'train_labels.csv')\n",
    "\n",
    "# Define the base model path\n",
    "#QWEN_BASE_MODEL_PATH = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "\n",
    "# Output directory for the fine-tuned model and results\n",
    "BASE_OUTPUT_DIR = \"./kaggle/working\"\n",
    "FINE_TUNED_MODEL_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, \"qwen_finetuned_dataset_classifier\")\n",
    "FINAL_RESULTS_CSV_PATH = os.path.join(BASE_OUTPUT_DIR, \"article_dataset_classification.csv\")\n",
    "\n",
    "# Load a spaCy model (e.g., 'en_core_web_sm')\n",
    "# python -m spacy download en_core_web_sm \n",
    "NLP_SPACY = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3e67ae",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d3d5266",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetCitation:\n",
    "    dataset_ids: Set[str] = field(default_factory=set)  # Set to store unique dataset IDs\n",
    "    citation_context: str = \"\"\n",
    "\n",
    "    def add_dataset_id(self, dataset_id: str):\n",
    "        self.dataset_ids.add(dataset_id)\n",
    "\n",
    "    def add_citation_context(self, context: str):\n",
    "        \"\"\"Adds a citation context to the dataset citation.\"\"\"\n",
    "        if context:\n",
    "            context = context.replace('\\n', '').replace('[', '').replace(']', '')\n",
    "            context = re.sub(r'\\s+', ' ', context.strip())\n",
    "            self.citation_context += context\n",
    "\n",
    "    def has_dataset(self) -> bool:\n",
    "        \"\"\"Returns True if there is both dataset IDs and citation context.\"\"\"\n",
    "        return bool(self.dataset_ids and self.citation_context.strip())\n",
    "\n",
    "@dataclass\n",
    "class ArticleData:\n",
    "    article_id: str = \"\"\n",
    "    article_doi: str = \"\"\n",
    "    title: str = \"\"\n",
    "    author: str = \"\"\n",
    "    abstract: str = \"\"\n",
    "    datasets: Set[str] = field(default_factory=set)  # Set to store unique dataset IDs\n",
    "    citation_context: str = \"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Custom initialization\n",
    "        if self.article_id and not self.article_doi:\n",
    "            # If article_id is provided but not article_doi, set article_doi\n",
    "            self.article_doi = self.article_id.replace(\"_\", \"/\").lower()\n",
    "\n",
    "    def add_dataset(self, dataset_id: str):\n",
    "        \"\"\"Adds a dataset citation to the article.\"\"\"\n",
    "        self.datasets.add(dataset_id)\n",
    "\n",
    "    def add_dataset_citation(self, dataset_citation: DatasetCitation):\n",
    "        \"\"\"Adds a dataset citation context to the article.\"\"\"\n",
    "        if dataset_citation.has_dataset():\n",
    "            self.citation_context += dataset_citation.citation_context + \"||\"\n",
    "            for dataset_id in dataset_citation.dataset_ids:\n",
    "                self.add_dataset(dataset_id)\n",
    "        \n",
    "    def to_dict(self):\n",
    "        d = asdict(self)\n",
    "        d[\"datasets\"] = list(self.datasets)\n",
    "        return d\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), separators=(',', ':'))\n",
    "\n",
    "    def has_data(self) -> bool:\n",
    "        \"\"\"Returns True if there is any data availability or dataset citation.\"\"\"\n",
    "        return bool(self.datasets or self.citation_context.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500100f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Information Extraction (IE) - Dataset Identification ---\n",
    "NON_STD_UNICODE_DASHES = re.compile(r'[\\u2010\\u2011\\u2012\\u2013\\u2014]')\n",
    "NON_STD_UNICODE_TICKS = re.compile(r'[\\u201c\\u201d]')\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input text by removing non-standard unicode dashes and extra whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Replace all non-standard unicode dashes with '-'\n",
    "    text = text.replace('\\u200b', '').replace('-\\n', '-').replace('_\\n', '_').replace('/\\n', '/')\n",
    "    text = NON_STD_UNICODE_DASHES.sub('-', text)\n",
    "    text = NON_STD_UNICODE_TICKS.sub(\"'\", text)\n",
    "    # Remove extra whitespace\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Regex patterns for common dataset identifiers\n",
    "# DOI_PATTERN = r'10\\.\\d{4,5}/[-._;()/:A-Za-z0-9\\u002D\\u2010\\u2011\\u2012\\u2013\\u2014\\u2015]+'\tDOI_PATTERN\n",
    "# DOI_PATTERN = r'10\\.\\s?\\d{4,5}\\/[-._()<>;\\/:A-Za-z0-9]+\\s?(?:(?![A-Z]+)(?!\\d{1,3}\\.))+[-._()<>;\\/:A-Za-z0-9]+'\n",
    "#DOI_PATTERN = r'\\bhttps://doi.org/10\\.\\d{4,5}\\/[-._\\/:A-Za-z0-9]+'\n",
    "DOI_PATTERN = r'\\b10\\.\\d{4,5}\\/[-._\\/:A-Za-z0-9]+'\n",
    "EPI_PATTERN = r'\\bEPI[-_A-Z0-9]{2,}'\n",
    "SAM_PATTERN = r'\\bSAMN[0-9]{2,}'          # SAMN07159041\n",
    "IPR_PATTERN = r'\\bIPR[0-9]{2,}'\n",
    "CHE_PATTERN = r'\\bCHEMBL[0-9]{2,}'\n",
    "PRJ_PATTERN = r'\\bPRJ[A-Z0-9]{2,}'\n",
    "E_G_PATTERN = r'\\bE-[A-Z]{4}-[0-9]{2,}'   # E-GEOD-19722 or E-PROT-100\n",
    "ENS_PATTERN = r'\\bENS[A-Z]{4}[0-9]{2,}'\n",
    "CVC_PATTERN = r'\\bCVCL_[A-Z0-9]{2,}'\n",
    "EMP_PATTERN = r'\\bEMPIAR-[0-9]{2,}'\n",
    "PXD_PATTERN = r'\\bPXD[0-9]{2,}'\n",
    "HPA_PATTERN = r'\\bHPA[0-9]{2,}'\n",
    "SRR_PATTERN = r'\\bSRR[0-9]{2,}'\n",
    "GSE_PATTERN = r'\\b(GSE|GSM|GDS|GPL)\\d{4,6}\\b' # Example for GEO accession numbers (e.g., GSE12345, GSM12345)\n",
    "GNB_PATTERN = r'\\b[A-Z]{1,2}\\d{5,6}\\b' # GenBank accession numbers (e.g., AB123456, AF000001)\n",
    "CAB_PATTERN = r'\\bCAB[0-9]{2,}'\n",
    "\n",
    "# Combine all patterns into a list\n",
    "DATASET_ID_PATTERNS = [\n",
    "    DOI_PATTERN,\n",
    "    EPI_PATTERN,\n",
    "    SAM_PATTERN,\n",
    "    IPR_PATTERN,\n",
    "    CHE_PATTERN,\n",
    "    PRJ_PATTERN,\n",
    "    E_G_PATTERN,\n",
    "    ENS_PATTERN,\n",
    "    CVC_PATTERN,\n",
    "    EMP_PATTERN,\n",
    "    PXD_PATTERN,\n",
    "    HPA_PATTERN,\n",
    "    SRR_PATTERN,\n",
    "    GSE_PATTERN,\n",
    "    GNB_PATTERN,\n",
    "    CAB_PATTERN,\n",
    "]\n",
    "\n",
    "# Compile all patterns for efficiency\n",
    "COMPILED_DATASET_ID_REGEXES = [re.compile(p) for p in DATASET_ID_PATTERNS]\n",
    "\n",
    "# Data related keywords to look for in the text\n",
    "# These keywords help to ensure that the text is relevant to datasets\n",
    "DATA_RELATED_KEYWORDS = ['data release', 'data associated', 'data availability', 'data access', 'download', 'program data', 'the data', 'dataset', 'database', 'repository', 'data source', 'data access', 'archive', 'arch.', 'digital']\n",
    "\n",
    "def is_text_data_related(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in DATA_RELATED_KEYWORDS)\n",
    "\n",
    "def extract_dataset_citation(text: str, context_chars: int = 250) -> DatasetCitation:\n",
    "    \"\"\"\n",
    "    Extract dataset identifiers with context from the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to search for dataset identifiers.\n",
    "        context_chars (int): Number of characters to include before and after the match for context.\n",
    "        \n",
    "    Returns:\n",
    "        DatasetCitation: with a list of extracted dataset identifiers with context.\n",
    "    \"\"\"\n",
    "    dataset_citation = DatasetCitation()\n",
    "    if not text:\n",
    "        return dataset_citation\n",
    "\n",
    "    text = clean_text(text)\n",
    "    if is_text_data_related(text):\n",
    "        dataset_citation.add_citation_context(text)\n",
    "        for regex in COMPILED_DATASET_ID_REGEXES:\n",
    "            matches = regex.finditer(text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                dataset_id = text[match.start() : match.end()]\n",
    "                dataset_citation.add_dataset_id(dataset_id)\n",
    "    \n",
    "    return dataset_citation\n",
    "\n",
    "# Use NLP to get sentences from the given text\n",
    "def get_sentences_from_text(text: str, nlp=NLP_SPACY) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Replace all non-standard unicode dashes with '-'\n",
    "    text = clean_text(text)\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    doc_spacy = nlp(text)\n",
    "    return \" \".join([sent.text for sent in doc_spacy.sents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
