{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2db1bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- 0. Environment Setup & Offline Preparation ---\n",
    "\n",
    "# Standard Imports\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections # For deque in parenthesis removal\n",
    "import fitz # PyMuPDF for PDF processing\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "from datasets import Dataset # Hugging Face datasets library\n",
    "import kagglehub\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = \"cuda\" if torch and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e263db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for file paths and model configurations\n",
    "BASE_INPUT_DIR = './kaggle/input/make-data-count-finding-data-references'\n",
    "ARTICLE_TRAIN_DIR = os.path.join(BASE_INPUT_DIR, 'train')\n",
    "ARTICLE_TEST_DIR = os.path.join(BASE_INPUT_DIR, 'test')\n",
    "\n",
    "# Define directories for articles in train and test sets\n",
    "LABELED_TRAINING_DATA_CSV_PATH = os.path.join(BASE_INPUT_DIR, 'train_labels.csv')\n",
    "\n",
    "# Output directory for the fine-tuned model and results\n",
    "BASE_OUTPUT_DIR = \"./kaggle/working\"\n",
    "FINE_TUNED_MODEL_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, \"qwen_finetuned_dataset_classifier\")\n",
    "FINAL_RESULTS_CSV_PATH = os.path.join(BASE_OUTPUT_DIR, \"article_dataset_classification.csv\")\n",
    "\n",
    "inference_model = None\n",
    "inference_tokenizer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a58f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading ---\n",
    "def load_file_paths(dataset_type_dir: str) -> pd.DataFrame: \n",
    "    pdf_path = os.path.join(dataset_type_dir, 'PDF')\n",
    "    xml_path = os.path.join(dataset_type_dir, 'XML')\n",
    "    dataset_type = os.path.basename(dataset_type_dir)\n",
    "    pdf_files = [f for f in os.listdir(pdf_path) if f.endswith('.pdf')]\n",
    "    xml_files = [f for f in os.listdir(xml_path) if f.endswith('.xml')]\n",
    "    df_pdf = pd.DataFrame({\n",
    "        'article_id': [f.replace('.pdf', '') for f in pdf_files],\n",
    "        'pdf_file_path': [os.path.join(pdf_path, f) for f in pdf_files]\n",
    "    })\n",
    "    df_xml = pd.DataFrame({\n",
    "        'article_id': [f.replace('.xml', '') for f in xml_files],\n",
    "        'xml_file_path': [os.path.join(xml_path, f) for f in xml_files]\n",
    "    })\n",
    "    merge_df = pd.merge(df_pdf, df_xml, on='article_id', how='outer', suffixes=('_pdf', '_xml'), validate=\"one_to_many\")\n",
    "    merge_df['dataset_type'] = dataset_type\n",
    "    return merge_df\n",
    "\n",
    "def read_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts all text from a PDF file using PyMuPDF.\"\"\"\n",
    "    text = \"\"\n",
    "    if not fitz:\n",
    "        return text  # Return empty string if fitz is not available\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_textpage().extractTEXT().replace('\\u200b', '').strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF {pdf_path}: {e}\")\n",
    "        \n",
    "    return text\n",
    "\n",
    "def read_xml_text(xml_file_path: str) -> str:\n",
    "    \"\"\"Reads and concatenates all text content from an XML file.\"\"\"\n",
    "    all_text_parts = []\n",
    "    try:\n",
    "        tree = ET.parse(xml_file_path)\n",
    "        root = tree.getroot()\n",
    "        for element in root.iter():\n",
    "            if element.text:\n",
    "                cleaned_text = element.text.strip()\n",
    "                if cleaned_text:\n",
    "                    all_text_parts.append(cleaned_text)\n",
    "            if element.tail:\n",
    "                cleaned_tail = element.tail.strip()\n",
    "                if cleaned_tail:\n",
    "                    all_text_parts.append(cleaned_tail)\n",
    "        return \" \".join(all_text_parts) if all_text_parts else \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading XML {xml_file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_article_text(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Loads text content from a single article file (PDF or XML).\n",
    "    Returns the text content of the given file.\n",
    "    \"\"\"\n",
    "    article_id = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    text_content = \"\"\n",
    "\n",
    "    if filepath.endswith(\".pdf\"):\n",
    "        text_content = read_pdf_text(filepath)\n",
    "    elif filepath.endswith(\".xml\"):\n",
    "        text_content = read_xml_text(filepath)\n",
    "\n",
    "    return text_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d7f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labeled training data from: ./kaggle/input/make-data-count-finding-data-references\\train_labels.csv\n",
      "Training labels shape: (1028, 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "has_dataset",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "b74ad255-9057-4c10-974a-cdbb50c5db69",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "https://doi.org/10.17882/49388",
         "Primary",
         "True"
        ],
        [
         "1",
         "10.1002_anie.201916483",
         "Missing",
         "Missing",
         "False"
        ],
        [
         "2",
         "10.1002_anie.202005531",
         "Missing",
         "Missing",
         "False"
        ],
        [
         "3",
         "10.1002_anie.202007717",
         "Missing",
         "Missing",
         "False"
        ],
        [
         "4",
         "10.1002_chem.201902131",
         "Missing",
         "Missing",
         "False"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>type</th>\n",
       "      <th>has_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/49388</td>\n",
       "      <td>Primary</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1002_anie.202007717</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1002_chem.201902131</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                      dataset_id     type  \\\n",
       "0    10.1002_2017jc013030  https://doi.org/10.17882/49388  Primary   \n",
       "1  10.1002_anie.201916483                         Missing  Missing   \n",
       "2  10.1002_anie.202005531                         Missing  Missing   \n",
       "3  10.1002_anie.202007717                         Missing  Missing   \n",
       "4  10.1002_chem.201902131                         Missing  Missing   \n",
       "\n",
       "   has_dataset  \n",
       "0         True  \n",
       "1        False  \n",
       "2        False  \n",
       "3        False  \n",
       "4        False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the labeled training data CSV file\n",
    "print(f\"Loading labeled training data from: {LABELED_TRAINING_DATA_CSV_PATH}\")\n",
    "train_labels_df = pd.read_csv(LABELED_TRAINING_DATA_CSV_PATH)\n",
    "train_labels_df['has_dataset'] = train_labels_df['dataset_id'] != 'Missing'\n",
    "\n",
    "print(f\"Training labels shape: {train_labels_df.shape}\")\n",
    "display(train_labels_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f61a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files paths shape: (524, 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "77175096-9fba-429d-a96d-e81e7d7cdef3",
       "rows": [
        [
         "272",
         "10.1186_s12885-018-4314-9",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12885-018-4314-9.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\train\\XML\\10.1186_s12885-018-4314-9.xml",
         "train"
        ],
        [
         "440",
         "10.1590_1981-52712015v43n3rb20180090",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1981-52712015v43n3rb20180090.pdf",
         null,
         "train"
        ],
        [
         "295",
         "10.1186_s12935-020-01373-x",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12935-020-01373-x.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\train\\XML\\10.1186_s12935-020-01373-x.xml",
         "train"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>10.1186_s12885-018-4314-9</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>10.1590_1981-52712015v43n3rb20180090</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>10.1186_s12935-020-01373-x</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               article_id  \\\n",
       "272             10.1186_s12885-018-4314-9   \n",
       "440  10.1590_1981-52712015v43n3rb20180090   \n",
       "295            10.1186_s12935-020-01373-x   \n",
       "\n",
       "                                         pdf_file_path  \\\n",
       "272  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "440  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "295  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                         xml_file_path dataset_type  \n",
       "272  ./kaggle/input/make-data-count-finding-data-re...        train  \n",
       "440                                                NaN        train  \n",
       "295  ./kaggle/input/make-data-count-finding-data-re...        train  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test files paths shape: (30, 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "2789863c-c3c8-4844-9c3f-53efbf5041cb",
       "rows": [
        [
         "21",
         "10.1002_ecs2.4619",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.4619.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ecs2.4619.xml",
         "test"
        ],
        [
         "28",
         "10.1002_nafm.10870",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_nafm.10870.pdf",
         null,
         "test"
        ],
        [
         "25",
         "10.1002_esp.5058",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_esp.5058.pdf",
         null,
         "test"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.1002_ecs2.4619</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.1002_nafm.10870</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.1002_esp.5058</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            article_id                                      pdf_file_path  \\\n",
       "21   10.1002_ecs2.4619  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "28  10.1002_nafm.10870  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "25    10.1002_esp.5058  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                        xml_file_path dataset_type  \n",
       "21  ./kaggle/input/make-data-count-finding-data-re...         test  \n",
       "28                                                NaN         test  \n",
       "25                                                NaN         test  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load file paths for training and testing datasets\n",
    "train_file_paths_df = load_file_paths(ARTICLE_TRAIN_DIR)\n",
    "test_file_paths_df = load_file_paths(ARTICLE_TEST_DIR)\n",
    "\n",
    "print(f\"Train files paths shape: {train_file_paths_df.shape}\")\n",
    "display(train_file_paths_df.sample(3))\n",
    "print(f\"Test files paths shape: {test_file_paths_df.shape}\")\n",
    "display(test_file_paths_df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e6b4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen model and tokenizer from: C:\\Users\\jim\\.cache\\kagglehub\\models\\qwen-lm\\qwen-3\\transformers\\0.6b\\1\n",
      "thinking content: <think>\n",
      "Okay, the user wants a short introduction to a large language model. Let me start by recalling what I know about LLMs. They are large language models, right? So I should mention their size and capabilities. Maybe start with \"Large language models\" as the main term. Then explain that they're trained on vast amounts of text, so they can understand and generate a lot of text.\n",
      "\n",
      "I should highlight their ability to understand and generate human-like text. Maybe mention specific examples, like answering questions or writing creative content. Also, it's important to note that they can work with various tasks, not just text. Oh, and maybe touch on their applications in fields like customer service, content creation, etc.\n",
      "\n",
      "Wait, the user might be a student or someone new to the topic. They need a concise yet informative introduction. I should keep it short, maybe two to three sentences. Avoid technical jargon if possible. Make sure to include key points without getting too detailed. Let me check if I covered all the main aspects: size, training data, capabilities, applications, and maybe an example. Yeah, that should work. Let me put it all together now.\n",
      "</think>\n",
      "content: A large language model (LLM) is a type of artificial intelligence designed to understand and generate human-like text. They are trained on vast datasets to comprehend complex language, enabling them to answer questions, write creative content, and perform various tasks like customer service or content creation. LLMs are powered by massive training corpora and can process and generate text in real-time.\n"
     ]
    }
   ],
   "source": [
    "# Global variables for LLM components (loaded once)\n",
    "llm_tokenizer = None\n",
    "llm_model = None\n",
    "\n",
    "# --- Load the Qwen Model and Tokenizer---\n",
    "model_name = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "print(f\"Loading Qwen model and tokenizer from: {model_name}\")\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "llm_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = llm_tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = llm_tokenizer([text], return_tensors=\"pt\").to(llm_model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = llm_model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 ()\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "\n",
    "thinking_content = llm_tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = llm_tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1390e9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcbe2522",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QwenChatbotSimple:\n",
    "    \"\"\" A simple chatbot class using the Qwen model for generating responses.\n",
    "    It maintains a history of messages to provide context for the conversation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.history = []\n",
    "\n",
    "    def generate_response(self, user_input):\n",
    "        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n",
    "\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\")\n",
    "        response_ids = self.model.generate(**inputs, max_new_tokens=32768)[0][len(inputs.input_ids[0]):].tolist()\n",
    "        response = self.tokenizer.decode(response_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Update history\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2640c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QwenChatbot Class ---\n",
    "class QwenChatbot:\n",
    "    def __init__(self, model_name = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\", trust_remote_code=True)\n",
    "        self.model.eval() # Set the model to evaluation mode here as well.\n",
    "        self.history = []\n",
    "\n",
    "    def generate_response(self, user_input, enable_thinking=True, max_new_tokens=512):  # Reduce max_new_tokens\n",
    "        messages = self.history + [{\"role\": \"user\", \"content\": user_input}]\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=enable_thinking\n",
    "        )\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "        with torch.no_grad(): # Disable gradient calculation during inference\n",
    "            generated_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,  # Use the reduced value\n",
    "                temperature=0.6 if enable_thinking else 0.7,\n",
    "                top_p=0.95 if enable_thinking else 0.8,\n",
    "                top_k=20,\n",
    "                min_p=0\n",
    "            )\n",
    "\n",
    "        output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist()\n",
    "\n",
    "        #parsing thinking content\n",
    "        try:\n",
    "            # rindex finding 151668 ()\n",
    "            index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        thinking_content = self.tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "        content = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "        response = content\n",
    "\n",
    "        # Update history\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "        return response, thinking_content\n",
    "    \n",
    "    def clear_history(self):\n",
    "        self.history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f2525aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot = QwenChatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8bee422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: How many r's in strawberries?\n",
      "Bot: ('How many r\\'s are in the word \"strawberries\"? Let\\'s break it down:\\n\\n- S, T, R, A, W, B, E, R, R, I, N, G, S.  \\n\\nThe letters \\'r\\' appear at positions 3 and', '<think>\\nOkay, so the user is asking, \"How many r\\'s in strawberries?\" Let me think about this. First, I need to check how many times the letter \\'r\\' appears in the word \"strawberries\". Let me write it out: S-T-R-A-W-B-E-R-R-I-N-G-S. Now, I\\'ll go through each letter one by one.\\n\\nStarting with the first letter, S. No \\'r\\'s here. Then T. Still no. R! That\\'s the first \\'r\\'. Next, A. Then W, B, E, R, R, I, N, G. So here, there are two more \\'r\\'s. Let me count again: positions 3 and 6. So total of 3 \\'r\\'s. Wait, is that correct? Let me double-check. The word is S-T-R-A-W-B-E-R-R-I-N-G-S. Yes, the letters are S-T-R-A-W-B-E-R-R-I-N-G-S. So the \\'r\\'s are at the 3rd and 6th positions. So that\\'s two \\'r\\'s. Wait, but I thought maybe three? Let me count again. S (no), T (no), R (yes, first), A (no), W (no), B (no), E (no), R (second), R (third), I (no), N (no), G (no), S (no). So that\\'s two \\'r\\'s. Wait, maybe I made a mistake here. Let me write it out with positions:\\n\\n1: S\\n\\n2: T\\n\\n3: R\\n\\n4: A\\n\\n5: W\\n\\n6: B\\n\\n7: E\\n\\n8: R\\n\\n9: R\\n\\n10: I\\n\\n11: N\\n\\n12: G\\n\\n13: S\\n\\nSo positions 3 and 8, 9. So two \\'r\\'s. So the answer should be two. But wait, maybe the user is thinking of the entire word as strawberries, which is spelled S-T-R-A-W-B-E-R-R-I-N-G-S. So yes, two \\'r\\'s. So the answer is 2.\\n</think>')\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage of QwenChatbot ---\n",
    "\n",
    "# First input (without /think or /no_think tags, thinking mode is enabled by default)\n",
    "user_input_1 = \"How many r's in strawberries?\"\n",
    "print(f\"User: {user_input_1}\")\n",
    "response_1 = chatbot.generate_response(user_input_1)\n",
    "print(f\"Bot: {response_1}\")\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "085559a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('How many r\\'s are in the word \"strawberries\"? Let\\'s break it down:\\n\\n- S, T, R, A, W, B, E, R, R, I, N, G, S.  \\n\\nThe letters \\'r\\' appear at positions 3 and',\n",
       " '<think>\\nOkay, so the user is asking, \"How many r\\'s in strawberries?\" Let me think about this. First, I need to check how many times the letter \\'r\\' appears in the word \"strawberries\". Let me write it out: S-T-R-A-W-B-E-R-R-I-N-G-S. Now, I\\'ll go through each letter one by one.\\n\\nStarting with the first letter, S. No \\'r\\'s here. Then T. Still no. R! That\\'s the first \\'r\\'. Next, A. Then W, B, E, R, R, I, N, G. So here, there are two more \\'r\\'s. Let me count again: positions 3 and 6. So total of 3 \\'r\\'s. Wait, is that correct? Let me double-check. The word is S-T-R-A-W-B-E-R-R-I-N-G-S. Yes, the letters are S-T-R-A-W-B-E-R-R-I-N-G-S. So the \\'r\\'s are at the 3rd and 6th positions. So that\\'s two \\'r\\'s. Wait, but I thought maybe three? Let me count again. S (no), T (no), R (yes, first), A (no), W (no), B (no), E (no), R (second), R (third), I (no), N (no), G (no), S (no). So that\\'s two \\'r\\'s. Wait, maybe I made a mistake here. Let me write it out with positions:\\n\\n1: S\\n\\n2: T\\n\\n3: R\\n\\n4: A\\n\\n5: W\\n\\n6: B\\n\\n7: E\\n\\n8: R\\n\\n9: R\\n\\n10: I\\n\\n11: N\\n\\n12: G\\n\\n13: S\\n\\nSo positions 3 and 8, 9. So two \\'r\\'s. So the answer should be two. But wait, maybe the user is thinking of the entire word as strawberries, which is spelled S-T-R-A-W-B-E-R-R-I-N-G-S. So yes, two \\'r\\'s. So the answer is 2.\\n</think>')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e5a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_prompt =  '''\n",
    "You are an advanced AI reasoning assistant tasked with delivering a comprehensive analysis of a specific problem or question.  Your goal is to outline your reasoning process in a structured and transparent manner, with each step reflecting a thorough examination of the issue at hand, culminating in a well-reasoned conclusion.\n",
    "\n",
    "### Key Instructions:\n",
    "1.  Conduct **at least 5 distinct reasoning steps**, each building on the previous one.\n",
    "2.  **Acknowledge the limitations** inherent to AI, specifically what you can accurately assess and what you may struggle with.\n",
    "3.  **Adopt multiple reasoning frameworks** to resolve the problem or derive conclusions, such as:\n",
    "- **Deductive reasoning** (drawing specific conclusions from general principles)\n",
    "- **Inductive reasoning** (deriving broader generalizations from specific observations)\n",
    "- **Abductive reasoning** (choosing the best possible explanation for the given evidence)\n",
    "- **Analogical reasoning** (solving problems through comparisons and analogies)\n",
    "4.  **Critically analyze your reasoning** to identify potential flaws, biases, or gaps in logic.\n",
    "5.  When reviewing, apply a **fundamentally different perspective or approach** to enhance your analysis.\n",
    "6.  **Employ at least 2 distinct reasoning methods** to derive or verify the accuracy of your conclusions.\n",
    "7.  **Incorporate relevant domain knowledge** and **best practices** where applicable, ensuring your reasoning aligns with established standards.\n",
    "8.  **Quantify certainty levels** for each step and your final conclusion, where applicable.\n",
    "9.  Consider potential **edge cases or exceptions** that could impact the outcome of your reasoning.\n",
    "10.  Provide **clear justifications** for dismissing alternative hypotheses or solutions that arise during your analysis.\n",
    "'''\n",
    "\n",
    "task_prompt = '''\n",
    "You are given a piece of academic text. Your task is to identify the single DOI citation string, if present.\n",
    "Then normalize it into its full URL format: https://doi.org/...\n",
    "\n",
    "Each object (paper and dataset) has a unique, persistent identifier to represent it. In this competition there will be two types:\n",
    "\n",
    "the definition of dataset_id is as follows:\n",
    "the dataset identifier and citation type in the paper.\n",
    "DOIs are used for all papers and some datasets. They take the following form: https://doi.org/[prefix]/[suffix]. Examples:\n",
    "https://doi.org/10.1371/journal.pone.0303785\n",
    "https://doi.org/10.5061/dryad.r6nq870\n",
    "Accession IDs are used for some datasets. They vary in form by individual data repository where the data live. Examples:\n",
    "\"GSE12345\" (Gene Expression Omnibus dataset)\n",
    "“PDB 1Y2T” (Protein Data Bank dataset)\n",
    "\"E-MEXP-568\" (ArrayExpress dataset)\n",
    "\n",
    "the definition of type is as follows:\n",
    "the type citation type, Primary - raw or processed data generated as part of this paper, specifically for this study\n",
    "Secondary - raw or processed data derived or reused from existing records or published data\n",
    "'''\n",
    "\n",
    "example_prompt = '''\n",
    "use markdown json style to get the result as the following examples:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"dataset_id\": \"https://doi.org/10.1371/journal.pone.0303785\",\n",
    "        \"type\": \"Primary\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_id\": \"https://doi.org/10.1371/journal.pone.0303785\",\n",
    "        \"type\": \"Secondary\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_id\": \"GSE12345\",\n",
    "        \"type\": \"Secondary\"\n",
    "    },\n",
    "    {\n",
    "        \"dataset_id\": \"Missing\",\n",
    "        \"type\": \"Missing, \"\n",
    "    }\n",
    "    ...\n",
    "]\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Data Preparation for LLM Training (Revised for Combined Task) ---\n",
    "\n",
    "def load_base_llm_for_training():\n",
    "    \"\"\"Loads the base Qwen model and tokenizer for fine-tuning.\"\"\"\n",
    "    global llm_tokenizer, llm_model\n",
    "    if not AutoModelForCausalLM or not QWEN_BASE_MODEL_PATH:\n",
    "        print(\"LLM components not available or base model path not set. Skipping LLM loading.\")\n",
    "        return False\n",
    "    try:\n",
    "        print(f\"Loading Qwen tokenizer from: {QWEN_BASE_MODEL_PATH}\")\n",
    "        llm_tokenizer = AutoTokenizer.from_pretrained(QWEN_BASE_MODEL_PATH, trust_remote_code=True)\n",
    "        if llm_tokenizer.pad_token is None:\n",
    "            llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "            print(\"Set tokenizer.pad_token to tokenizer.eos_token\")\n",
    "\n",
    "        print(f\"Loading Qwen model from: {QWEN_BASE_MODEL_PATH}\")\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "            QWEN_BASE_MODEL_PATH,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32,\n",
    "            device_map=\"auto\", # Automatically uses GPU if available\n",
    "            trust_remote_code=True,\n",
    "            # load_in_8bit=True if bnb else False # Uncomment if bitsandbytes is used\n",
    "        )\n",
    "        print(f\"Base LLM loaded successfully on {llm_model.device}.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading base LLM for training: {e}\")\n",
    "        llm_tokenizer, llm_model = None, None # Reset to None on failure\n",
    "        return False\n",
    "\n",
    "def prepare_training_data_for_llm(\n",
    "    training_df: pd.DataFrame,\n",
    "    all_article_texts: dict[str, str],\n",
    "    tokenizer_max_length: int\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Prepares training data for LLM fine-tuning, aggregating dataset IDs and classifications\n",
    "    per article and formatting into ChatML JSON output.\n",
    "    \"\"\"\n",
    "    formatted_examples = []\n",
    "\n",
    "    # Group training data by article_id to get all datasets for each article\n",
    "    # This creates a dictionary where keys are article_ids and values are lists of dataset dicts\n",
    "    grouped_training_data = training_df.groupby('article_id').apply(\n",
    "        lambda x: [{\"dataset_id\": row['dataset_id'], \"classification\": row['label']} for _, row in x]\n",
    "    ).to_dict()\n",
    "\n",
    "    # Get all article IDs for which we have text content\n",
    "    all_article_ids_with_text = set(all_article_texts.keys())\n",
    "    \n",
    "    # Iterate through all articles for which we have text (these are our potential training examples)\n",
    "    for article_id in all_article_ids_with_text:\n",
    "        article_text = all_article_texts.get(article_id, \"\")\n",
    "        if not article_text:\n",
    "            print(f\"Warning: Article text for {article_id} not found. Skipping training example.\")\n",
    "            continue\n",
    "\n",
    "        # Truncate article text to fit within the model's context window\n",
    "        # Reserve tokens for the prompt and the expected JSON response.\n",
    "        # A typical Qwen 1.5 model has 32768 max_seq_length.\n",
    "        # 512 tokens for prompt/response is a safe estimate.\n",
    "        truncated_article_text = article_text[:tokenizer_max_length - 512] \n",
    "\n",
    "        # Determine the ground truth output for this article\n",
    "        if article_id in grouped_training_data:\n",
    "            # Article has datasets, format them as JSON\n",
    "            ground_truth_datasets = grouped_training_data[article_id]\n",
    "            assistant_response_json = json.dumps(ground_truth_datasets, ensure_ascii=False)\n",
    "        else:\n",
    "            # Article has no datasets in training data, so the model should output an empty list.\n",
    "            # This explicitly trains the model to output '[]' for \"Missing\" cases.\n",
    "            assistant_response_json = \"[]\"\n",
    "            # print(f\"Info: Article {article_id} has no datasets in training data. Training to output '[]'.\")\n",
    "\n",
    "        # Construct the user message for the LLM\n",
    "        user_message = f\"\"\"\n",
    "Article Text:\n",
    "{truncated_article_text}\n",
    "\n",
    "Task: Identify all datasets or databases used in this research article and classify each as \"Primary\" (if created by the authors for this research) or \"Secondary\" (if an existing dataset used in this research).\n",
    "\n",
    "Output Format: Provide a JSON list of objects. Each object should have \"dataset_id\" and \"classification\" keys. If no datasets are identified, return an empty JSON list: [].\n",
    "\"\"\"\n",
    "        # Construct the full ChatML formatted string for SFTTrainer\n",
    "        # The trainer will use this entire string as the 'text' field.\n",
    "        chatml_formatted_string = f\"<|im_start|>system\\nYou are an expert research assistant. Your task is to extract and classify datasets from scientific articles.<|im_end|>\\n<|im_start|>user\\n{user_message.strip()}<|im_end|>\\n<|im_start|>assistant\\n{assistant_response_json}<|im_end|>\"\n",
    "        \n",
    "        formatted_examples.append({\"text\": chatml_formatted_string})\n",
    "\n",
    "    if not formatted_examples:\n",
    "        raise ValueError(\"No training examples could be prepared. Check your data and article texts.\")\n",
    "\n",
    "    return Dataset.from_list(formatted_examples)\n",
    "\n",
    "# --- 3. LLM Model Training (Fine-tuning) ---\n",
    "\n",
    "# Attempt to load tokenizer and model if not already loaded (e.g., if previous training failed or was skipped)\n",
    "if llm_model is None:\n",
    "    load_base_llm_for_training()\n",
    "\n",
    "if llm_model and not training_df.empty and Dataset: # Ensure Dataset is imported\n",
    "    print(\"\\n--- Preparing data for Fine-tuning (Combined Task) ---\")\n",
    "    # Use the model's max_length for context, or a reasonable default if tokenizer isn't loaded\n",
    "    max_len = llm_tokenizer.model_max_length if llm_tokenizer else 4096 \n",
    "    train_dataset = prepare_training_data_for_llm(training_df, all_article_texts, max_len)\n",
    "    \n",
    "    print(f\"Prepared {len(train_dataset)} examples for fine-tuning.\")\n",
    "    print(\"Example formatted training instance (first 500 chars):\")\n",
    "    print(train_dataset[0]['text'][:500])\n",
    "\n",
    "    print(\"\\n--- Starting Fine-tuning (Combined Task) ---\")\n",
    "    try:\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{FINE_TUNED_MODEL_OUTPUT_DIR}/checkpoints\",\n",
    "            num_train_epochs=1,  # Start with 1 epoch, adjust as needed\n",
    "            per_device_train_batch_size=1, # Adjust based on VRAM\n",
    "            gradient_accumulation_steps=4, # Effective batch size = 1 * 4 = 4\n",
    "            learning_rate=2e-5,\n",
    "            logging_steps=10,\n",
    "            save_steps=50, # Save checkpoints periodically\n",
    "            fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
    "            bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "            optim=\"paged_adamw_8bit\", # Good for memory efficiency if bitsandbytes is installed\n",
    "            # report_to=\"none\", # Disable logging to external services\n",
    "            # max_steps=100, # For quick testing\n",
    "        )\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model=llm_model,\n",
    "            tokenizer=llm_tokenizer,\n",
    "            train_dataset=train_dataset,\n",
    "            dataset_text_field=\"text\", # This field contains the full ChatML string\n",
    "            args=training_args,\n",
    "            max_seq_length=max_len, # Use the model's full context length\n",
    "            packing=False, # Set to True if your inputs are much shorter than max_seq_length\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        print(\"Fine-tuning completed.\")\n",
    "\n",
    "        print(f\"Saving fine-tuned model to: {FINE_TUNED_MODEL_OUTPUT_DIR}\")\n",
    "        trainer.save_model(FINE_TUNED_MODEL_OUTPUT_DIR)\n",
    "        print(\"Model and tokenizer saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during fine-tuning: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        llm_model = None # Mark model as failed to load/train\n",
    "else:\n",
    "    print(\"Skipping LLM fine-tuning due to missing training data or LLM components.\")\n",
    "\n",
    "\n",
    "# --- 4. LLM-based Extraction & Classification (Inference) ---\n",
    "\n",
    "# Load the fine-tuned model for inference (if training was successful)\n",
    "# If training was skipped or failed, this will attempt to load from the base path or fail.\n",
    "if inference_model is None: # Only load if not already loaded\n",
    "    if AutoModelForCausalLM: # Check if transformers is available\n",
    "        if os.path.exists(FINE_TUNED_MODEL_OUTPUT_DIR) and os.path.isdir(FINE_TUNED_MODEL_OUTPUT_DIR):\n",
    "            MODEL_TO_LOAD = FINE_TUNED_MODEL_OUTPUT_DIR\n",
    "            print(f\"Loading fine-tuned model for inference from: {MODEL_TO_LOAD}\")\n",
    "        else:\n",
    "            MODEL_TO_LOAD = QWEN_BASE_MODEL_PATH\n",
    "            print(f\"Fine-tuned model not found. Loading base model for inference from: {MODEL_TO_LOAD}\")\n",
    "\n",
    "        try:\n",
    "            inference_tokenizer = AutoTokenizer.from_pretrained(MODEL_TO_LOAD, trust_remote_code=True)\n",
    "            if inference_tokenizer.pad_token is None:\n",
    "                inference_tokenizer.pad_token = inference_tokenizer.eos_token\n",
    "            inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_TO_LOAD,\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            ).eval() # Set to evaluation mode\n",
    "            print(f\"Inference LLM loaded successfully on {inference_model.device}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading inference LLM from {MODEL_TO_LOAD}: {e}\")\n",
    "            inference_model, inference_tokenizer = None, None\n",
    "    else:\n",
    "        print(\"Transformers library not available. Cannot load LLM for inference.\")\n",
    "\n",
    "\n",
    "def extract_and_classify_with_llm(article_text: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Uses the loaded LLM to extract dataset IDs and classify them.\n",
    "    Returns a list of dictionaries like [{\"dataset_id\": \"...\", \"classification\": \"...\"}].\n",
    "    Returns an empty list if LLM is unavailable or parsing fails.\n",
    "    \"\"\"\n",
    "    if not inference_model or not inference_tokenizer:\n",
    "        print(\"  LLM unavailable for extraction/classification.\")\n",
    "        return [] # Return empty list if LLM is not loaded\n",
    "\n",
    "    # Truncate article text for inference if it exceeds model's context window\n",
    "    # Use the same max_length as during training for consistency\n",
    "    max_inference_context_length = inference_tokenizer.model_max_length - 256 # Reserve tokens for prompt and response\n",
    "    truncated_article_text = article_text[:max_inference_context_length]\n",
    "\n",
    "    user_message = f\"\"\"\n",
    "Article Text:\n",
    "{truncated_article_text}\n",
    "\n",
    "Task: Identify all datasets or databases used in this research article and classify each as \"Primary\" (if created by the authors for this research) or \"Secondary\" (if an existing dataset used in this research).\n",
    "\n",
    "Output Format: Provide a JSON list of objects. Each object should have \"dataset_id\" and \"classification\" keys. If no datasets are identified, return an empty JSON list: [].\n",
    "\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert research assistant. Your task is to extract and classify datasets from scientific articles.\"},\n",
    "        {\"role\": \"user\", \"content\": user_message.strip()}\n",
    "    ]\n",
    "    \n",
    "    input_ids = inference_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(inference_model.device)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=512, # Allow more tokens for multiple dataset outputs\n",
    "                pad_token_id=inference_tokenizer.eos_token_id,\n",
    "                eos_token_id=inference_tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "            )\n",
    "        \n",
    "        response_text = inference_tokenizer.decode(\n",
    "            outputs[0][input_ids.shape[1]:],\n",
    "            skip_special_tokens=False # Keep special tokens to remove <|im_end|> explicitly\n",
    "        ).strip()\n",
    "        response_text = response_text.replace(\"<|im_end|>\", \"\").strip()\n",
    "        \n",
    "        print(f\"  LLM raw response: '{response_text}'\")\n",
    "\n",
    "        # Attempt to parse the JSON output\n",
    "        try:\n",
    "            parsed_data = json.loads(response_text)\n",
    "            if isinstance(parsed_data, list):\n",
    "                # Validate structure: each item should be a dict with 'dataset_id' and 'classification'\n",
    "                valid_datasets = []\n",
    "                for item in parsed_data:\n",
    "                    if isinstance(item, dict) and 'dataset_id' in item and 'classification' in item:\n",
    "                        # Basic validation for classification label\n",
    "                        if item['classification'] in [\"Primary\", \"Secondary\"]:\n",
    "                            valid_datasets.append(item)\n",
    "                        else:\n",
    "                            print(f\"  Warning: Invalid classification '{item['classification']}' for dataset '{item.get('dataset_id', 'N/A')}'. Skipping.\")\n",
    "                    else:\n",
    "                        print(f\"  Warning: Malformed JSON object: {item}. Skipping.\")\n",
    "                return valid_datasets\n",
    "            else:\n",
    "                print(f\"  Warning: LLM did not return a JSON list: {response_text}\")\n",
    "                return []\n",
    "        except json.JSONDecodeError as jde:\n",
    "            print(f\"  Error decoding JSON from LLM response: {jde}. Raw response: '{response_text}'\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error during LLM generation: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Main Processing Loop for all articles (Revised) ---\n",
    "print(\"\\n--- Starting Article Processing and Classification (LLM-driven) ---\")\n",
    "final_results = []\n",
    "\n",
    "for article_id, article_text in all_article_texts.items():\n",
    "    print(f\"\\nProcessing article: {article_id}\")\n",
    "    \n",
    "    # LLM directly extracts and classifies\n",
    "    identified_datasets = extract_and_classify_with_llm(article_text)\n",
    "    \n",
    "    if not identified_datasets:\n",
    "        # If LLM returns an empty list, classify the article as \"Missing\"\n",
    "        print(f\"  LLM identified no datasets for {article_id}. Classifying as 'Missing'.\")\n",
    "        final_results.append({\n",
    "            \"article_id\": article_id,\n",
    "            \"dataset_id\": \"N/A\", # Indicate no specific dataset ID\n",
    "            \"classification_label\": \"Missing\"\n",
    "        })\n",
    "    else:\n",
    "        print(f\"  LLM identified {len(identified_datasets)} dataset(s) for {article_id}.\")\n",
    "        for item in identified_datasets:\n",
    "            final_results.append({\n",
    "                \"article_id\": article_id,\n",
    "                \"dataset_id\": item.get(\"dataset_id\", \"Unknown\"), # Use .get() for safety\n",
    "                \"classification_label\": item.get(\"classification\", \"Uncertain_LLM\")\n",
    "            })\n",
    "\n",
    "\n",
    "# --- 5. Results & Output ---\n",
    "\n",
    "print(\"\\n--- Final Results ---\")\n",
    "if final_results:\n",
    "    results_df = pd.DataFrame(final_results)\n",
    "    print(results_df.head(10)) # Print first 10 rows\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(FINAL_RESULTS_CSV_PATH, index=False)\n",
    "    print(f\"\\nResults saved to: {FINAL_RESULTS_CSV_PATH}\")\n",
    "else:\n",
    "    print(\"No results generated.\")\n",
    "\n",
    "print(\"\\nProcessing complete, Jim!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
