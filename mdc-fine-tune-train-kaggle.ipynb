{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3195093d",
   "metadata": {},
   "source": [
    "#### 1. Setup and Dependencies\n",
    "\n",
    "First, we'll ensure all necessary libraries are installed. Given your previous work with `lxml`, `PyMuPDF`, and `spaCy`, we'll include those for text extraction and potentially more advanced NLP preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ca2607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# 1.1. Install necessary libraries\n",
    "# Use !pip install for notebook environment\n",
    "# !pip install transformers trl accelerate bitsandbytes sentencepiece lxml PyMuPDF spacy peft\n",
    "# !python -m spacy download en_core_web_sm # Download a small spaCy model\n",
    "\n",
    "# 1.2. Import Libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Set, List, Optional, Dict, Any\n",
    "\n",
    "import fitz # PyMuPDF\n",
    "from lxml import etree # For XML parsing\n",
    "import spacy\n",
    "import kagglehub\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset #, load_metric\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import gc # For garbage collection\n",
    "\n",
    "# For KaggleHub integration (assuming it's set up or models are downloaded)\n",
    "# You might need to install kagglehub if you plan to use it directly for model download\n",
    "# !pip install kagglehub\n",
    "\n",
    "# 1.3. Configure CUDA for local GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available! Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache() # Clear GPU memory\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b02acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for file paths and model configurations\n",
    "BASE_INPUT_DIR = './kaggle/input/make-data-count-finding-data-references'\n",
    "BASE_OUTPUT_DIR = \"./kaggle/working\"\n",
    "\n",
    "# Define directories for articles in train and test sets\n",
    "TRAIN_DATA_DIR = os.path.join(BASE_INPUT_DIR, 'train')\n",
    "TEST_DATA_DIR = os.path.join(BASE_INPUT_DIR, 'test')\n",
    "TRAIN_LABELS_PATH = os.path.join(BASE_INPUT_DIR, 'train_labels.csv')\n",
    "\n",
    "# Define the base model path\n",
    "QWEN_BASE_MODEL_PATH = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "\n",
    "# Output directory for the fine-tuned model and results\n",
    "FINE_TUNED_MODEL_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, \"results\")\n",
    "SAMPLE_SUBMISSION_PATH = os.path.join(BASE_OUTPUT_DIR, \"submission.csv\")\n",
    "\n",
    "# Load spaCy model for sentence segmentation and potentially other NLP tasks\n",
    "# python -m spacy download en_core_web_sm \n",
    "NLP_SPACY = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d8b9daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Extraction (IE) - Dataset Identification ---\n",
    "NON_STD_UNICODE_DASHES = re.compile(r'[\\u2010\\u2011\\u2012\\u2013\\u2014]')\n",
    "NON_STD_UNICODE_TICKS = re.compile(r'[\\u201c\\u201d]')\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input text by removing non-standard unicode dashes and extra whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Replace all non-standard unicode dashes with '-'\n",
    "    text = text.replace('\\u200b', '').replace('-\\n', '-').replace('_\\n', '_').replace('/\\n', '/').replace('dryad.\\n', 'dryad.').replace('doi.\\norg', 'doi.org')\n",
    "    text = NON_STD_UNICODE_DASHES.sub('-', text)\n",
    "    text = NON_STD_UNICODE_TICKS.sub(\"'\", text)\n",
    "    # Remove extra whitespace\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Information Extraction (IE) - Dataset Identification\n",
    "# Regex patterns for common dataset identifiers\n",
    "#DOI_PATTERN = r'\\b10\\.\\d{4,9}\\/[-._\\/:A-Za-z0-9]+'\n",
    "#DOI_PATTERN = r\"(?:doi:|https?://(?:dx\\.)?doi\\.org/)(10\\.\\d{4,9}/[-._;/:A-Z0-9]+)\"\n",
    "# DOI_PATTERN = r\"(?:doi:|https?://(?:dx\\.)?doi\\.org/)(10\\.\\d{4,9}/[-._;/:A-Za-z0-9]+)\"\n",
    "DOI_PATTERN = r\"(?:doi:|https?\\s*://(?:dx\\.)?doi\\.org/)(10\\.\\d{4,9}/[-._;/:A-Za-z0-9]+)\"\n",
    "\n",
    "EPI_PATTERN = r'\\bEPI[-_A-Z0-9]{2,}'\n",
    "SAM_PATTERN = r'\\bSAMN[0-9]{2,}'          # SAMN07159041\n",
    "IPR_PATTERN = r'\\bIPR[0-9]{2,}'\n",
    "CHE_PATTERN = r'\\bCHEMBL[0-9]{2,}'\n",
    "PRJ_PATTERN = r'\\bPRJ[A-Z0-9]{2,}'\n",
    "E_G_PATTERN = r'\\bE-[A-Z]{4}-[0-9]{2,}'   # E-GEOD-19722 or E-PROT-100\n",
    "ENS_PATTERN = r'\\bENS[A-Z]{4}[0-9]{2,}'\n",
    "CVC_PATTERN = r'\\bCVCL_[A-Z0-9]{2,}'\n",
    "EMP_PATTERN = r'\\bEMPIAR-[0-9]{2,}'\n",
    "PXD_PATTERN = r'\\bPXD[0-9]{2,}'\n",
    "HPA_PATTERN = r'\\bHPA[0-9]{2,}'\n",
    "SRR_PATTERN = r'\\bSRR[0-9]{2,}'\n",
    "GSE_PATTERN = r'\\b(GSE|GSM|GDS|GPL)\\d{4,6}\\b' # Example for GEO accession numbers (e.g., GSE12345, GSM12345)\n",
    "#GNB_PATTERN = r'\\b[A-Z]{1,2}\\d{5,6}\\b' # GenBank accession numbers (e.g., AB123456, AF000001)\n",
    "GNB_PATTERN = r'\\b[A-Z]{1,2}\\d{5,6}(?:\\.\\d)?\\b'\n",
    "CAB_PATTERN = r'\\bCAB[0-9]{2,}'\n",
    "PDB_PATTERN = r\"\\bpdb\\s*\\d[A-Za-z0-9]{3}\" # Example: pdb 5yfp\n",
    "\n",
    "# Combine all patterns into a list\n",
    "DATASET_ID_PATTERNS = [\n",
    "    DOI_PATTERN,\n",
    "    EPI_PATTERN,\n",
    "    SAM_PATTERN,\n",
    "    IPR_PATTERN,\n",
    "    CHE_PATTERN,\n",
    "    PRJ_PATTERN,\n",
    "    E_G_PATTERN,\n",
    "    ENS_PATTERN,\n",
    "    CVC_PATTERN,\n",
    "    EMP_PATTERN,\n",
    "    PXD_PATTERN,\n",
    "    HPA_PATTERN,\n",
    "    SRR_PATTERN,\n",
    "    GSE_PATTERN,\n",
    "    GNB_PATTERN,\n",
    "    CAB_PATTERN,\n",
    "    PDB_PATTERN\n",
    "]\n",
    "\n",
    "# Compile all patterns for efficiency\n",
    "COMPILED_DATASET_ID_REGEXES = [re.compile(p) for p in DATASET_ID_PATTERNS]\n",
    "\n",
    "# Data related keywords to look for in the text\n",
    "# These keywords help to ensure that the text is relevant to datasets\n",
    "#DATA_RELATED_KEYWORDS = ['data release', 'data associated', 'data availability', 'data access', 'download', 'program data', 'the data', 'dataset', 'database', 'repository', 'data source', 'data access', 'archive', 'arch.', 'digital']\n",
    "# DATA_RELATED_KEYWORDS = ['data ', 'dataset', 'database', 'download', 'repository', 'archive', 'arch.', 'digital']\n",
    "#DATA_RELATED_KEYWORDS = ['data ', 'dataset', 'database']\n",
    "DATA_RELATED_KEYWORDS = ['data release', 'data associated', 'data referring', 'data availability', 'data access', 'data source', 'program data', 'our data', 'the data', 'dataset', 'database',]\n",
    "\n",
    "def is_text_data_related(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in DATA_RELATED_KEYWORDS)\n",
    "\n",
    "REFERENCE_KEYWORDS = ['references','bibliography','works cited','literature cited','citations','reference list']\n",
    "def is_reference_heading_line(line: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a given Markdown line is a recognized reference section heading.\n",
    "    It looks for headings formatted with '#' or '**'.\n",
    "    \n",
    "    Args:\n",
    "        line (str): A single line of text from the Markdown content.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the line is a reference heading, False otherwise.\n",
    "    \"\"\"\n",
    "    # Normalize line by stripping leading/trailing whitespace and lowercasing\n",
    "    lower_line = line.strip().lower() \n",
    "    \n",
    "    for keyword in REFERENCE_KEYWORDS:\n",
    "        # Escape keyword for regex safety, in case it contains special regex characters\n",
    "        escaped_keyword = re.escape(keyword) \n",
    "        \n",
    "        # 1. Check for Markdown heading syntax (e.g., # References, ## Bibliography)\n",
    "        #    Pattern: Starts with one or more '#', followed by optional whitespace,\n",
    "        #    then the keyword, optional whitespace, and end of line.\n",
    "        if re.match(rf\"^#+\\s*{escaped_keyword}\\s*$\", lower_line):\n",
    "            return True\n",
    "        \n",
    "        # 2. Check for bold heading syntax (e.g., **References**, **Works Cited**)\n",
    "        #    Pattern: Optional leading whitespace, then two asterisks, optional whitespace,\n",
    "        #    the keyword, optional whitespace, two asterisks, optional trailing whitespace, and end of line.\n",
    "        if re.match(rf\"^\\s*\\*{2}\\s*{escaped_keyword}\\s*\\*{2}\\s*$\", lower_line):\n",
    "            return True\n",
    "            \n",
    "    return False # If no matching heading pattern is found after checking all keywords\n",
    "\n",
    "\n",
    "# def find_refernce_keyword_idx_in_page_text(page_text: str) -> int:\n",
    "#     page_text = page_text.lower()\n",
    "#     for keyword in REFERENCE_KEYWORDS:\n",
    "#         idx = page_text.find(keyword)\n",
    "#         if idx != -1:\n",
    "#             return idx  # Return the index of the first found keyword\n",
    "#     return -1  # Return -1 if no keyword is found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914e15c",
   "metadata": {},
   "source": [
    "#### 2. Data Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "701fe0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1. DatasetCitation Class\n",
    "@dataclass\n",
    "class DatasetCitation:\n",
    "    dataset_id: str = \"\"\n",
    "    citation_context: str = \"\"\n",
    "    citation_type: Optional[str] = None # \"Primary\", \"Secondary\", or \"Missing\" - for ground truth during training\n",
    "\n",
    "    def set_citation_context(self, context: str, check_data_related: bool = False):\n",
    "        \"\"\"Sets the citation context, cleaning it.\"\"\"\n",
    "        if context and (not check_data_related or not self.is_doi or is_text_data_related(context)):\n",
    "            # Replace newlines with spaces, remove brackets, and normalize whitespace\n",
    "            context = context.replace('\\n', ' ').replace('[', '').replace(']', '')\n",
    "            context = re.sub(r'\\s+', ' ', context.strip())\n",
    "            self.citation_context = context\n",
    "        else:\n",
    "            print(f\"Citation context not data related: {self.dataset_id}, {context}\")\n",
    "\n",
    "    def is_doi(self)-> bool:\n",
    "        return self.dataset_id.startswith(\"10.\")\n",
    "    \n",
    "    def has_dataset(self) -> bool:\n",
    "        \"\"\"Returns True if there are both dataset IDs and citation context.\"\"\"\n",
    "        return bool(self.dataset_id and self.citation_context.strip())\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "# 2.2. ArticleData Class\n",
    "@dataclass\n",
    "class ArticleData:\n",
    "    article_id: str = \"\"\n",
    "    article_doi: str = \"\"\n",
    "    title: str = \"\"\n",
    "    author: str = \"\"\n",
    "    abstract: str = \"\"\n",
    "    dataset_citations: List[DatasetCitation] = field(default_factory=list)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Custom initialization\n",
    "        if self.article_id and not self.article_doi:\n",
    "            # If article_id is provided but not article_doi, set article_doi\n",
    "            self.article_doi = self.article_id.replace(\"_\", \"/\").lower()\n",
    "\n",
    "    def add_dataset_citation(self, dataset_citation: DatasetCitation):\n",
    "        \"\"\"Adds a DatasetCitation object to the article.\"\"\"\n",
    "        if dataset_citation.has_dataset() and dataset_citation.dataset_id != self.article_doi:\n",
    "            self.dataset_citations.append(dataset_citation)\n",
    "        else:\n",
    "            print(f\"ID not addded: {dataset_citation.dataset_id}\")\n",
    "        \n",
    "    def to_dict(self):\n",
    "        d = asdict(self)\n",
    "        # Convert list of DatasetCitation objects to their dict representation\n",
    "        d[\"dataset_citations\"] = [dc.to_dict() for dc in self.dataset_citations]\n",
    "        return d\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), separators=(',', ':'))\n",
    "\n",
    "    def has_data(self) -> bool:\n",
    "        \"\"\"Returns True if there are any dataset citations.\"\"\"\n",
    "        return bool(self.dataset_citations)\n",
    "    \n",
    "@dataclass\n",
    "class LlmTrainingData:\n",
    "    article_id: str = \"\"\n",
    "    article_doi: str = \"\"\n",
    "    article_abstract: str = \"\"\n",
    "    citation_context: str = \"\"\n",
    "    dataset_id: str = \"\"\n",
    "    label: str = \"\"\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), separators=(',', ':'))\n",
    "\n",
    "@dataclass\n",
    "class SubmissionData:\n",
    "    article_id: str = \"\"\n",
    "    dataset_id: str = \"\"\n",
    "    type: str = \"\"\n",
    "    context: str = \"\"\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), separators=(',', ':'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b4259",
   "metadata": {},
   "source": [
    "#### 3. Data Loading and Initial Preprocessing\n",
    "\n",
    "This section will cover how to load the raw competition data (full text articles and labels) and begin structuring it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5d12b",
   "metadata": {},
   "source": [
    "#### Load Labeled Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "438f57fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labeled training data from: ./kaggle/input/make-data-count-finding-data-references\\train_labels.csv\n",
      "Training labels shape: (1028, 3)\n",
      "Example grouped training data for article_id '10.1002_2017jc013030': [{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]\n",
      "Files paths shape: (262, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ground_truth_dataset_info",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "84d718a9-e597-48c1-91f3-d61d6853350e",
       "rows": [
        [
         "3",
         "10.1111_2041-210x.12453",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_2041-210x.12453.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\train\\XML\\10.1111_2041-210x.12453.xml",
         "train",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.h4f7p', 'type': 'Primary'}]"
        ],
        [
         "22",
         "10.1186_s12920-020-00737-6",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12920-020-00737-6.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\train\\XML\\10.1186_s12920-020-00737-6.xml",
         "train",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]"
        ],
        [
         "98",
         "10.1186_s13750-020-0184-0",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13750-020-0184-0.pdf",
         "",
         "train",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>ground_truth_dataset_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1111_2041-210x.12453</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>train</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.1186_s12920-020-00737-6</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>train</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>10.1186_s13750-020-0184-0</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td></td>\n",
       "      <td>train</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    article_id  \\\n",
       "3      10.1111_2041-210x.12453   \n",
       "22  10.1186_s12920-020-00737-6   \n",
       "98   10.1186_s13750-020-0184-0   \n",
       "\n",
       "                                        pdf_file_path  \\\n",
       "3   ./kaggle/input/make-data-count-finding-data-re...   \n",
       "22  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "98  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                        xml_file_path dataset_type  \\\n",
       "3   ./kaggle/input/make-data-count-finding-data-re...        train   \n",
       "22  ./kaggle/input/make-data-count-finding-data-re...        train   \n",
       "98                                                           train   \n",
       "\n",
       "                            ground_truth_dataset_info  \n",
       "3   [{'dataset_id': 'https://doi.org/10.5061/dryad...  \n",
       "22     [{'dataset_id': 'Missing', 'type': 'Missing'}]  \n",
       "98     [{'dataset_id': 'Missing', 'type': 'Missing'}]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_file_paths(dataset_type_dir: str) -> pd.DataFrame: \n",
    "    pdf_path = os.path.join(dataset_type_dir, 'PDF')\n",
    "    xml_path = os.path.join(dataset_type_dir, 'XML')\n",
    "    dataset_type = os.path.basename(dataset_type_dir)\n",
    "    pdf_files = [f for f in os.listdir(pdf_path) if f.endswith('.pdf')]\n",
    "    xml_files = [f for f in os.listdir(xml_path) if f.endswith('.xml')]\n",
    "    df_pdf = pd.DataFrame({\n",
    "        'article_id': [f.replace('.pdf', '') for f in pdf_files],\n",
    "        'pdf_file_path': [os.path.join(pdf_path, f) for f in pdf_files]\n",
    "    })\n",
    "    df_xml = pd.DataFrame({\n",
    "        'article_id': [f.replace('.xml', '') for f in xml_files],\n",
    "        'xml_file_path': [os.path.join(xml_path, f) for f in xml_files]\n",
    "    })\n",
    "    merge_df = pd.merge(df_pdf, df_xml, on='article_id', how='outer', suffixes=('_pdf', '_xml'), validate=\"one_to_many\")\n",
    "    merge_df['dataset_type'] = dataset_type\n",
    "    return merge_df\n",
    "\n",
    "# Load the labeled training data CSV file\n",
    "print(f\"Loading labeled training data from: {TRAIN_LABELS_PATH}\")\n",
    "train_labels_df = pd.read_csv(TRAIN_LABELS_PATH)\n",
    "print(f\"Training labels shape: {train_labels_df.shape}\")\n",
    "\n",
    "# Group training data by article_id to get all datasets for each article\n",
    "# This creates a dictionary where keys are article_ids and values are lists of dataset dicts\n",
    "grouped_training_data = {}\n",
    "for article_id, group_df in train_labels_df.groupby('article_id'):\n",
    "    grouped_training_data[article_id] = group_df[['dataset_id', 'type']].to_dict('records')\n",
    "\n",
    "# Example usage of grouped_training_data\n",
    "print(f\"Example grouped training data for article_id '10.1002_2017jc013030': {grouped_training_data['10.1002_2017jc013030']}\")\n",
    "\n",
    "# Just for testing, always set to the TEST_DATA_DIR\n",
    "base_file_dir = TRAIN_DATA_DIR\n",
    "\n",
    "# Load file paths for base directory\n",
    "file_paths_df = load_file_paths(base_file_dir)\n",
    "file_paths_df['xml_file_path'] = file_paths_df['xml_file_path'].fillna('')\n",
    "\n",
    "# Merge the file paths with the grouped_training_data\n",
    "file_paths_df['ground_truth_dataset_info'] = file_paths_df['article_id'].map(grouped_training_data)\n",
    "file_paths_df['ground_truth_dataset_info'] = file_paths_df['ground_truth_dataset_info'].fillna('')\n",
    "\n",
    "# Reduce the file paths DataFrame to only those with ground truth dataset info and get a sample\n",
    "# This is to ensure we have a manageable dataset for training\n",
    "file_paths_df = file_paths_df[file_paths_df['ground_truth_dataset_info'].astype(bool)]\n",
    "file_paths_df = file_paths_df.reset_index(drop=True)\n",
    "file_paths_df = file_paths_df.sample(frac=.5, random_state=42).reset_index(drop=True)  # Shuffle the DataFrame\n",
    "print(f\"Files paths shape: {file_paths_df.shape}\")\n",
    "display(file_paths_df.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f0763",
   "metadata": {},
   "source": [
    "#### Define File Extract Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "971f8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf4llm\n",
    "\n",
    "# 3.1. Helper function to extract text from various file types\n",
    "def extract_text_from_file(filepath: str) -> str:\n",
    "    \"\"\"Extracts text from XML, PDF, or TXT files.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return \"\"\n",
    "    \n",
    "    print(f\"Extracting md text from file: {filepath}\")\n",
    "    if filepath.endswith(\".xml\"):\n",
    "        parser = etree.XMLParser(resolve_entities=False, no_network=True)\n",
    "        try:\n",
    "            tree = etree.parse(filepath, parser)\n",
    "            # A common way to get all text from an XML scientific article\n",
    "            # This might need adjustment based on the specific XML schema\n",
    "            return clean_text(\" \".join(tree.xpath(\"//text()\")).strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing XML {filepath}: {e}\")\n",
    "            return \"\"\n",
    "    elif filepath.endswith(\".pdf\"):\n",
    "        try:\n",
    "            text = pymupdf4llm.to_markdown(filepath, ignore_images=True, ignore_graphics=True)\n",
    "            lines = text.split('\\n')\n",
    "            text_before_references = []\n",
    "            \n",
    "            for line in lines:\n",
    "                # Use the new helper function to check if the current line is a reference heading\n",
    "                if is_reference_heading_line(line):\n",
    "                    break # Stop processing lines, we've found the references section\n",
    "                else:\n",
    "                    text_before_references.append(line) # Keep adding lines if not a reference heading\n",
    "                    \n",
    "            return clean_text(\"\\n\".join(text_before_references))\n",
    "\n",
    "            # ref_keyword_idx = find_refernce_keyword_idx_in_page_text(text)\n",
    "            # if ref_keyword_idx > 0:\n",
    "            #     # Only get text prior to the References section of the article\n",
    "            #     text = text[:ref_keyword_idx]\n",
    "\n",
    "            # # doc = fitz.open(filepath)\n",
    "            # # text = \"\"\n",
    "            # # for page in doc:\n",
    "            # #     page_text = page.get_textpage().extractTEXT()+\"\\n\"\n",
    "            # #     ref_keyword_idx = find_refernce_keyword_idx_in_page_text(page_text)\n",
    "            # #     if ref_keyword_idx >= 0:\n",
    "            # #         # Only get text prior to the References section of the article\n",
    "            # #         text += page_text[:ref_keyword_idx]\n",
    "            # #         break # Break from outer loop (page iteration)\n",
    "            # #     else:\n",
    "            # #         text += page_text\n",
    "            # return clean_text(text.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing PDF {filepath}: {e}\")\n",
    "            return \"\"\n",
    "    elif filepath.endswith(\".txt\"):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read().strip()\n",
    "    return \"\"\n",
    "\n",
    "def extract_first_few_sentences(text: str, num_sentences: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the first few sentences from the text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        num_sentences (int): The number of sentences to extract.\n",
    "        \n",
    "    Returns:\n",
    "        str: The first few sentences from the text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    doc = NLP_SPACY(text)\n",
    "    sentences = list(doc.sents)\n",
    "    \n",
    "    # Join the first few sentences\n",
    "    return \" \".join([sent.text for sent in sentences[:num_sentences]]).strip()\n",
    "\n",
    "def extract_article_data_from_text(full_text: str, article_id: str) -> ArticleData:\n",
    "    \"\"\"\n",
    "    Extracts article data from the full text.\n",
    "    \n",
    "    Args:\n",
    "        full_text (str): The full text of the article.\n",
    "        article_id (str): The ID of the article.\n",
    "        \n",
    "    Returns:\n",
    "        ArticleData: An instance of ArticleData with extracted information.\n",
    "    \"\"\"\n",
    "    abstract_match = re.search(r\"Abstract\\s*(.*?)(?=\\n\\n|\\Z)\", full_text, re.IGNORECASE | re.DOTALL)\n",
    "    abstract = abstract_match.group(1).strip() if abstract_match else \"No Abstract\"\n",
    "    abstract = extract_first_few_sentences(abstract[:400], num_sentences=3)  # Extract first few sentences for the abstract\n",
    "\n",
    "    return ArticleData(\n",
    "        article_id=article_id,\n",
    "        abstract=abstract\n",
    "    )\n",
    "\n",
    "# 4.2. Function to extract context around an ID\n",
    "def extract_context_around_id(sentences, dataset_id: str, window_size_sentences: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Extracts a window of sentences around a given dataset ID in the text.\n",
    "    Uses spaCy for sentence segmentation.\n",
    "    \"\"\"\n",
    "    if not sentences or not dataset_id or dataset_id == \"Missing\":\n",
    "        return \"\"\n",
    "        \n",
    "    # Find all occurrences of the dataset_id (case-insensitive)\n",
    "    matches = [(i, sent) for i, sent in enumerate(sentences) if dataset_id.lower() in sent.lower()]\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "\n",
    "    # For simplicity, take the context around the first match.\n",
    "    # You might want to refine this to capture all relevant contexts or the most prominent one.\n",
    "    first_match_idx = matches[0][0]\n",
    "    \n",
    "    start_idx = max(0, first_match_idx - window_size_sentences)\n",
    "    end_idx = min(len(sentences), first_match_idx + 1)\n",
    "    \n",
    "    context_sentences = sentences[start_idx:end_idx]\n",
    "    return \" \".join(context_sentences)\n",
    "\n",
    "\n",
    "def extract_training_data_for_llm(file_paths_df: pd.DataFrame) -> list[dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extracts article data for training set with ground truth.\n",
    "    \n",
    "    Args:\n",
    "        file_paths_df (pd.DataFrame): DataFrame containing file paths and ground truth info.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, ArticleData]: Dictionary mapping article IDs to ArticleData objects.\n",
    "    \"\"\"\n",
    "    training_data_for_llm: list[dict[str, str]] = [] # This will be a list of LlmTrainingData for the LLM training dataset\n",
    "    for i, row in tqdm(file_paths_df.iterrows(), total=len(file_paths_df)):\n",
    "        article_id = row['article_id']\n",
    "        filepath = row['pdf_file_path'] if row['pdf_file_path'] else row['xml_file_path']\n",
    "        ground_truth_list = row['ground_truth_dataset_info'] if 'ground_truth_dataset_info' in row else []\n",
    "        \n",
    "        full_text = extract_text_from_file(filepath)\n",
    "        article_data = extract_article_data_from_text(full_text, article_id)\n",
    "\n",
    "        doc = NLP_SPACY(full_text)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "        if not ground_truth_list:\n",
    "            print(f\"No ground truth data found for article_id: {article_id}. Skipping this article.\")\n",
    "            continue\n",
    "        for gt in ground_truth_list:\n",
    "            dataset_id = gt['dataset_id'].replace(\"https://doi.org/\", \"\").replace(\"doi:\", \"\").strip()\n",
    "            citation_type = gt.get('type', 'Primary')\n",
    "            if dataset_id:\n",
    "                # Convert to dict for LLM training data\n",
    "                training_data_for_llm.append(\n",
    "                    {\n",
    "                        \"article_id\": article_data.article_id,\n",
    "                        \"article_doi\": article_data.article_doi,\n",
    "                        \"article_abstract\": article_data.abstract,\n",
    "                        \"citation_context\": extract_context_around_id(sentences, dataset_id),\n",
    "                        \"dataset_id\": dataset_id,\n",
    "                        \"label\": citation_type\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    print(f\"Loaded training data for {len(training_data_for_llm)} articles.\")\n",
    "    return training_data_for_llm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aeda081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ground_truth_dataset_info",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "3267ea33-5fd5-4dab-82e2-10d584fc60e2",
       "rows": [
        [
         "154",
         "10.1002_esp.5058",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_esp.5058.pdf",
         "",
         "train",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.jh9w0vt9t', 'type': 'Primary'}]"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>ground_truth_dataset_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>10.1002_esp.5058</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td></td>\n",
       "      <td>train</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           article_id                                      pdf_file_path  \\\n",
       "154  10.1002_esp.5058  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "    xml_file_path dataset_type  \\\n",
       "154                      train   \n",
       "\n",
       "                             ground_truth_dataset_info  \n",
       "154  [{'dataset_id': 'https://doi.org/10.5061/dryad...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing, let's extract training data for a specific article\n",
    "sample_file_paths_df = file_paths_df.loc[file_paths_df['article_id'] == '10.1002_esp.5058']\n",
    "sample_file_paths_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef756a4e",
   "metadata": {},
   "source": [
    "#### 4. Advanced Preprocessing: Extracting Dataset Mentions and Context (Training)\n",
    "\n",
    "Use regex to find the given dataset IDs from the training_labels and then use spaCy to extract surrounding sentences as context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f23263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b56dc177a3468c84db329b62108b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7717_peerj.12422.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0198382.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_chem.202000235.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_2041-210x.12453.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41597-019-0101-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12974-020-01860-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7717_peerj.13193.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3390_s23177333.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_22221751.2020.1738277.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1103_physrevresearch.4.023008.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ece3.3985.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_tc-17-3617-2023.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41467-019-10357-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1980-5918.033.ao15.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3390_molecules191017026.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536808011148.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_anie.202005531.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2052252514012081.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1364_oe.25.001985.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1098_rspb.2015.1498.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13046-018-0843-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1414-431x20198292.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12920-020-00737-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12916-019-1469-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13072-019-0322-5.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12920-018-0426-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12868-018-0468-2.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3133_ofr20231027.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1980-5373-mr-2018-0921.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_essd-8-663-2016.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7554_elife.74937.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1021_acsomega.3c06074.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5937_bnhmb1811227u.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13104-018-4014-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1007_jhep11(2018)115.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41467-018-04041-x.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7554_elife.29944.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_hdy.2014.75.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_chem.202003167.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41597-022-01555-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1055_s-0039-1693681.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1073_pnas.1711872115.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12913-018-3333-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1098_rspb.2015.2726.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1980-5918.032.ao27.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12918-015-0209-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41437-020-0318-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12859-018-2414-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s40170-020-00212-x.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41562-021-01247-w.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7717_peerj.10452.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1039_c9sc02930c.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_s1677-5538.ibju.2019.0167.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7554_eLife.63194.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s11689-019-9287-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1140_epjds_s13688-018-0132-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1093_sysbio_syy011.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2664.13136.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3133_ofr20201035.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13617-019-0084-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13073-020-00727-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12862-019-1388-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13059-019-1908-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12931-019-1001-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_ncomms11871.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.14379_iodp.proc.390393.208.2024.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s160053681103220x.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41598-024-56373-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1029_2021pa004379.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pcbi.1011828.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1039_c9ra06638a.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12864-019-6324-7.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2656.12501.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1007_s00382-012-1636-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41558-022-01301-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41598-020-59839-x.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_2017jc013030.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ece3.4466.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12866-015-0509-2.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ejoc.202000139.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12870-018-1542-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1806-90882019000500004.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12859-018-2036-2.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7554_elife.63455.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2052252515023945.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1806-93042019000400002.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ece3.9627.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2664.13446.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13068-018-1316-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ece3.6303.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_0001-3765201920180768.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1016_j.ast.2022.107401.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3390_ijms12117360.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1016_j.jobe.2023.107105.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1039_d0sc01197e.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2414314616000523.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1140_epjc_s10052-019-6583-0.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.12688_f1000research.11698.1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13750-020-0184-0.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0253228.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0159387.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_0104-4060.59642.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13007-019-0403-2.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1809-2950_19008627012020.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536812027390.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12859-016-0922-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_hdy.2015.99.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s40657-020-00194-w.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13024-018-0266-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2435.13431.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1617_s11527-023-02260-3.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12860-020-00261-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1145_3461702.3462538.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_eva.12151.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13100-019-0153-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13068-018-1167-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12859-016-1206-3.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536812046892.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1678-4499.20190067.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2656.12491.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1242_dev.138545.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_0102.3772e35417.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1130_ges01387.1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13104-019-4127-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12915-018-0498-3.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.18438_eblip29674.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s40851-018-0089-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_sdata.2017.167.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3389_fchem.2019.00828.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_evo.13972.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_0047-2085000000239.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ecs2.1280.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13568-018-0680-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1021_jacs.2c06519.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.12688_f1000research.13064.1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s40793-015-0095-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.17581_bp.2020.09104.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12866-019-1542-3.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_esp.5090.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_s1678-86212019000400340.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1016_j.dib.2023.109949.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13059-019-1924-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12870-020-2295-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2435.13087.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12920-019-0646-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1098_rsos.160417.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13148-019-0719-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1007_jhep12(2018)117.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_14756366.2020.1740692.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12859-020-3415-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12859-018-2263-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13643-018-0859-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1029_2018gl078007.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_amt-15-3969-2022.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_esp.5058.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ece3.961.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12879-019-3766-0.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1029_2023wr035126.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3897_neobiota.82.87455.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3389_fcimb.2024.1292467.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1039_d0sc01518k.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1519-6984.192126.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s40168-018-0550-0.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ejoc.202000916.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12866-020-01863-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12885-018-4768-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0284951.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_chem.201903120.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_21645515.2023.2189598.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1016_j.jlp.2022.104761.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_acp-2021-570.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12920-019-0611-7.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ece3.6784.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3762_bjoc.8.42.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.12688_f1000research.4660.1.pdf\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_anie.202007717.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3390_d13010019.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_fst.33717.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1029_2020jf005675.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_0284186x.2020.1714721.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13073-019-0674-2.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12885-020-06724-5.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1007_jhep11(2018)113.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3390_v11060565.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_2041-210x.13817.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_mp.14424.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_chem.202001412.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2664.13168.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3390_s19030479.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_02713683.2019.1607392.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1007_s12263-014-0408-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2052252515011665.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_essd-12-1287-2020.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1093_beheco_arad016.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_15476286.2016.1232238.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12885-018-4314-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2656.12382.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1140_epjc_s10052-018-6468-7.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1128_spectrum.00422-24.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536810036299.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_eva.12768.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0188323.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1534_g3.119.400993.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1806-9479.2019.185555.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3897_zoologia.36.e32053.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13321-015-0110-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3389_fevo.2023.1112519.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536809014883.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0139215.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_hdy.2013.74.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3847_1538-4357_aae92c.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2414314616007033.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_cas.12935.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3897_zookeys.500.9360.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0262974.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12863-019-0790-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_eva.12446.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41598-017-15852-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_0284186x.2019.1669817.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7554_eLife.72626.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536810047185.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1039_d0gc00363h.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536812024269.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12864-019-6131-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_0104-6632.20190362s20180340.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_essd-2023-187.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41598-021-85671-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1073_pnas.1705601114.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2056989020010658.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2056989015019891.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_mec.16743.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12870-019-1889-5.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41467-018-07681-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13068-018-1078-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12864-015-2206-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3897_zoologia.35.e23481.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1093_beheco_arw167.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_acp-22-5701-2022.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13058-015-0618-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12881-019-0773-3.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.12688_f1000research.13483.1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_2236-3459_83528.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3897_bdj.7.e47369.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13024-018-0254-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0212669.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_essd-2023-198.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1980-5373-mr-2018-0766.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13073-019-0709-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13071-018-3237-2.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1007_s10904-014-0054-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_gcb.13914.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536807066780.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0137181.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12874-018-0583-x.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12943-019-1017-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12967-019-2100-3.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1029_2019pa003774.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_07350015.2020.1766469.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12903-018-0656-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12884-018-1751-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0070749.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536812014614.pdf\n",
      "Loaded training data for 573 articles.\n",
      "Prepared 573 training examples for the LLM.\n",
      "Training set size: 515 examples\n",
      "Validation set size: 58 examples\n"
     ]
    }
   ],
   "source": [
    "# 4.3. Populate ArticleData with DatasetCitation objects and ground truth\n",
    "testing_data_for_llm = extract_training_data_for_llm(file_paths_df)\n",
    "print(f\"Prepared {len(testing_data_for_llm)} training examples for the LLM.\")\n",
    "\n",
    "# Convert the list of LlmTrainingData to a DataFrame and save it\n",
    "training_data_for_llm_df = pd.DataFrame(testing_data_for_llm)\n",
    "training_data_for_llm_df.to_csv(os.path.join(BASE_OUTPUT_DIR, \"training_data_for_llm.csv\"), index=False)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_list(testing_data_for_llm)\n",
    "train_dataset = train_dataset.shuffle(seed=42) # Shuffle for good measure\n",
    "\n",
    "# Split into train/validation\n",
    "train_test_split = train_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "print(f\"Training set size: {len(train_dataset)} examples\")\n",
    "print(f\"Validation set size: {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac610853f6744f7d9d9984ccb2b1598d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28af41d1153497a82b67798c5cdff47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "53426"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save off the datasets to CSV for later use\n",
    "train_dataset.to_csv(os.path.join(BASE_OUTPUT_DIR, \"train_dataset.csv\"), index=False)\n",
    "eval_dataset.to_csv(os.path.join(BASE_OUTPUT_DIR, \"eval_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066f50c",
   "metadata": {},
   "source": [
    "#### 5. Model Selection and Configuration\n",
    "\n",
    "We'll use a Qwen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12e2982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model C:\\Users\\jim\\.cache\\kagglehub\\models\\qwen-lm\\qwen-3\\transformers\\0.6b\\1 loaded with 4-bit quantization.\n"
     ]
    }
   ],
   "source": [
    "# 5.1. Choose a Model from KaggleHub\n",
    "# Example: Qwen/Qwen1.5-0.5B-Chat (or 1.8B-Chat if 0.5B is too small/performs poorly)\n",
    "# You can find these on KaggleHub or Hugging Face Hub.\n",
    "model_name = QWEN_BASE_MODEL_PATH\n",
    "\n",
    "# 5.2. Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Qwen uses EOS for padding\n",
    "\n",
    "# 5.3. Load Model with Quantization (4-bit)\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Or torch.float16 if bfloat16 is not supported by your GPU\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=nf4_config,\n",
    "    torch_dtype=torch.bfloat16, # Match compute_dtype\n",
    "    device_map=\"auto\", # Automatically maps model to available devices\n",
    "    trust_remote_code=True # Required for some models like Qwen\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training (LoRA compatible)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"Model {model_name} loaded with 4-bit quantization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a83df8",
   "metadata": {},
   "source": [
    "#### 6. Dataset Preparation for Training\n",
    "\n",
    "Format the extracted data into instruction-tuning prompts using the ChatML format, which Qwen models are trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a008679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of formatted training data (string output):\n",
      "<|im_start|>system\n",
      "You are an expert assistant for classifying research data citations. /no_think<|im_end|>\n",
      "<|im_start|>user\n",
      "Given the following article context and a specific data citation, classify if the data was generated as 'Primary' (newly generated for this study), 'Secondary' (reused from existing records), or 'Missing' (no data citation context given).\n",
      "\n",
      "Article DOI: 10.1111/cas.12935\n",
      "Article Abstract: No Abstract\n",
      "Data Citation Context: The difference was more significant in ER-negative BCs (P = 0.0276 for ER-negative vs healthy control; P = 0.2277 for ER-positive vs healthy control). The area under the curve value was 0.604 for all BC patients versus healthy Table 1. Twelve proteins selected as marker candidates for breast cancer Accession no. Protein name Gene name MS  MS spectral count HPA database MCF-7 MDA-MB-231 SK-BR-3 Hs578T HPA Ab Average IHC score Percent of location Normal Breast cancer n  c  m c  m n Q99538 Legumain LGMN 7 24 13 1 HPA001426 1 2 0 100 0 O75787 Renin receptor ATP6AP2 0 7 3 0 HPA003156 1 1.77 0 100 0\n",
      "Dataset ID: Q99538\n",
      "\n",
      "Classification:<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Secondary<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.1. Define the formatting function for ChatML (Corrected for trl 0.19.1)\n",
    "def format_example(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert assistant for classifying research data citations. /no_think\"},\n",
    "        {\"role\": \"user\", \"content\": (\n",
    "            f\"Given the following article context and a specific data citation, classify if the data was generated as 'Primary' (newly generated for this study), 'Secondary' (reused from existing records), or 'Missing' (no data citation context given).\\n\\n\"\n",
    "            f\"Article DOI: {example['article_doi']}\\n\"\n",
    "            f\"Article Abstract: {example['article_abstract']}\\n\" \n",
    "            f\"Data Citation Context: {example['citation_context']}\\n\"\n",
    "            f\"Dataset ID: {example['dataset_id']}\\n\\n\"\n",
    "            f\"Classification:\"\n",
    "        )}\n",
    "    ]\n",
    "    # The target output for the model is just \"Primary\" or \"Secondary\"\n",
    "    messages.append({\"role\": \"assistant\", \"content\": example['label']})\n",
    "    \n",
    "    # Apply chat template and return the string directly\n",
    "    # <--- IMPORTANT CHANGE: Directly return the string, not a dictionary\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False, enable_thinking=False)\n",
    "\n",
    "# Apply the formatting to the dataset\n",
    "# IMPORTANT: When formatting_func returns a string directly, you typically don't\n",
    "# need to call .map() on the dataset beforehand if SFTTrainer handles it internally.\n",
    "# However, if you want to inspect the formatted text, you can still do this:\n",
    "# formatted_train_dataset = train_dataset.map(format_example)\n",
    "# But for SFTTrainer, you pass the original `train_dataset` and the `formatting_func`\n",
    "# and `dataset_text_field` (which will be ignored if formatting_func is used to generate the text).\n",
    "\n",
    "# Print an example to verify (you'll need to call format_example directly for this)\n",
    "print(\"\\nExample of formatted training data (string output):\")\n",
    "# You can't directly print from formatted_train_dataset if you don't map it first.\n",
    "# Let's print by calling the function on a sample:\n",
    "if len(train_dataset) > 0:\n",
    "    sample_formatted_text = format_example(train_dataset[0])\n",
    "    print(sample_formatted_text)\n",
    "else:\n",
    "    print(\"No training data to display example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d8fdd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd84cd99cb3f4e44a088082f80a53738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557aa8acdcd54112b2ccff021bd1a780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65aebf7615e247669ee2a641788f3e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9594033cf4646319561ac45d3c548e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbe417c70e64a7bb02e42d08dcfb349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c827abe0c04d21b4db2b28b28afa75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9ae5e353534bbeb7d171667c68a41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f7b058fa004b49bacbe84dbd4a916f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# This version uses the evaluation dataset in the SFTTrainer\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 7.1. Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\", # Adjust based on model architecture if needed\n",
    ")\n",
    "\n",
    "# 7.2. Configure Training Arguments (now using SFTConfig)\n",
    "training_args = SFTConfig(\n",
    "    output_dir=FINE_TUNED_MODEL_OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[],\n",
    "    \n",
    "    # SFTTrainer-specific parameters moved into SFTConfig\n",
    "    max_seq_length=256,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False},\n",
    "\n",
    "    # --- NEW: Evaluation Parameters ---\n",
    "    eval_strategy=\"steps\", # Evaluate every 'eval_steps'\n",
    "    eval_steps=500,              # How often to run evaluation (e.g., every 500 steps)\n",
    "                                 # You can also use \"epoch\" for evaluation_strategy\n",
    "    save_strategy=\"steps\",       # How often to save checkpoints\n",
    "    save_total_limit=1,          # Only keep the best model checkpoint\n",
    "    load_best_model_at_end=True, # Load the model with the best validation metric at the end of training\n",
    "    metric_for_best_model=\"eval_loss\", # Metric to monitor for best model (default for CLM)\n",
    "    greater_is_better=False,     # For loss, lower is better\n",
    ")\n",
    "\n",
    "# 7.3. Initialize SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, # <--- Pass the evaluation dataset here\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    "    formatting_func=format_example\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f90683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612c42297d1e41398bf21a46892eb71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc41c736f5a42d08bd9f7c003ac7441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8068b3693b584593b7012fe602578e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00bac153dd446b9a53dbb795a245c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# This version works but does NOT use the evaluation dataset\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# # 7.1. Configure LoRA\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     r=64,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=\"all-linear\", # Adjust based on model architecture if needed\n",
    "# )\n",
    "\n",
    "# # 7.2. Configure Training Arguments (now using SFTConfig)\n",
    "# # SFTConfig combines TrainingArguments with SFTTrainer-specific parameters\n",
    "# training_args = SFTConfig( # <--- IMPORTANT CHANGE: Use SFTConfig instead of TrainingArguments\n",
    "#     output_dir=FINE_TUNED_MODEL_OUTPUT_DIR,\n",
    "#     per_device_train_batch_size=1, # Adjust based on your GPU memory\n",
    "#     gradient_accumulation_steps=16,\n",
    "#     learning_rate=2e-4,\n",
    "#     num_train_epochs=3,\n",
    "#     logging_steps=10,\n",
    "#     save_steps=500,\n",
    "#     optim=\"paged_adamw_8bit\",\n",
    "#     fp16=True,  # <--- CHANGED: Try fp16 for broader compatibility and memory\n",
    "#     bf16=False, # <--- CHANGED: Disable bf16 if fp16 is used\n",
    "#     max_grad_norm=0.3,\n",
    "#     warmup_ratio=0.03,\n",
    "#     lr_scheduler_type=\"constant\",\n",
    "#     report_to=\"none\",\n",
    "#     disable_tqdm=False,\n",
    "#     remove_unused_columns=False, # Keep columns for formatting\n",
    "#     label_names=[], # Explicitly tell Trainer not to look for label columns in the dataset\n",
    "#     # Additional SFT-specific parameters    \n",
    "#     max_seq_length=512, # Max input sequence length (adjust based on context size)\n",
    "#     packing=False, # Set to True for more efficient training if your data is short\n",
    "#     dataset_text_field=\"text\", # The name of the column in your dataset containing the text\n",
    "#     # <--- NEW: Enable gradient checkpointing\n",
    "#     gradient_checkpointing=True,\n",
    "#     # This line is important for gradient checkpointing with PeftModel\n",
    "#     # It tells the model to use the Peft (LoRA) layers for checkpointing\n",
    "#     gradient_checkpointing_kwargs={'use_reentrant':False} # Recommended for newer PyTorch/Accelerate\n",
    "# )\n",
    "\n",
    "# # 7.3. Initialize SFTTrainer (Corrected for trl 0.19.1)\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     processing_class=tokenizer, \n",
    "#     train_dataset=train_dataset,\n",
    "#     peft_config=peft_config,\n",
    "#     args=training_args, # This is now an SFTConfig object\n",
    "#     formatting_func=format_example # This remains a direct argument to SFTTrainer\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45982673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='99' max='99' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [99/99 16:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Fine-tuned model saved to ./kaggle/working\\results\\final_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7.4. Start Training\n",
    "print(\"\\nStarting model training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the fine-tuned model (LoRA adapters)\n",
    "trainer.save_model(os.path.join(FINE_TUNED_MODEL_OUTPUT_DIR, \"final_model\"))\n",
    "print(f\"Fine-tuned model saved to {os.path.join(FINE_TUNED_MODEL_OUTPUT_DIR, 'final_model')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initiating GPU memory cleanup...\n",
      "GPU memory cleanup complete. Please check nvidia-smi to confirm.\n"
     ]
    }
   ],
   "source": [
    "# --- Explicit GPU Memory Cleanup ---\n",
    "print(\"\\nInitiating GPU memory cleanup...\")\n",
    "\n",
    "# 1. Explicitly delete large objects that consume GPU memory\n",
    "#    This removes references, allowing Python's garbage collector to act.\n",
    "if 'trainer' in locals() and trainer is not None:\n",
    "    del trainer\n",
    "if 'model' in locals() and model is not None:\n",
    "    del model\n",
    "if 'tokenizer' in locals() and tokenizer is not None:\n",
    "    del tokenizer\n",
    "# If you had other large tensors or datasets explicitly moved to GPU,\n",
    "# you would delete them here too. For Hugging Face datasets, they are usually\n",
    "# on CPU unless you manually call .to('cuda').\n",
    "\n",
    "# 2. Force Python's garbage collection\n",
    "#    This helps ensure that deleted objects are immediately cleaned up.\n",
    "gc.collect()\n",
    "\n",
    "# 3. Clear PyTorch's CUDA memory cache\n",
    "#    This tells PyTorch to release any cached memory back to the OS/driver.\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"GPU memory cleanup complete. Please check nvidia-smi to confirm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e3154",
   "metadata": {},
   "source": [
    "#### 8. Inference and Evaluation\n",
    "\n",
    "After training, load the best model (or the final one) and apply it to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "583fed9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded for inference.\n"
     ]
    }
   ],
   "source": [
    "# 8.1. Load the Trained Model (or merge LoRA adapters for full model)\n",
    "# If you saved LoRA adapters, you'll need to load the base model and then the adapters.\n",
    "# For inference, it's often easier to merge them.\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=nf4_config, # Use the same config as training\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# model = PeftModel.from_pretrained(model, os.path.join(FINE_TUNED_MODEL_OUTPUT_DIR, \"final_model\"))\n",
    "# model = model.merge_and_unload() # Merge LoRA adapters into the base model\n",
    "\n",
    "# For simplicity, if you just want to test the last saved checkpoint:\n",
    "# You can also load the model directly from the checkpoint if it's a full save\n",
    "# model = AutoModelForCausalLM.from_pretrained(os.path.join(FINE_TUNED_MODEL_OUTPUT_DIR, \"final_model\"), device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(os.path.join(FINE_TUNED_MODEL_OUTPUT_DIR, \"final_model\"))\n",
    "\n",
    "# If you want to load the base model and then the adapters for inference:\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=nf4_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, os.path.join(FINE_TUNED_MODEL_OUTPUT_DIR, \"final_model\"))\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "print(\"Model loaded for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bc475f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files paths shape: (30, 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "fb1e0aa2-bc3e-4e5c-a6b9-93731059b528",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_2017jc013030.xml",
         "test"
        ],
        [
         "5",
         "10.1002_chem.201903120",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.201903120.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.201903120.xml",
         "test"
        ],
        [
         "20",
         "10.1002_ecs2.1280",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.1280.pdf",
         "",
         "test"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1002_chem.201903120</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.1002_ecs2.1280</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                article_id                                      pdf_file_path  \\\n",
       "0     10.1002_2017jc013030  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "5   10.1002_chem.201903120  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "20       10.1002_ecs2.1280  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                        xml_file_path dataset_type  \n",
       "0   ./kaggle/input/make-data-count-finding-data-re...         test  \n",
       "5   ./kaggle/input/make-data-count-finding-data-re...         test  \n",
       "20                                                            test  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For testing, always set to the TEST_DATA_DIR\n",
    "base_file_dir = TEST_DATA_DIR\n",
    "\n",
    "# Load file paths for base directory\n",
    "test_file_paths_df = load_file_paths(base_file_dir)\n",
    "test_file_paths_df['xml_file_path'] = test_file_paths_df['xml_file_path'].fillna('')\n",
    "\n",
    "print(f\"Files paths shape: {test_file_paths_df.shape}\")\n",
    "display(test_file_paths_df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b06b8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_potential_dataset_ids(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Finds potential dataset IDs in the given text using predefined regex patterns.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to search for dataset IDs.\n",
    "        \n",
    "    Returns:\n",
    "        Set[str]: A set of unique dataset IDs found in the text.\n",
    "    \"\"\"\n",
    "    dataset_ids = set()\n",
    "    for regex in COMPILED_DATASET_ID_REGEXES:\n",
    "        for match in re.finditer(regex, text):\n",
    "            if regex.pattern == DOI_PATTERN:\n",
    "                dataset_id = match.group(1)\n",
    "            else:\n",
    "                dataset_id = match.group(0)\n",
    "            dataset_ids.add(dataset_id)\n",
    "    return list(dataset_ids)\n",
    "\n",
    "def invoke_model_for_inference(tokenizer, article_data: ArticleData) -> list[SubmissionData]:\n",
    "    submission_data_list = []\n",
    "    article_id = article_data.article_id\n",
    "    dataset_citations = article_data.dataset_citations\n",
    "    if not dataset_citations:\n",
    "        submission_data_list.append(SubmissionData(article_id, dataset_id=\"Missing\", type=\"Missing\"))\n",
    "        return submission_data_list\n",
    "\n",
    "    print(f\"Found {len(dataset_citations)} citations for {article_id}\")\n",
    "    for dc in dataset_citations:\n",
    "        # Create the prompt for inference\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert assistant for classifying research data citations. /no_think\"},\n",
    "            {\"role\": \"user\", \"content\": (\n",
    "                f\"Given the following article context and a specific data citation, classify if the data was generated as 'Primary' (newly generated for this study) or 'Secondary' (reused from existing records).\\n\\n\"\n",
    "                f\"Article Title: {article_data.title}\\n\"\n",
    "                f\"Article Abstract: {article_data.abstract}\\n\"\n",
    "                f\"Data Citation Context: {dc.citation_context}\\n\"\n",
    "                f\"Dataset ID: {dc.dataset_id}\\n\\n\"\n",
    "                f\"Classification:\"\n",
    "            )}\n",
    "        ]\n",
    "\n",
    "        # --- CHANGE STARTS HERE ---\n",
    "        # Tokenize and get both input_ids and attention_mask\n",
    "        input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs, # <--- Pass the entire dictionary (includes input_ids and attention_mask)\n",
    "                max_new_tokens=10, # Expecting \"Primary\" or \"Secondary\"\n",
    "                do_sample=False, # Use greedy decoding as per your preference\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        # --- CHANGE ENDS HERE ---        \n",
    "\n",
    "        generated_text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip() # Use inputs['input_ids']\n",
    "        # print(f\"LLM Resp: {generated_text}\")        \n",
    "        \n",
    "        # Post-process the generated text to get the classification\n",
    "        predicted_type = \"Missing\"\n",
    "        if \"Primary\" in generated_text:\n",
    "            predicted_type = \"Primary\"\n",
    "        elif \"Secondary\" in generated_text:\n",
    "            predicted_type = \"Secondary\"\n",
    "        \n",
    "        submission_data_list.append(SubmissionData(article_id, dataset_id=dc.dataset_id, type=predicted_type, context=dc.citation_context))\n",
    "\n",
    "    return submission_data_list\n",
    "\n",
    "def extract_article_data_for_inference(article_id: str, filepath: str) -> ArticleData:\n",
    "    full_text = extract_text_from_file(filepath)\n",
    "    article_data = extract_article_data_from_text(full_text, article_id)\n",
    "    potential_dataset_ids = find_potential_dataset_ids(full_text)\n",
    "    if potential_dataset_ids:\n",
    "        doc = NLP_SPACY(full_text)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "        # Populate article_data with potentially valid dataset_citations\n",
    "        # the set_citation_context and add_dataset_citation methods do all of the appropriate filtering\n",
    "        for dataset_id in potential_dataset_ids:\n",
    "            citation = DatasetCitation(dataset_id=dataset_id)\n",
    "            context = extract_context_around_id(sentences, dataset_id)\n",
    "            citation.set_citation_context(context, check_data_related=True)\n",
    "            article_data.add_dataset_citation(citation)\n",
    "\n",
    "    return article_data\n",
    "\n",
    "def process_test_articles(tokenizer, file_paths_df: pd.DataFrame) -> list[SubmissionData]:\n",
    "    \"\"\"\n",
    "    Extracts article data for testing set without ground truth.\n",
    "    \n",
    "    Args:\n",
    "        file_paths_df (pd.DataFrame): DataFrame containing file paths and ground truth info.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, ArticleData]: Dictionary mapping article IDs to ArticleData objects.\n",
    "    \"\"\"\n",
    "    submission_data_list = []\n",
    "    for i, row in tqdm(file_paths_df.iterrows(), total=len(file_paths_df)):\n",
    "        article_id = row['article_id']\n",
    "        filepath = row['pdf_file_path'] if row['pdf_file_path'] else row['xml_file_path']\n",
    "        \n",
    "        # Extract article data\n",
    "        article_data = extract_article_data_for_inference(article_id, filepath)\n",
    "\n",
    "        # Invoke the model with the collected article_data\n",
    "        submission_data_list.extend(invoke_model_for_inference(tokenizer, article_data))\n",
    "\n",
    "    print(f\"Processed testing data for {len(submission_data_list)} article and dataset_id combos.\")\n",
    "    return submission_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ee07a9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GSE45042',\n",
       " '10.1234/dryad.as2345',\n",
       " 'GSE28166',\n",
       " 'GSE37569',\n",
       " '10.25386/genetics.11365982',\n",
       " 'pdb 5yfp']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"adff doi:10.1234/dryad.as2345 accession numbers GSE1 GSE37569, GSE45042 , GSE28166  (pdb 5yfp) https://doi.org/10.25386/genetics.11365982 10.1234/dryad.as123 \"\n",
    "rsp1 = find_potential_dataset_ids(text)\n",
    "display(rsp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "903dc0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "145545df-624b-4522-a006-888d038b5623",
       "rows": [
        [
         "13",
         "10.1002_ece3.5260",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.5260.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.5260.xml",
         "test"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.1002_ece3.5260</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           article_id                                      pdf_file_path  \\\n",
       "13  10.1002_ece3.5260  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                        xml_file_path dataset_type  \n",
       "13  ./kaggle/input/make-data-count-finding-data-re...         test  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_file_paths_df = test_file_paths_df.sample(2, random_state=42)\n",
    "sample_test_file_paths_df = test_file_paths_df.loc[test_file_paths_df['article_id']=='10.1002_ece3.5260']\n",
    "sample_test_file_paths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "efde80b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051d5dfa3b0c442b815f04b244440385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting md text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.5260.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Citation context not data related: 10.1111/j.1463-6409.2008.00359.x, _ Zoologica Scripta_, _38_ [(1), 43-62. https](https://doi.org/10.1111/j.1463-6409.2008.00359.x)\n",
      "ID not addded: 10.1111/j.1463-6409.2008.00359.x\n",
      "Citation context not data related: 10.1093/bioinformatics/btu033, RAxML version 8: A tool for phylogenetic analysis and post-analysis of large phylogenies. _ Bioinformatics_, _30_ (9), 1312- [1313. https://doi.org/10.1093/bioinformatics/btu033](https://doi.org/10.1093/bioinformatics/btu033)\n",
      "ID not addded: 10.1093/bioinformatics/btu033\n",
      "Citation context not data related: 10.1111/syen.12244, [org/10.1111/evo.13133](https://doi.org/10.1111/evo.13133) Baca, S. M., Alexander, A., Gustafson, G. T., & Short, A. E. Z. (2017). Ultraconserved elements show utility in phylogenetic inference of Adephaga (Coleoptera) and suggest paraphyly of 'Hydradephaga'. _Systematic Entomology_, _42_ [(4), 786-795. https://doi.org/10.1111/](https://doi.org/10.1111/syen.12244)\n",
      "ID not addded: 10.1111/syen.12244\n",
      "Citation context not data related: MK838511, The rest were obtained from the literature (Gomez, Will, & Maddison, 2016; Kanda, Pflug, Sproul, Dasenko, & Maddison, 2015; Maddison, 2012; Maddison Baker, & Ober, 1999a, 1999b; Maddison et al., 2009; Maddison & Swanson, 2010; McKenna et al., 2015; Ober, 2002; Sproul & Maddison, 2017), or were newly obtained from Sanger sequencing of PCR products, as specified in Supplemental Information Table S1. Methods for obtain- ing the new sequences from PCR/Sanger sequencing are given in Maddison (2012). New sequences have been submitted to GenBank [with accession numbers MK838494 to MK838511.](info:ddbj-embl-genbank/MK838494)\n",
      "ID not addded: MK838511\n",
      "Citation context not data related: 10.1093/molbev/msw056, _ Molecular Biology__and Evolution_, _33_ [(7), 1875-1886. https://doi.org/10.1093/molbev/](https://doi.org/10.1093/molbev/msw056)\n",
      "ID not addded: 10.1093/molbev/msw056\n",
      "Citation context not data related: 10.1093/bioinformatics/btp324, Fast and accurate short read alignment with Burrows-Wheeler transform. _ Bioinformatics_, _25_ [, 1754-1760. https://](https://doi.org/10.1093/bioinformatics/btp324)\n",
      "ID not addded: 10.1093/bioinformatics/btp324\n",
      "Citation context not data related: 10.1016/j.ympev.2011.12.007, Applications of next-generation sequencing to phylo- geography and phylogenetics. _ Molecular Phylogenetics and Evolution_, _66_ [(2), 526-538. https://doi.org/10.1016/j.ympev.2011.12.007](https://doi.org/10.1016/j.ympev.2011.12.007)\n",
      "ID not addded: 10.1016/j.ympev.2011.12.007\n",
      "Citation context not data related: 10.1093/bioinformatics/btv351, Assessing genome assembly and annotation completeness with single-copy orthologs. _ Bioinformatics_, _31_ [(19), 3210-3212. https://doi.org/10.1093/bioinformatics/btv351](https://doi.org/10.1093/bioinformatics/btv351) lipiski, A., Leschen, R. A. B., & Lawrence, J. F. (2011).\n",
      "ID not addded: 10.1093/bioinformatics/btv351\n",
      "Citation context not data related: 10.1111/1755-0998.12655, Molecular_ _ Ecology_ _ Resources_, _17_ (5), 1003-1008. [https://doi.](https://doi.org/10.1111/1755-0998.12655)\n",
      "ID not addded: 10.1111/1755-0998.12655\n",
      "Citation context not data related: 10.1016/S1055-7903, Phylogenetic relationships of the carabid subfam- ily Harpalinae (Coleoptera) based on molecular sequence data. _ Molecular Phylogenetics and Evolution_, _24_ [, 228-248. https://doi.](https://doi.org/10.1016/S1055-7903(02)00251-8) [org/10.1016/S1055-7903(02)00251-8](https://doi.org/10.1016/S1055-7903(02)00251-8) Paijmans, J. L. A., Fickel, J., Courtiol, A., Hofreiter, M., & Frster, D. W. (2016).\n",
      "ID not addded: 10.1016/S1055-7903\n",
      "Citation context not data related: 10.1111/syen.12270, Systematics of aquatic beetles (Coleoptera): Current state and future directions. _ Systematic Entomology_, _43_, 1-18. [https://doi.org/10.1111/syen.12270](https://doi.org/10.1111/syen.12270) Simo, F. A., Waterhouse, R. M., Ioannidis, P., Kriventseva, E. V., & Zdobnov, E. M. (2015).\n",
      "ID not addded: 10.1111/syen.12270\n",
      "Citation context not data related: 10.1038/s41467-017-02644-4, _ Nature Communications_, _9_ [, 205. https](https://doi.org/10.1038/s41467-017-02644-4) [://doi.org/10.1038/s41467-017-02644-4](https://doi.org/10.1038/s41467-017-02644-4) **SUPPORTING INFORMATION**\n",
      "ID not addded: 10.1038/s41467-017-02644-4\n",
      "Citation context not data related: 10.1093/bioinformatics/btv646, PHYLUCE is a software package for the analysis of conserved genomic loci. _ Bioinformatics_, _32_ [(5), 786-788. https://doi.](https://doi.org/10.1093/bioinformatics/btv646)\n",
      "ID not addded: 10.1093/bioinformatics/btv646\n",
      "Citation context not data related: 10.1016/j.ympev.2017.10.013, Evaluating methods for phy- logenomic analyses, and a new phylogeny for a major frog clade (Hyloidea) based on 2214 loci. _ Molecular Phylogenetics and Evolution_, _119_ [, 128-143. https://doi.org/10.1016/j.ympev.2017.10.013](https://doi.org/10.1016/j.ympev.2017.10.013)\n",
      "ID not addded: 10.1016/j.ympev.2017.10.013\n",
      "Citation context not data related: 10.1111/1755-0998.12574, Molecular_ _ Ecology_ _ Resources_, _16_ (5), 1051-1058. [https://doi.](https://doi.org/10.1111/1755-0998.12574)\n",
      "ID not addded: 10.1111/1755-0998.12574\n",
      "Citation context not data related: 10.1002/ece3.5260, Received: 8 April 2019 | Revised: 26 April 2019 | Accepted: 28 April 2019 DOI: 10.1002/ece3.5260 O R\n",
      "ID not addded: 10.1002/ece3.5260\n",
      "Citation context not data related: 10.1186/s12862-015-0552-5, _ BMC Evolutionary Biology_, _15_ [, 271. https](https://doi.org/10.1186/s12862-015-0552-5)\n",
      "ID not addded: 10.1186/s12862-015-0552-5\n",
      "Citation context not data related: 10.1111/1755-0998.12660, Sequencing historical specimens: Successful preparation of small specimens with low amounts of de- graded DNA. _ Molecular Ecology Resources_, _17_ [, 1183-1201. https://](https://doi.org/10.1111/1755-0998.12660)\n",
      "ID not addded: 10.1111/1755-0998.12660\n",
      "Citation context not data related: 10.1016/j.ympev.2012.01.015, Phylogeny of _Bembidion_ and related ground beetles (Coleoptera: Carabidae: Trechinae: Bembidiini: Bembidiina). _ Molecular Phylogenetics and Evolution_, _63_ [, 533-576. https://doi.](https://doi.org/10.1016/j.ympev.2012.01.015)\n",
      "ID not addded: 10.1016/j.ympev.2012.01.015\n",
      "Citation context not data related: 10.1111/1755-0998.12466, _ Molecular Ecology Resources_, _16_ [(5), 1189-1203. https://doi.](https://doi.org/10.1111/1755-0998.12466)\n",
      "ID not addded: 10.1111/1755-0998.12466\n",
      "Citation context not data related: 10.1186/gb-2009-10-10-r116, Enrichment of sequencing targets from the human genome by solution hybridization. _ Genome Biology_, _10_, R116. [https://doi.org/10.1186/gb-2009-10-10-r116](https://doi.org/10.1186/gb-2009-10-10-r116) Toussaint, E. F. A., Seidel, M., Arriaga-Varela, E., Hjek, J., Krl, D., Sekerka, L.,  Fikek, M. (2017).\n",
      "ID not addded: 10.1186/gb-2009-10-10-r116\n",
      "Citation context not data related: 10.1093/molbev/msr148, Multiple genome alignments facilitate development of NPCL markers: A case study of tetrapod phylogeny focusing on the position of turtles. _ Molecular__Biology and Evolution_, _28_ [(12), 3237-3252. https://doi.org/10.1093/](https://doi.org/10.1093/molbev/msr148)\n",
      "ID not addded: 10.1093/molbev/msr148\n",
      "Citation context not data related: 10.1098/rspb.2017.0210, Trait and dispersal evolution in the landfowl (Aves: Galliformes). _ Proceedings of the Royal Society B: Biological Sciences_, _284_ (1854), [20170210. https://doi.org/10.1098/rspb.2017.0210](https://doi.org/10.1098/rspb.2017.0210)\n",
      "ID not addded: 10.1098/rspb.2017.0210\n",
      "Citation context not data related: 10.1093/molbev/msu300, IQ- TREE: A fast and effective stochastic algorithm for estimating max- imum likelihood phylogenies. _ Molecular Biology and Evolution_, _32_ (1), [268-274. https://doi.org/10.1093/molbev/msu300](https://doi.org/10.1093/molbev/msu300)\n",
      "ID not addded: 10.1093/molbev/msu300\n",
      "Citation context not data related: 10.1111/syen.12132, _ Systematic Entomology_, _40_ [, 835-880. https://doi.org/10.1111/syen.12132](https://doi.org/10.1111/syen.12132)\n",
      "ID not addded: 10.1111/syen.12132\n",
      "Citation context not data related: 10.1093/sysbio/sys004, Ultraconserved elements anchor thousands of genetic markers spanning multiple evolu- tionary timescales. _ Systematic Biology_, _61_ [(5), 717-726. https://doi.](https://doi.org/10.1093/sysbio/sys004)\n",
      "ID not addded: 10.1093/sysbio/sys004\n",
      "Citation context not data related: 10.1038/s41598-017-08403-1, _ Scientific Reports_, _7_ [, 8619. https://](https://doi.org/10.1038/s41598-017-08403-1)\n",
      "ID not addded: 10.1038/s41598-017-08403-1\n",
      "Citation context not data related: 10.1111/1755-0998.12420, Impact of enrichment conditions on cross-species capture of fresh and degraded DNA. _ Molecular Ecology Resources_, _16_ (1), 42-55. [https://doi.org/10.1111/1755-0998.12420](https://doi.org/10.1111/1755-0998.12420) Quattrini, A. M., Faircloth, B. C., Dueas, L. F., Bridge, T. C. L., Brugler, M. R., Calixto-Bota, I. F.,  McFadden, C. S. (2018).\n",
      "ID not addded: 10.1111/1755-0998.12420\n",
      "Citation context not data related: 10.1098/rspb.2014.0823, The evolution of peafowl and other taxa with ocelli (eyespots): A phylogenomic approach. _ Proceedings of_ _the Royal Society B: Biological Sciences_, _281_ [, 20140823. https://doi.](https://doi.org/10.1098/rspb.2014.0823) [org/10.1098/rspb.2014.0823](https://doi.org/10.1098/rspb.2014.0823) Talavera, G., & Castresana, J. (2007).\n",
      "ID not addded: 10.1098/rspb.2014.0823\n",
      "Citation context not data related: 10.1046/j.1365-3113.1999.00088.x, Phylogeny of ca- rabid beetles as inferred from 18S ribosomal DNA (Coleoptera: Carabidae). _ Systematic Entomology_, _24_ [, 103-138. https://doi.](https://doi.org/10.1046/j.1365-3113.1999.00088.x)\n",
      "ID not addded: 10.1046/j.1365-3113.1999.00088.x\n",
      "Citation context not data related: 10.1111/1755-0998.12664, _ Molecular Ecology Resources_, _17_ (6), 1342- [1358. https://doi.org/10.1111/1755-0998.12664](https://doi.org/10.1111/1755-0998.12664) Chiari, Y., Cahais, V., Galtier, N., & Delsuc, F. (2012).\n",
      "ID not addded: 10.1111/1755-0998.12664\n",
      "Citation context not data related: 10.1186/s12862-016-0611-6, Phylogenomic data reveal multiple Southeast Asian origins for Indian Dragon Lizards. _ BMC Evolutionary Biology_, _16_ [(1), 43. https://doi.](https://doi.org/10.1186/s12862-016-0611-6)\n",
      "ID not addded: 10.1186/s12862-016-0611-6\n",
      "Citation context not data related: 10.1093/molbev/, MAFFT multiple sequence align- ment software version 7: Improvements in performance and us- ability. _ Molecular Biology and Evolution_, _30_ [(4), 772-780. https://doi.](https://doi.org/10.1093/molbev/mst010)\n",
      "ID not addded: 10.1093/molbev/\n",
      "Citation context not data related: 10.1371/journal.pone.0054848, A phylogeny of birds based on over 1,500 loci collected by target enrichment and high-throughput se- quencing. _PLoS ONE_, _8_ [, e54848. https://doi.org/10.1371/journ](https://doi.org/10.1371/journal.pone.0054848)\n",
      "ID not addded: 10.1371/journal.pone.0054848\n",
      "Citation context not data related: 10.1111/1755-0998.12721, _ Molecular Ecology Resources_, _18_ [(2), 356-361. https://doi.](https://doi.org/10.1111/1755-0998.12721)\n",
      "ID not addded: 10.1111/1755-0998.12721\n",
      "Citation context not data related: 10.1111/1755-0998.12595, _ Molecular Ecology Resources_, _17_ [(3), 508-522. https://doi.org/10.1111/1755-0998.12595](https://doi.org/10.1111/1755-0998.12595)\n",
      "ID not addded: 10.1111/1755-0998.12595\n",
      "Citation context not data related: 10.1093/bioinformatics/btp698, Fast and accurate long-read alignment with Burrows-Wheeler transform. _ Bioinformatics_, _26_ [, 589-595. https://](https://doi.org/10.1093/bioinformatics/btp698)\n",
      "ID not addded: 10.1093/bioinformatics/btp698\n",
      "Citation context not data related: 10.3732/apps.1600016, _ Applications in Plant Sciences_, _4_ (7), [1600016. https://doi.org/10.3732/apps.1600016](https://doi.org/10.3732/apps.1600016)\n",
      "ID not addded: 10.3732/apps.1600016\n",
      "Citation context not data related: 10.1093/oxfordjournals.mol-, Selection of conserved blocks from multiple align- ments for their use in phylogenetic analysis. _ Molecular Biology and_ _Evolution_, _17_ [, 540-552. https://doi.org/10.1093/oxfordjournals.mol-](https://doi.org/10.1093/oxfordjournals.molbev.a026334)\n",
      "ID not addded: 10.1093/oxfordjournals.mol-\n",
      "Citation context not data related: 10.1111/2041-210X.12742, Enriching the ant tree of life: Enhanced UCE bait set for genome-scale phylogenetics of ants and other Hymenoptera. _ Methods in Ecology and_ _Evolution_, _8_ [, 768-776. https://doi.org/10.1111/2041-210X.12742](https://doi.org/10.1111/2041-210X.12742) Bushnell, B. (2014).\n",
      "ID not addded: 10.1111/2041-210X.12742\n",
      "Citation context not data related: 10.1111/evo.13133, _ Evolution_, _71_ [(2), 475-488. https://doi.](https://doi.org/10.1111/evo.13133)\n",
      "ID not addded: 10.1111/evo.13133\n",
      "Citation context not data related: 10.1111/1755-0998.12736, _ Molecular Ecology Resources_, _18_ [(2), 281-295. https://doi.org/10.1111/1755-0998.12736](https://doi.org/10.1111/1755-0998.12736)\n",
      "ID not addded: 10.1111/1755-0998.12736\n",
      "Citation context not data related: 10.1186/s12898-018-0176-x, Quantifying the unquantifiable: Why Hymenoptera, not Coleoptera, is the most speciose animal order. _ BMC Ecology_, _18_, 21. [https://doi.org/10.1186/s12898-018-0176-x](https://doi.org/10.1186/s12898-018-0176-x) Glenn, T. C., & Faircloth, B. C. (2016).\n",
      "ID not addded: 10.1186/s12898-018-0176-x\n",
      "Citation context not data related: 10.1098/rsbl.2017.0393, Phylogenomic analyses of more than 4000 nuclear loci resolve the origin of snakes among lizard families. _ Biology Letters_, _13_ [(9), 20170393. https://doi.org/10.1098/](https://doi.org/10.1098/rsbl.2017.0393)\n",
      "ID not addded: 10.1098/rsbl.2017.0393\n",
      "Citation context not data related: 10.1093/oxfordjournals.molbev.a026334, Selection of conserved blocks from multiple align- ments for their use in phylogenetic analysis. _ Molecular Biology and_ _Evolution_, _17_ [, 540-552. https://doi.org/10.1093/oxfordjournals.mol-](https://doi.org/10.1093/oxfordjournals.molbev.a026334)\n",
      "ID not addded: 10.1093/oxfordjournals.molbev.a026334\n",
      "Citation context not data related: 10.1371/journal.pone.0161531, Sequence capture and phylogenetic utility of genomic ultracon- served elements obtained from pinned insect specimens. _PLoS ONE_, _11_ [, e0161531. https://doi.org/10.1371/journal.pone.0161531](https://doi.org/10.1371/journal.pone.0161531)\n",
      "ID not addded: 10.1371/journal.pone.0161531\n",
      "Citation context not data related: 10.1111/2041-210X.12754, Identifying conserved genomic elements and design- ing universal bait sets to enrich them. _ Methods in Ecology and Evolution_, _8_ [(9), 1103-1112. https://doi.org/10.1111/2041-210X.12754](https://doi.org/10.1111/2041-210X.12754)\n",
      "ID not addded: 10.1111/2041-210X.12754\n",
      "Citation context not data related: 10.1371/journal.pone.0065923, A phylog- enomic perspective on the radiation of ray-finned fishes based upon targeted sequencing of ultraconserved elements (UCEs). _PLoS ONE_, _8_ [, e65923. https://doi.org/10.1371/journal.pone.0065923](https://doi.org/10.1371/journal.pone.0065923)\n",
      "ID not addded: 10.1371/journal.pone.0065923\n",
      "Citation context not data related: 10.2144/000113809, Length and GC-biases during sequenc- ing library amplification: A comparison of various polymerase-buf- fer systems with ancient and modern DNA sequencing libraries. _ BioTechniques_, _52_ [, 87-94. https://doi.org/10.2144/000113809](https://doi.org/10.2144/000113809)\n",
      "ID not addded: 10.2144/000113809\n",
      "Citation context not data related: 10.1098/rsbl.2012.0331, More than 1000 ultraconserved elements provide evidence that turtles are the sister group of ar- chosaurs. _ Biology Letters_, _8_ [(5), 783-786. https://doi.org/10.1098/](https://doi.org/10.1098/rsbl.2012.0331)\n",
      "ID not addded: 10.1098/rsbl.2012.0331\n",
      "Citation context not data related: 10.2144/000114039, Capturing protein-coding genes across highly divergent species. _ BioTechniques_, _54_ [, 321-326. https://doi.org/10.2144/000114039](https://doi.org/10.2144/000114039) Li, H., & Durbin, R. (2009).\n",
      "ID not addded: 10.2144/000114039\n",
      "Citation context not data related: 10.1016/S0168-9525, EMBOSS: The European mo- lecular biology open software suite. _ Trends in Genetics_, _16_ (6), 276- [277. https://doi.org/10.1016/S0168-9525(00)02024-2](https://doi.org/10.1016/S0168-9525(00)02024-2) Ruane, S., & Austin, C. C. (2017).\n",
      "ID not addded: 10.1016/S0168-9525\n",
      "Citation context not data related: 10.1016/j.ympev.2017.09.013, Phylogenomics and species de- limitation of a complex radiation of Neotropical suboscine birds ( _Pachyramphus_ ). _ Molecular Phylogenetics and Evolution_, _118_, 204-221. [https://doi.org/10.1016/j.ympev.2017.09.013](https://doi.org/10.1016/j.ympev.2017.09.013) Nguyen, L.-T., Schmidt, H. A., von Haeseler, A., & Minh, B. Q. (2015).\n",
      "ID not addded: 10.1016/j.ympev.2017.09.013\n",
      "Citation context not data related: 10.1371/journ, Sequence capture and phylogenetic utility of genomic ultracon- served elements obtained from pinned insect specimens. _PLoS ONE_, _11_ [, e0161531. https://doi.org/10.1371/journal.pone.0161531](https://doi.org/10.1371/journal.pone.0161531)\n",
      "ID not addded: 10.1371/journ\n",
      "Citation context not data related: 10.1080/10635150701472164, _ Systematic Biology_, _56_ [, 564-577. https://doi.](https://doi.org/10.1080/10635150701472164)\n",
      "ID not addded: 10.1080/10635150701472164\n",
      "Citation context not data related: 10.1093/bioin, PHYLUCE is a software package for the analysis of conserved genomic loci. _ Bioinformatics_, _32_ [(5), 786-788. https://doi.](https://doi.org/10.1093/bioinformatics/btv646)\n",
      "ID not addded: 10.1093/bioin\n",
      "Citation context not data related: 10.1111/1755-0998.12621, Molecular_ _ Ecology_ _ Resources_, _17_ (4), 812-823. [https://doi.](https://doi.org/10.1111/1755-0998.12621)\n",
      "ID not addded: 10.1111/1755-0998.12621\n",
      "Citation context not data related: 10.1111/1755-0998.12328, _ Molecular Ecology Resources_, _15_ [(3), 489-501. https://](https://doi.org/10.1111/1755-0998.12328)\n",
      "ID not addded: 10.1111/1755-0998.12328\n",
      "Citation context not data related: 10.1093/molbev/mst010, MAFFT multiple sequence align- ment software version 7: Improvements in performance and us- ability. _ Molecular Biology and Evolution_, _30_ [(4), 772-780. https://doi.](https://doi.org/10.1093/molbev/mst010)\n",
      "ID not addded: 10.1093/molbev/mst010\n",
      "Citation context not data related: 10.1101/gr.111120.110, Stampy: A statistical algorithm for sen- sitive and fast mapping of Illumina sequence reads. _ Genome Research_, _21_ [, 936-939. https://doi.org/10.1101/gr.111120.110](https://doi.org/10.1101/gr.111120.110) Maddison, D. R. (2012).\n",
      "ID not addded: 10.1101/gr.111120.110\n",
      "Citation context not data related: 10.1371/journal.pone.0143929, _PLoS ONE_, _10_ [(12), e0143929. https://doi.org/10.1371/journal.pone.0143929](https://doi.org/10.1371/journal.pone.0143929) Katoh, K., & Standley, D. M. (2013).\n",
      "ID not addded: 10.1371/journal.pone.0143929\n",
      "Citation context not data related: MK838494, The rest were obtained from the literature (Gomez, Will, & Maddison, 2016; Kanda, Pflug, Sproul, Dasenko, & Maddison, 2015; Maddison, 2012; Maddison Baker, & Ober, 1999a, 1999b; Maddison et al., 2009; Maddison & Swanson, 2010; McKenna et al., 2015; Ober, 2002; Sproul & Maddison, 2017), or were newly obtained from Sanger sequencing of PCR products, as specified in Supplemental Information Table S1. Methods for obtain- ing the new sequences from PCR/Sanger sequencing are given in Maddison (2012). New sequences have been submitted to GenBank [with accession numbers MK838494 to MK838511.](info:ddbj-embl-genbank/MK838494)\n",
      "ID not addded: MK838494\n",
      "Citation context not data related: 10.1038/nmeth.1419, Target-enrichment strategies for next-generation sequencing. _ Nature Methods_, _7_ [(2), 111-118. https://](https://doi.org/10.1038/nmeth.1419)\n",
      "ID not addded: 10.1038/nmeth.1419\n",
      "Citation context not data related: 10.3897/zookeys.43.390, A preliminary characterization of Bembidion perspicuum LeConte, with a reclassification of related species (Coleoptera, Carabidae) north of Mxico. _ ZooKeys_, _43_, 15- [31. https://doi.org/10.3897/zookeys.43.390](https://doi.org/10.3897/zookeys.43.390) Mamanova, L., Coffey, A. J., Scott, C. E., Kozarewa, I., Turner, E. H., Kumar, A.,  Turner, D. J. (2010).\n",
      "ID not addded: 10.3897/zookeys.43.390\n",
      "Citation context not data related: 10.1111/syen.12198, The peril of dating beetles. _ Systematic__Entomology_, _42_ [(1), 1-10. https://doi.org/10.1111/syen.12198](https://doi.org/10.1111/syen.12198) Van Dam, M. H., Lam, A. W., Sagata, K., Gewa, B., Laufa, R., Balke, M.,  Reidel, A. (2017).\n",
      "ID not addded: 10.1111/syen.12198\n",
      "Citation context not data related: 10.1093/bioinformatics/btw451, Optimization of signal-to-noise ratio for efficient microarray probe design. _ Bioinformatics_, _30_ [(17), i552-i558. https://doi.org/10.1093/bioin](https://doi.org/10.1093/bioinformatics/btw451)\n",
      "ID not addded: 10.1093/bioinformatics/btw451\n",
      "Citation context not data related: 10.1101/gr.107524.110, _ Genome Resources_, _20_ [(9), 1297-1303. https://](https://doi.org/10.1101/gr.107524.110)\n",
      "ID not addded: 10.1101/gr.107524.110\n",
      "Citation context not data related: 10.1093/sysbio/syt061, _ Systematic Biology_, _63_ [(1), 83-95. https://doi.](https://doi.org/10.1093/sysbio/syt061)\n",
      "ID not addded: 10.1093/sysbio/syt061\n",
      "Found 2 citations for 10.1002_ece3.5260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed testing data for 2 article and dataset_id combos.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SubmissionData(article_id='10.1002_ece3.5260', dataset_id='10.5061/dryad.2f62927;', type='Secondary', context='J.S.S., J.M.P., and D.R.M. ob- tained and provided the Illumina and Sanger sequence data, assem- bled the base genomes, and produced genomic assembly metrics. All authors contributed to writing of the manuscript. **DATA AVAILABILITY** DNA sequences: GenBank MK838494 to MK838511; P(info:x-wiley/peptideatlas/MK838494) hyluce BED/ BAM files for probe design experiments and Adephaga_2.9Kv1 final probe set fasta file: Dryad https://doi.org/10.5061/dryad.2f62927;(https://doi.org/10.5061/dryad.2f62927)'),\n",
       " SubmissionData(article_id='10.1002_ece3.5260', dataset_id='10.5061/dryad.2f62927', type='Secondary', context='J.S.S., J.M.P., and D.R.M. ob- tained and provided the Illumina and Sanger sequence data, assem- bled the base genomes, and produced genomic assembly metrics. All authors contributed to writing of the manuscript. **DATA AVAILABILITY** DNA sequences: GenBank MK838494 to MK838511; P(info:x-wiley/peptideatlas/MK838494) hyluce BED/ BAM files for probe design experiments and Adephaga_2.9Kv1 final probe set fasta file: Dryad https://doi.org/10.5061/dryad.2f62927;(https://doi.org/10.5061/dryad.2f62927)')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_sub = process_test_articles(tokenizer, sample_test_file_paths_df)\n",
    "display(sample_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09442e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    " https  ://doi.org/10.5061/dryad.2f62927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbddd2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.2. Preprocess Test Data (similar to training data)\n",
    "test_articles_data: Dict[str, ArticleData] = {}\n",
    "# Assuming test data structure is similar to train data (full text files)\n",
    "for article_file in os.listdir(TEST_DATA_DIR):\n",
    "    article_id = os.path.splitext(article_file)[0]\n",
    "    filepath = os.path.join(TEST_DATA_DIR, article_file)\n",
    "    \n",
    "    full_text = extract_text_from_file(filepath)\n",
    "    \n",
    "    # Extract title, author, abstract (same as training)\n",
    "    title_match = re.search(r\"Title:\\s*(.*)\", full_text, re.IGNORECASE)\n",
    "    title = title_match.group(1).strip() if title_match else \"Unknown Title\"\n",
    "    author_match = re.search(r\"Author(?:s)?:\\s*(.*)\", full_text, re.IGNORECASE)\n",
    "    author = author_match.group(1).strip() if author_match else \"Unknown Author\"\n",
    "    abstract_match = re.search(r\"Abstract\\s*(.*?)(?=\\n\\n|\\Z)\", full_text, re.IGNORECASE | re.DOTALL)\n",
    "    abstract = abstract_match.group(1).strip() if abstract_match else \"No Abstract\"\n",
    "\n",
    "    article_data = ArticleData(\n",
    "        article_id=article_id,\n",
    "        title=title,\n",
    "        author=author,\n",
    "        abstract=abstract\n",
    "    )\n",
    "    # For test data, we need to find *all* potential dataset IDs, not just ground truth\n",
    "    # This is the \"finding datasets\" part of your goal.\n",
    "    \n",
    "    # Use regex to find all potential dataset IDs in the full text\n",
    "    found_dataset_mentions = []\n",
    "    for pattern in ALL_ID_PATTERNS:\n",
    "        for match in re.finditer(pattern, full_text, re.IGNORECASE):\n",
    "            dataset_id = match.group(1) if pattern == DOI_PATTERN else match.group(0)\n",
    "            span_text = match.group(0) # The full matched text\n",
    "            \n",
    "            context = extract_context_around_id(full_text, span_text, window_size_sentences=3)\n",
    "            \n",
    "            if context:\n",
    "                dc = DatasetCitation()\n",
    "                dc.add_dataset_id(dataset_id)\n",
    "                dc.set_citation_context(context)\n",
    "                found_dataset_mentions.append(dc)\n",
    "                \n",
    "    article_data.dataset_citations = found_dataset_mentions # Assign found mentions\n",
    "    test_articles_data[article_id] = article_data\n",
    "\n",
    "print(f\"Prepared {len(test_articles_data)} test articles for inference.\")\n",
    "\n",
    "# 8.3. Generate Predictions\n",
    "predictions = []\n",
    "true_labels = [] # Only if you have a test_labels.json for evaluation\n",
    "\n",
    "for article_id, article_data in test_articles_data.items():\n",
    "    for dc in article_data.dataset_citations:\n",
    "        # Create the prompt for inference\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert assistant for classifying research data citations.\"},\n",
    "            {\"role\": \"user\", \"content\": (\n",
    "                f\"Given the following article context and a specific data citation, classify if the data was generated as 'Primary' (newly generated for this study) or 'Secondary' (reused from existing records).\\n\\n\"\n",
    "                f\"Article Title: {article_data.title}\\n\"\n",
    "                f\"Article Abstract: {article_data.abstract}\\n\"\n",
    "                f\"Data Citation Context: {dc.citation_context}\\n\"\n",
    "                f\"Dataset ID: {list(dc.dataset_ids)[0]}\\n\\n\" # Assuming one ID per citation\n",
    "                f\"Classification:\"\n",
    "            )}\n",
    "        ]\n",
    "        \n",
    "        input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=10, # Expecting \"Primary\" or \"Secondary\"\n",
    "                do_sample=False, # Use greedy decoding as per your preference\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Post-process the generated text to get the classification\n",
    "        predicted_type = \"Unknown\"\n",
    "        if \"Primary\" in generated_text:\n",
    "            predicted_type = \"Primary\"\n",
    "        elif \"Secondary\" in generated_text:\n",
    "            predicted_type = \"Secondary\"\n",
    "        \n",
    "        predictions.append({\n",
    "            \"article_id\": article_id,\n",
    "            \"dataset_id\": list(dc.dataset_ids)[0],\n",
    "            \"predicted_type\": predicted_type\n",
    "        })\n",
    "\n",
    "        # If you have test labels, you can collect true_labels here for evaluation\n",
    "        # For Kaggle, you'll typically submit predictions without knowing test labels.\n",
    "\n",
    "print(f\"Generated {len(predictions)} predictions.\")\n",
    "\n",
    "# 8.4. Evaluation (if test labels are available)\n",
    "# If you have a separate test_labels.json for local evaluation:\n",
    "# test_labels = load_labels(TEST_LABELS_PATH) # Load test labels\n",
    "#\n",
    "# # Match predictions to true labels and calculate metrics\n",
    "# # This part requires careful matching of dataset_id within article_id\n",
    "# # and might involve fuzzy matching for context if exact span isn't available.\n",
    "# # For simplicity, assuming exact match on article_id and dataset_id.\n",
    "#\n",
    "# y_true = []\n",
    "# y_pred = []\n",
    "#\n",
    "# for pred_entry in predictions:\n",
    "#     article_id = pred_entry[\"article_id\"]\n",
    "#     dataset_id = pred_entry[\"dataset_id\"]\n",
    "#     predicted_type = pred_entry[\"predicted_type\"]\n",
    "#\n",
    "#     # Find the true label for this specific dataset_id in this article\n",
    "#     found_true_label = False\n",
    "#     if article_id in test_labels:\n",
    "#         for gt_info in test_labels[article_id]:\n",
    "#             if gt_info[\"dataset_id\"] == dataset_id: # Exact match on ID\n",
    "#                 y_true.append(gt_info[\"citation_type\"])\n",
    "#                 y_pred.append(predicted_type)\n",
    "#                 found_true_label = True\n",
    "#                 break\n",
    "#     if not found_true_label:\n",
    "#         # Handle cases where a predicted ID might not be in ground truth\n",
    "#         # or where the ID extraction was imperfect.\n",
    "#         # For competition, this means your ID extraction needs to be precise.\n",
    "#         pass\n",
    "#\n",
    "# if y_true and y_pred:\n",
    "#     from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "#     # Map \"Primary\" to 1, \"Secondary\" to 0 for sklearn metrics\n",
    "#     label_map = {\"Primary\": 1, \"Secondary\": 0}\n",
    "#     y_true_mapped = [label_map.get(l, -1) for l in y_true]\n",
    "#     y_pred_mapped = [label_map.get(l, -1) for l in y_pred]\n",
    "#\n",
    "#     # Filter out -1 if there were unknown labels\n",
    "#     valid_indices = [i for i, val in enumerate(y_true_mapped) if val != -1 and y_pred_mapped[i] != -1]\n",
    "#     y_true_mapped = [y_true_mapped[i] for i in valid_indices]\n",
    "#     y_pred_mapped = [y_pred_mapped[i] for i in valid_indices]\n",
    "#\n",
    "#     if y_true_mapped:\n",
    "#         print(\"\\nEvaluation Results:\")\n",
    "#         print(f\"Accuracy: {accuracy_score(y_true_mapped, y_pred_mapped):.4f}\")\n",
    "#         print(f\"F1 Score (weighted): {f1_score(y_true_mapped, y_pred_mapped, average='weighted'):.4f}\")\n",
    "#         print(f\"Precision (weighted): {precision_score(y_true_mapped, y_pred_mapped, average='weighted'):.4f}\")\n",
    "#         print(f\"Recall (weighted): {recall_score(y_true_mapped, y_pred_mapped, average='weighted'):.4f}\")\n",
    "#     else:\n",
    "#         print(\"No matching true labels found for evaluation.\")\n",
    "# else:\n",
    "#     print(\"Not enough data to perform evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef4754",
   "metadata": {},
   "source": [
    "#### 9. Submission File Generation (Kaggle Specific)\n",
    "\n",
    "Finally, format your predictions into the required `submission.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1. Create Submission DataFrame\n",
    "\n",
    "submission_df = pd.DataFrame(predictions)\n",
    "# Rename columns to match Kaggle's expected format (e.g., 'id', 'class_label')\n",
    "# This will depend on the exact submission format specified by Kaggle.\n",
    "# Example:\n",
    "# submission_df = submission_df.rename(columns={\"article_id\": \"Id\", \"dataset_id\": \"DatasetId\", \"predicted_type\": \"Type\"})\n",
    "# submission_df[\"Id\"] = submission_df[\"Id\"] + \"_\" + submission_df[\"DatasetId\"] # If Id is a combination\n",
    "\n",
    "# Assuming the submission format is a list of dictionaries with 'article_id', 'dataset_id', 'citation_type'\n",
    "# You might need to adjust this based on the exact competition requirements.\n",
    "# For example, if it expects a single ID column like \"article_id_dataset_id\"\n",
    "final_submission_data = []\n",
    "for pred in predictions:\n",
    "    final_submission_data.append({\n",
    "        \"Id\": f\"{pred['article_id']}_{pred['dataset_id']}\", # Example: combine IDs\n",
    "        \"Type\": pred['predicted_type']\n",
    "    })\n",
    "\n",
    "final_submission_df = pd.DataFrame(final_submission_data)\n",
    "final_submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf893421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(tp, fp, fn):\n",
    "    return 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n",
    "    \n",
    "    \n",
    "# if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "pred_df = submission_df.copy()\n",
    "label_df = pd.read_csv(\"./kaggle/input/make-data-count-finding-data-references/sample_submission.csv\")\n",
    "label_df = label_df[label_df['type'] != 'Missing'].reset_index(drop=True)\n",
    "\n",
    "hits_df = label_df.merge(pred_df, on=[\"article_id\", \"dataset_id\", \"type\"])\n",
    "\n",
    "tp = hits_df.shape[0]\n",
    "fp = pred_df.shape[0] - tp\n",
    "fn = label_df.shape[0] - tp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
