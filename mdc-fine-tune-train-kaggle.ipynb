{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3195093d",
   "metadata": {},
   "source": [
    "#### 1. Setup and Dependencies\n",
    "\n",
    "First, we'll ensure all necessary libraries are installed. Given your previous work with `lxml`, `PyMuPDF`, and `spaCy`, we'll include those for text extraction and potentially more advanced NLP preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ca2607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# 1.1. Install necessary libraries\n",
    "# Use !pip install for notebook environment\n",
    "# !pip install transformers trl accelerate bitsandbytes sentencepiece lxml PyMuPDF spacy peft\n",
    "# !python -m spacy download en_core_web_sm # Download a small spaCy model\n",
    "\n",
    "# 1.2. Import Libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Set, List, Optional, Dict, Any\n",
    "\n",
    "import fitz # PyMuPDF\n",
    "from lxml import etree # For XML parsing\n",
    "import spacy\n",
    "import kagglehub\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset #, load_metric\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import gc # For garbage collection\n",
    "\n",
    "# For KaggleHub integration (assuming it's set up or models are downloaded)\n",
    "# You might need to install kagglehub if you plan to use it directly for model download\n",
    "# !pip install kagglehub\n",
    "\n",
    "# 1.3. Configure CUDA for local GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available! Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache() # Clear GPU memory\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b02acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for file paths and model configurations\n",
    "BASE_INPUT_DIR = './kaggle/input/make-data-count-finding-data-references'\n",
    "BASE_OUTPUT_DIR = \"./kaggle/working\"\n",
    "\n",
    "# Define directories for articles in train and test sets\n",
    "TRAIN_DATA_DIR = os.path.join(BASE_INPUT_DIR, 'train')\n",
    "TEST_DATA_DIR = os.path.join(BASE_INPUT_DIR, 'test')\n",
    "TRAIN_LABELS_PATH = os.path.join(BASE_INPUT_DIR, 'train_labels.csv')\n",
    "\n",
    "# Define the base model path\n",
    "QWEN_BASE_MODEL_PATH = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "\n",
    "# Output directory for the fine-tuned model and results\n",
    "FINE_TUNED_MODEL_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, \"results\")\n",
    "SAMPLE_SUBMISSION_PATH = os.path.join(BASE_OUTPUT_DIR, \"submission.csv\")\n",
    "\n",
    "# Load spaCy model for sentence segmentation and potentially other NLP tasks\n",
    "# python -m spacy download en_core_web_sm \n",
    "NLP_SPACY = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d8b9daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Extraction (IE) - Dataset Identification ---\n",
    "NON_STD_UNICODE_DASHES = re.compile(r'[\\u2010\\u2011\\u2012\\u2013\\u2014]')\n",
    "NON_STD_UNICODE_TICKS = re.compile(r'[\\u201c\\u201d]')\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input text by removing non-standard unicode dashes and extra whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Replace all non-standard unicode dashes with '-'\n",
    "    text = text.replace('\\u200b', '').replace('-\\n', '-').replace('_\\n', '_').replace('/\\n', '/').replace('dryad.\\n', 'dryad.').replace('doi.\\norg', 'doi.org')\n",
    "    text = NON_STD_UNICODE_DASHES.sub('-', text)\n",
    "    text = NON_STD_UNICODE_TICKS.sub(\"'\", text)\n",
    "    # Remove extra whitespace\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Information Extraction (IE) - Dataset Identification\n",
    "# Regex patterns for common dataset identifiers\n",
    "DOI_PATTERN = r'\\b10\\.\\d{4,5}\\/[-._\\/:A-Za-z0-9]+'\n",
    "#DOI_PATTERN = r\"(?:doi:|https?://(?:dx\\.)?doi\\.org/)(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+)\"\n",
    "EPI_PATTERN = r'\\bEPI[-_A-Z0-9]{2,}'\n",
    "SAM_PATTERN = r'\\bSAMN[0-9]{2,}'          # SAMN07159041\n",
    "IPR_PATTERN = r'\\bIPR[0-9]{2,}'\n",
    "CHE_PATTERN = r'\\bCHEMBL[0-9]{2,}'\n",
    "PRJ_PATTERN = r'\\bPRJ[A-Z0-9]{2,}'\n",
    "E_G_PATTERN = r'\\bE-[A-Z]{4}-[0-9]{2,}'   # E-GEOD-19722 or E-PROT-100\n",
    "ENS_PATTERN = r'\\bENS[A-Z]{4}[0-9]{2,}'\n",
    "CVC_PATTERN = r'\\bCVCL_[A-Z0-9]{2,}'\n",
    "EMP_PATTERN = r'\\bEMPIAR-[0-9]{2,}'\n",
    "PXD_PATTERN = r'\\bPXD[0-9]{2,}'\n",
    "HPA_PATTERN = r'\\bHPA[0-9]{2,}'\n",
    "SRR_PATTERN = r'\\bSRR[0-9]{2,}'\n",
    "GSE_PATTERN = r'\\b(GSE|GSM|GDS|GPL)\\d{4,6}\\b' # Example for GEO accession numbers (e.g., GSE12345, GSM12345)\n",
    "GNB_PATTERN = r'\\b[A-Z]{1,2}\\d{5,6}\\b' # GenBank accession numbers (e.g., AB123456, AF000001)\n",
    "CAB_PATTERN = r'\\bCAB[0-9]{2,}'\n",
    "PDB_PATTERN = r\"\\bpdb\\s*\\d[A-Za-z0-9]{3}\" # Example: pdb 5yfp\n",
    "\n",
    "# Combine all patterns into a list\n",
    "DATASET_ID_PATTERNS = [\n",
    "    DOI_PATTERN,\n",
    "    EPI_PATTERN,\n",
    "    SAM_PATTERN,\n",
    "    IPR_PATTERN,\n",
    "    CHE_PATTERN,\n",
    "    PRJ_PATTERN,\n",
    "    E_G_PATTERN,\n",
    "    ENS_PATTERN,\n",
    "    CVC_PATTERN,\n",
    "    EMP_PATTERN,\n",
    "    PXD_PATTERN,\n",
    "    HPA_PATTERN,\n",
    "    SRR_PATTERN,\n",
    "    GSE_PATTERN,\n",
    "    GNB_PATTERN,\n",
    "    CAB_PATTERN,\n",
    "    PDB_PATTERN\n",
    "]\n",
    "\n",
    "# Compile all patterns for efficiency\n",
    "COMPILED_DATASET_ID_REGEXES = [re.compile(p) for p in DATASET_ID_PATTERNS]\n",
    "\n",
    "# Data related keywords to look for in the text\n",
    "# These keywords help to ensure that the text is relevant to datasets\n",
    "#DATA_RELATED_KEYWORDS = ['data release', 'data associated', 'data availability', 'data access', 'download', 'program data', 'the data', 'dataset', 'database', 'repository', 'data source', 'data access', 'archive', 'arch.', 'digital']\n",
    "# DATA_RELATED_KEYWORDS = ['data ', 'dataset', 'database', 'download', 'repository', 'archive', 'arch.', 'digital']\n",
    "#DATA_RELATED_KEYWORDS = ['data ', 'dataset', 'database']\n",
    "DATA_RELATED_KEYWORDS = ['data release', 'data associated', 'data availability', 'data access', 'data source', 'program data', 'our data', 'the data', 'dataset', 'database',]\n",
    "\n",
    "def is_text_data_related(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in DATA_RELATED_KEYWORDS)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914e15c",
   "metadata": {},
   "source": [
    "#### 2. Data Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "701fe0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1. DatasetCitation Class\n",
    "@dataclass\n",
    "class DatasetCitation:\n",
    "    dataset_id: str = \"\"\n",
    "    citation_context: str = \"\"\n",
    "    citation_type: Optional[str] = None # \"Primary\", \"Secondary\", or \"Missing\" - for ground truth during training\n",
    "\n",
    "    def set_citation_context(self, context: str, check_data_related: bool = False):\n",
    "        \"\"\"Sets the citation context, cleaning it.\"\"\"\n",
    "        if context and (not check_data_related or not self.is_doi or is_text_data_related(context)):\n",
    "            # Replace newlines with spaces, remove brackets, and normalize whitespace\n",
    "            context = context.replace('\\n', ' ').replace('[', '').replace(']', '')\n",
    "            context = re.sub(r'\\s+', ' ', context.strip())\n",
    "            self.citation_context = context \n",
    "\n",
    "    def is_doi(self)-> bool:\n",
    "        return self.dataset_id.startswith(\"10.\")\n",
    "    \n",
    "    def has_dataset(self) -> bool:\n",
    "        \"\"\"Returns True if there are both dataset IDs and citation context.\"\"\"\n",
    "        return bool(self.dataset_id and self.citation_context.strip())\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "# 2.2. ArticleData Class\n",
    "@dataclass\n",
    "class ArticleData:\n",
    "    article_id: str = \"\"\n",
    "    article_doi: str = \"\"\n",
    "    title: str = \"\"\n",
    "    author: str = \"\"\n",
    "    abstract: str = \"\"\n",
    "    dataset_citations: List[DatasetCitation] = field(default_factory=list)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Custom initialization\n",
    "        if self.article_id and not self.article_doi:\n",
    "            # If article_id is provided but not article_doi, set article_doi\n",
    "            self.article_doi = self.article_id.replace(\"_\", \"/\").lower()\n",
    "\n",
    "    def add_dataset_citation(self, dataset_citation: DatasetCitation):\n",
    "        \"\"\"Adds a DatasetCitation object to the article.\"\"\"\n",
    "        if dataset_citation.has_dataset() and dataset_citation.dataset_id != self.article_doi:\n",
    "            self.dataset_citations.append(dataset_citation)\n",
    "        \n",
    "    def to_dict(self):\n",
    "        d = asdict(self)\n",
    "        # Convert list of DatasetCitation objects to their dict representation\n",
    "        d[\"dataset_citations\"] = [dc.to_dict() for dc in self.dataset_citations]\n",
    "        return d\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), separators=(',', ':'))\n",
    "\n",
    "    def has_data(self) -> bool:\n",
    "        \"\"\"Returns True if there are any dataset citations.\"\"\"\n",
    "        return bool(self.dataset_citations)\n",
    "    \n",
    "@dataclass\n",
    "class LlmTrainingData:\n",
    "    article_id: str = \"\"\n",
    "    article_doi: str = \"\"\n",
    "    article_abstract: str = \"\"\n",
    "    citation_context: str = \"\"\n",
    "    dataset_id: str = \"\"\n",
    "    label: str = \"\"\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), separators=(',', ':'))\n",
    "\n",
    "@dataclass\n",
    "class SubmissionData:\n",
    "    article_id: str = \"\"\n",
    "    dataset_id: str = \"\"\n",
    "    type: str = \"\"\n",
    "    context: str = \"\"\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), separators=(',', ':'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b4259",
   "metadata": {},
   "source": [
    "#### 3. Data Loading and Initial Preprocessing\n",
    "\n",
    "This section will cover how to load the raw competition data (full text articles and labels) and begin structuring it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5d12b",
   "metadata": {},
   "source": [
    "#### Load Labeled Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "438f57fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labeled training data from: ./kaggle/input/make-data-count-finding-data-references\\train_labels.csv\n",
      "Training labels shape: (1028, 3)\n",
      "Example grouped training data for article_id '10.1002_2017jc013030': [{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]\n",
      "Files paths shape: (262, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ground_truth_dataset_info",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "84d718a9-e597-48c1-91f3-d61d6853350e",
       "rows": [
        [
         "3",
         "10.1111_2041-210x.12453",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_2041-210x.12453.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\train\\XML\\10.1111_2041-210x.12453.xml",
         "train",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.h4f7p', 'type': 'Primary'}]"
        ],
        [
         "22",
         "10.1186_s12920-020-00737-6",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12920-020-00737-6.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\train\\XML\\10.1186_s12920-020-00737-6.xml",
         "train",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]"
        ],
        [
         "98",
         "10.1186_s13750-020-0184-0",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13750-020-0184-0.pdf",
         "",
         "train",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>ground_truth_dataset_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1111_2041-210x.12453</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>train</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.1186_s12920-020-00737-6</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>train</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>10.1186_s13750-020-0184-0</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td></td>\n",
       "      <td>train</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    article_id  \\\n",
       "3      10.1111_2041-210x.12453   \n",
       "22  10.1186_s12920-020-00737-6   \n",
       "98   10.1186_s13750-020-0184-0   \n",
       "\n",
       "                                        pdf_file_path  \\\n",
       "3   ./kaggle/input/make-data-count-finding-data-re...   \n",
       "22  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "98  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                        xml_file_path dataset_type  \\\n",
       "3   ./kaggle/input/make-data-count-finding-data-re...        train   \n",
       "22  ./kaggle/input/make-data-count-finding-data-re...        train   \n",
       "98                                                           train   \n",
       "\n",
       "                            ground_truth_dataset_info  \n",
       "3   [{'dataset_id': 'https://doi.org/10.5061/dryad...  \n",
       "22     [{'dataset_id': 'Missing', 'type': 'Missing'}]  \n",
       "98     [{'dataset_id': 'Missing', 'type': 'Missing'}]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_file_paths(dataset_type_dir: str) -> pd.DataFrame: \n",
    "    pdf_path = os.path.join(dataset_type_dir, 'PDF')\n",
    "    xml_path = os.path.join(dataset_type_dir, 'XML')\n",
    "    dataset_type = os.path.basename(dataset_type_dir)\n",
    "    pdf_files = [f for f in os.listdir(pdf_path) if f.endswith('.pdf')]\n",
    "    xml_files = [f for f in os.listdir(xml_path) if f.endswith('.xml')]\n",
    "    df_pdf = pd.DataFrame({\n",
    "        'article_id': [f.replace('.pdf', '') for f in pdf_files],\n",
    "        'pdf_file_path': [os.path.join(pdf_path, f) for f in pdf_files]\n",
    "    })\n",
    "    df_xml = pd.DataFrame({\n",
    "        'article_id': [f.replace('.xml', '') for f in xml_files],\n",
    "        'xml_file_path': [os.path.join(xml_path, f) for f in xml_files]\n",
    "    })\n",
    "    merge_df = pd.merge(df_pdf, df_xml, on='article_id', how='outer', suffixes=('_pdf', '_xml'), validate=\"one_to_many\")\n",
    "    merge_df['dataset_type'] = dataset_type\n",
    "    return merge_df\n",
    "\n",
    "# Load the labeled training data CSV file\n",
    "print(f\"Loading labeled training data from: {TRAIN_LABELS_PATH}\")\n",
    "train_labels_df = pd.read_csv(TRAIN_LABELS_PATH)\n",
    "print(f\"Training labels shape: {train_labels_df.shape}\")\n",
    "\n",
    "# Group training data by article_id to get all datasets for each article\n",
    "# This creates a dictionary where keys are article_ids and values are lists of dataset dicts\n",
    "grouped_training_data = {}\n",
    "for article_id, group_df in train_labels_df.groupby('article_id'):\n",
    "    grouped_training_data[article_id] = group_df[['dataset_id', 'type']].to_dict('records')\n",
    "\n",
    "# Example usage of grouped_training_data\n",
    "print(f\"Example grouped training data for article_id '10.1002_2017jc013030': {grouped_training_data['10.1002_2017jc013030']}\")\n",
    "\n",
    "# Just for testing, always set to the TEST_DATA_DIR\n",
    "base_file_dir = TRAIN_DATA_DIR\n",
    "\n",
    "# Load file paths for base directory\n",
    "file_paths_df = load_file_paths(base_file_dir)\n",
    "file_paths_df['xml_file_path'] = file_paths_df['xml_file_path'].fillna('')\n",
    "\n",
    "# Merge the file paths with the grouped_training_data\n",
    "file_paths_df['ground_truth_dataset_info'] = file_paths_df['article_id'].map(grouped_training_data)\n",
    "file_paths_df['ground_truth_dataset_info'] = file_paths_df['ground_truth_dataset_info'].fillna('')\n",
    "\n",
    "# Reduce the file paths DataFrame to only those with ground truth dataset info and get a sample\n",
    "# This is to ensure we have a manageable dataset for training\n",
    "file_paths_df = file_paths_df[file_paths_df['ground_truth_dataset_info'].astype(bool)]\n",
    "file_paths_df = file_paths_df.reset_index(drop=True)\n",
    "file_paths_df = file_paths_df.sample(frac=.5, random_state=42).reset_index(drop=True)  # Shuffle the DataFrame\n",
    "print(f\"Files paths shape: {file_paths_df.shape}\")\n",
    "display(file_paths_df.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f0763",
   "metadata": {},
   "source": [
    "#### Define File Extract Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Helper function to extract text from various file types\n",
    "def extract_text_from_file(filepath: str) -> str:\n",
    "    \"\"\"Extracts text from XML, PDF, or TXT files.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return \"\"\n",
    "    \n",
    "    print(f\"Extracting text from file: {filepath}\")\n",
    "    if filepath.endswith(\".xml\"):\n",
    "        parser = etree.XMLParser(resolve_entities=False, no_network=True)\n",
    "        try:\n",
    "            tree = etree.parse(filepath, parser)\n",
    "            # A common way to get all text from an XML scientific article\n",
    "            # This might need adjustment based on the specific XML schema\n",
    "            return clean_text(\" \".join(tree.xpath(\"//text()\")).strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing XML {filepath}: {e}\")\n",
    "            return \"\"\n",
    "    elif filepath.endswith(\".pdf\"):\n",
    "        try:\n",
    "            doc = fitz.open(filepath)\n",
    "            text = \"\"\n",
    "            for page in doc:\n",
    "                text += page.get_textpage().extractTEXT()+\"\\n\"\n",
    "            return clean_text(text.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing PDF {filepath}: {e}\")\n",
    "            return \"\"\n",
    "    elif filepath.endswith(\".txt\"):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read().strip()\n",
    "    return \"\"\n",
    "\n",
    "def extract_first_few_sentences(text: str, num_sentences: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the first few sentences from the text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        num_sentences (int): The number of sentences to extract.\n",
    "        \n",
    "    Returns:\n",
    "        str: The first few sentences from the text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    doc = NLP_SPACY(text)\n",
    "    sentences = list(doc.sents)\n",
    "    \n",
    "    # Join the first few sentences\n",
    "    return \" \".join([sent.text for sent in sentences[:num_sentences]]).strip()\n",
    "\n",
    "def extract_article_data_from_text(full_text: str, article_id: str) -> ArticleData:\n",
    "    \"\"\"\n",
    "    Extracts article data from the full text.\n",
    "    \n",
    "    Args:\n",
    "        full_text (str): The full text of the article.\n",
    "        article_id (str): The ID of the article.\n",
    "        \n",
    "    Returns:\n",
    "        ArticleData: An instance of ArticleData with extracted information.\n",
    "    \"\"\"\n",
    "    abstract_match = re.search(r\"Abstract\\s*(.*?)(?=\\n\\n|\\Z)\", full_text, re.IGNORECASE | re.DOTALL)\n",
    "    abstract = abstract_match.group(1).strip() if abstract_match else \"No Abstract\"\n",
    "    abstract = extract_first_few_sentences(abstract[:400], num_sentences=3)  # Extract first few sentences for the abstract\n",
    "\n",
    "    return ArticleData(\n",
    "        article_id=article_id,\n",
    "        abstract=abstract\n",
    "    )\n",
    "\n",
    "# 4.2. Function to extract context around an ID\n",
    "def extract_context_around_id(sentences, dataset_id: str, window_size_sentences: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Extracts a window of sentences around a given dataset ID in the text.\n",
    "    Uses spaCy for sentence segmentation.\n",
    "    \"\"\"\n",
    "    if not sentences or not dataset_id or dataset_id == \"Missing\":\n",
    "        return \"\"\n",
    "        \n",
    "    # Find all occurrences of the dataset_id (case-insensitive)\n",
    "    matches = [(i, sent) for i, sent in enumerate(sentences) if dataset_id.lower() in sent.lower()]\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "\n",
    "    # For simplicity, take the context around the first match.\n",
    "    # You might want to refine this to capture all relevant contexts or the most prominent one.\n",
    "    first_match_idx = matches[0][0]\n",
    "    \n",
    "    start_idx = max(0, first_match_idx - window_size_sentences)\n",
    "    end_idx = min(len(sentences), first_match_idx + 1)\n",
    "    \n",
    "    context_sentences = sentences[start_idx:end_idx]\n",
    "    return \" \".join(context_sentences)\n",
    "\n",
    "\n",
    "def extract_training_data_for_llm(file_paths_df: pd.DataFrame) -> list[dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extracts article data for training set with ground truth.\n",
    "    \n",
    "    Args:\n",
    "        file_paths_df (pd.DataFrame): DataFrame containing file paths and ground truth info.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, ArticleData]: Dictionary mapping article IDs to ArticleData objects.\n",
    "    \"\"\"\n",
    "    training_data_for_llm: list[dict[str, str]] = [] # This will be a list of LlmTrainingData for the LLM training dataset\n",
    "    for i, row in tqdm(file_paths_df.iterrows(), total=len(file_paths_df)):\n",
    "        article_id = row['article_id']\n",
    "        filepath = row['pdf_file_path'] if row['pdf_file_path'] else row['xml_file_path']\n",
    "        ground_truth_list = row['ground_truth_dataset_info'] if 'ground_truth_dataset_info' in row else []\n",
    "        \n",
    "        full_text = extract_text_from_file(filepath)\n",
    "        article_data = extract_article_data_from_text(full_text, article_id)\n",
    "\n",
    "        doc = NLP_SPACY(full_text)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "        if not ground_truth_list:\n",
    "            print(f\"No ground truth data found for article_id: {article_id}. Skipping this article.\")\n",
    "            continue\n",
    "        for gt in ground_truth_list:\n",
    "            dataset_id = gt['dataset_id'].replace(\"https://doi.org/\", \"\").replace(\"doi:\", \"\").strip()\n",
    "            citation_type = gt.get('type', 'Primary')\n",
    "            if dataset_id:\n",
    "                # Convert to dict for LLM training data\n",
    "                training_data_for_llm.append(\n",
    "                    {\n",
    "                        \"article_id\": article_data.article_id,\n",
    "                        \"article_doi\": article_data.article_doi,\n",
    "                        \"article_abstract\": article_data.abstract,\n",
    "                        \"citation_context\": extract_context_around_id(sentences, dataset_id),\n",
    "                        \"dataset_id\": dataset_id,\n",
    "                        \"label\": citation_type\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    print(f\"Loaded training data for {len(training_data_for_llm)} articles.\")\n",
    "    return training_data_for_llm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aeda081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ground_truth_dataset_info",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "3267ea33-5fd5-4dab-82e2-10d584fc60e2",
       "rows": [
        [
         "154",
         "10.1002_esp.5058",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_esp.5058.pdf",
         "",
         "train",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.jh9w0vt9t', 'type': 'Primary'}]"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>ground_truth_dataset_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>10.1002_esp.5058</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td></td>\n",
       "      <td>train</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           article_id                                      pdf_file_path  \\\n",
       "154  10.1002_esp.5058  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "    xml_file_path dataset_type  \\\n",
       "154                      train   \n",
       "\n",
       "                             ground_truth_dataset_info  \n",
       "154  [{'dataset_id': 'https://doi.org/10.5061/dryad...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing, let's extract training data for a specific article\n",
    "sample_file_paths_df = file_paths_df.loc[file_paths_df['article_id'] == '10.1002_esp.5058']\n",
    "sample_file_paths_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef756a4e",
   "metadata": {},
   "source": [
    "#### 4. Advanced Preprocessing: Extracting Dataset Mentions and Context (Training)\n",
    "\n",
    "Use regex to find the given dataset IDs from the training_labels and then use spaCy to extract surrounding sentences as context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f23263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b56dc177a3468c84db329b62108b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/262 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7717_peerj.12422.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0198382.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_chem.202000235.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_2041-210x.12453.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41597-019-0101-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12974-020-01860-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7717_peerj.13193.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3390_s23177333.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_22221751.2020.1738277.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1103_physrevresearch.4.023008.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ece3.3985.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_tc-17-3617-2023.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41467-019-10357-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1980-5918.033.ao15.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3390_molecules191017026.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536808011148.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_anie.202005531.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2052252514012081.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1364_oe.25.001985.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1098_rspb.2015.1498.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13046-018-0843-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1414-431x20198292.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12920-020-00737-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12916-019-1469-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13072-019-0322-5.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12920-018-0426-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12868-018-0468-2.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3133_ofr20231027.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1980-5373-mr-2018-0921.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_essd-8-663-2016.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7554_elife.74937.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1021_acsomega.3c06074.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5937_bnhmb1811227u.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13104-018-4014-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1007_jhep11(2018)115.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41467-018-04041-x.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7554_elife.29944.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_hdy.2014.75.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_chem.202003167.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41597-022-01555-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1055_s-0039-1693681.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1073_pnas.1711872115.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12913-018-3333-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1098_rspb.2015.2726.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1980-5918.032.ao27.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12918-015-0209-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41437-020-0318-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12859-018-2414-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s40170-020-00212-x.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41562-021-01247-w.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7717_peerj.10452.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1039_c9sc02930c.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_s1677-5538.ibju.2019.0167.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7554_eLife.63194.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s11689-019-9287-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1140_epjds_s13688-018-0132-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1093_sysbio_syy011.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2664.13136.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3133_ofr20201035.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13617-019-0084-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13073-020-00727-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12862-019-1388-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13059-019-1908-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12931-019-1001-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_ncomms11871.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.14379_iodp.proc.390393.208.2024.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s160053681103220x.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41598-024-56373-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1029_2021pa004379.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pcbi.1011828.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1039_c9ra06638a.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12864-019-6324-7.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2656.12501.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1007_s00382-012-1636-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41558-022-01301-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41598-020-59839-x.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_2017jc013030.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ece3.4466.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12866-015-0509-2.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ejoc.202000139.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12870-018-1542-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1806-90882019000500004.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12859-018-2036-2.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7554_elife.63455.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2052252515023945.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1806-93042019000400002.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ece3.9627.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2664.13446.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13068-018-1316-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ece3.6303.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_0001-3765201920180768.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1016_j.ast.2022.107401.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3390_ijms12117360.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1016_j.jobe.2023.107105.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1039_d0sc01197e.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2414314616000523.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1140_epjc_s10052-019-6583-0.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.12688_f1000research.11698.1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13750-020-0184-0.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0253228.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0159387.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_0104-4060.59642.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13007-019-0403-2.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1809-2950_19008627012020.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536812027390.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12859-016-0922-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_hdy.2015.99.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s40657-020-00194-w.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13024-018-0266-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2435.13431.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1617_s11527-023-02260-3.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12860-020-00261-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1145_3461702.3462538.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_eva.12151.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13100-019-0153-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13068-018-1167-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12859-016-1206-3.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536812046892.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1678-4499.20190067.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2656.12491.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1242_dev.138545.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_0102.3772e35417.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1130_ges01387.1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13104-019-4127-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12915-018-0498-3.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.18438_eblip29674.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s40851-018-0089-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_sdata.2017.167.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3389_fchem.2019.00828.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_evo.13972.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_0047-2085000000239.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ecs2.1280.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13568-018-0680-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1021_jacs.2c06519.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.12688_f1000research.13064.1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s40793-015-0095-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.17581_bp.2020.09104.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12866-019-1542-3.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_esp.5090.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_s1678-86212019000400340.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1016_j.dib.2023.109949.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13059-019-1924-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12870-020-2295-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2435.13087.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12920-019-0646-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1098_rsos.160417.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13148-019-0719-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1007_jhep12(2018)117.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_14756366.2020.1740692.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12859-020-3415-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12859-018-2263-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13643-018-0859-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1029_2018gl078007.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_amt-15-3969-2022.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_esp.5058.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ece3.961.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12879-019-3766-0.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1029_2023wr035126.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3897_neobiota.82.87455.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3389_fcimb.2024.1292467.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1039_d0sc01518k.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1519-6984.192126.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s40168-018-0550-0.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ejoc.202000916.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12866-020-01863-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12885-018-4768-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0284951.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_chem.201903120.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_21645515.2023.2189598.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1016_j.jlp.2022.104761.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_acp-2021-570.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12920-019-0611-7.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_ece3.6784.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3762_bjoc.8.42.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.12688_f1000research.4660.1.pdf\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "MuPDF error: unsupported error: cannot create appearance stream for  annotations\n",
      "\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_anie.202007717.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3390_d13010019.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_fst.33717.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1029_2020jf005675.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_0284186x.2020.1714721.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13073-019-0674-2.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12885-020-06724-5.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1007_jhep11(2018)113.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3390_v11060565.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_2041-210x.13817.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_mp.14424.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_chem.202001412.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2664.13168.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3390_s19030479.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_02713683.2019.1607392.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1007_s12263-014-0408-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2052252515011665.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_essd-12-1287-2020.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1093_beheco_arad016.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_15476286.2016.1232238.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12885-018-4314-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_1365-2656.12382.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1140_epjc_s10052-018-6468-7.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1128_spectrum.00422-24.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536810036299.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_eva.12768.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0188323.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1534_g3.119.400993.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1806-9479.2019.185555.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3897_zoologia.36.e32053.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13321-015-0110-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3389_fevo.2023.1112519.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536809014883.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0139215.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_hdy.2013.74.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3847_1538-4357_aae92c.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2414314616007033.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_cas.12935.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3897_zookeys.500.9360.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0262974.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12863-019-0790-4.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_eva.12446.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41598-017-15852-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_0284186x.2019.1669817.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.7554_eLife.72626.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536810047185.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1039_d0gc00363h.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536812024269.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12864-019-6131-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_0104-6632.20190362s20180340.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_essd-2023-187.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41598-021-85671-y.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1073_pnas.1705601114.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2056989020010658.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s2056989015019891.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_mec.16743.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12870-019-1889-5.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1038_s41467-018-07681-1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13068-018-1078-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12864-015-2206-9.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3897_zoologia.35.e23481.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1093_beheco_arw167.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_acp-22-5701-2022.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13058-015-0618-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12881-019-0773-3.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.12688_f1000research.13483.1.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_2236-3459_83528.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.3897_bdj.7.e47369.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13024-018-0254-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0212669.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.5194_essd-2023-198.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_1980-5373-mr-2018-0766.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13073-019-0709-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s13071-018-3237-2.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1007_s10904-014-0054-8.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1111_gcb.13914.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536807066780.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0137181.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12874-018-0583-x.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12943-019-1017-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12967-019-2100-3.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1029_2019pa003774.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1080_07350015.2020.1766469.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12903-018-0656-6.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s12884-018-1751-z.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1371_journal.pone.0070749.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1107_s1600536812014614.pdf\n",
      "Loaded training data for 573 articles.\n",
      "Prepared 573 training examples for the LLM.\n",
      "Training set size: 515 examples\n",
      "Validation set size: 58 examples\n"
     ]
    }
   ],
   "source": [
    "# 4.3. Populate ArticleData with DatasetCitation objects and ground truth\n",
    "testing_data_for_llm = extract_training_data_for_llm(file_paths_df)\n",
    "print(f\"Prepared {len(testing_data_for_llm)} training examples for the LLM.\")\n",
    "\n",
    "# Convert the list of LlmTrainingData to a DataFrame and save it\n",
    "training_data_for_llm_df = pd.DataFrame(testing_data_for_llm)\n",
    "training_data_for_llm_df.to_csv(os.path.join(BASE_OUTPUT_DIR, \"training_data_for_llm.csv\"), index=False)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_list(testing_data_for_llm)\n",
    "train_dataset = train_dataset.shuffle(seed=42) # Shuffle for good measure\n",
    "\n",
    "# Split into train/validation\n",
    "train_test_split = train_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "print(f\"Training set size: {len(train_dataset)} examples\")\n",
    "print(f\"Validation set size: {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac610853f6744f7d9d9984ccb2b1598d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28af41d1153497a82b67798c5cdff47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "53426"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save off the datasets to CSV for later use\n",
    "train_dataset.to_csv(os.path.join(BASE_OUTPUT_DIR, \"train_dataset.csv\"), index=False)\n",
    "eval_dataset.to_csv(os.path.join(BASE_OUTPUT_DIR, \"eval_dataset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066f50c",
   "metadata": {},
   "source": [
    "#### 5. Model Selection and Configuration\n",
    "\n",
    "We'll use a Qwen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12e2982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model C:\\Users\\jim\\.cache\\kagglehub\\models\\qwen-lm\\qwen-3\\transformers\\0.6b\\1 loaded with 4-bit quantization.\n"
     ]
    }
   ],
   "source": [
    "# 5.1. Choose a Model from KaggleHub\n",
    "# Example: Qwen/Qwen1.5-0.5B-Chat (or 1.8B-Chat if 0.5B is too small/performs poorly)\n",
    "# You can find these on KaggleHub or Hugging Face Hub.\n",
    "model_name = QWEN_BASE_MODEL_PATH\n",
    "\n",
    "# 5.2. Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Qwen uses EOS for padding\n",
    "\n",
    "# 5.3. Load Model with Quantization (4-bit)\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Or torch.float16 if bfloat16 is not supported by your GPU\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=nf4_config,\n",
    "    torch_dtype=torch.bfloat16, # Match compute_dtype\n",
    "    device_map=\"auto\", # Automatically maps model to available devices\n",
    "    trust_remote_code=True # Required for some models like Qwen\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training (LoRA compatible)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"Model {model_name} loaded with 4-bit quantization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a83df8",
   "metadata": {},
   "source": [
    "#### 6. Dataset Preparation for Training\n",
    "\n",
    "Format the extracted data into instruction-tuning prompts using the ChatML format, which Qwen models are trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a008679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of formatted training data (string output):\n",
      "<|im_start|>system\n",
      "You are an expert assistant for classifying research data citations. /no_think<|im_end|>\n",
      "<|im_start|>user\n",
      "Given the following article context and a specific data citation, classify if the data was generated as 'Primary' (newly generated for this study), 'Secondary' (reused from existing records), or 'Missing' (no data citation context given).\n",
      "\n",
      "Article DOI: 10.1111/cas.12935\n",
      "Article Abstract: No Abstract\n",
      "Data Citation Context: The difference was more significant in ER-negative BCs (P = 0.0276 for ER-negative vs healthy control; P = 0.2277 for ER-positive vs healthy control). The area under the curve value was 0.604 for all BC patients versus healthy Table 1. Twelve proteins selected as marker candidates for breast cancer Accession no. Protein name Gene name MS  MS spectral count HPA database MCF-7 MDA-MB-231 SK-BR-3 Hs578T HPA Ab Average IHC score Percent of location Normal Breast cancer n  c  m c  m n Q99538 Legumain LGMN 7 24 13 1 HPA001426 1 2 0 100 0 O75787 Renin receptor ATP6AP2 0 7 3 0 HPA003156 1 1.77 0 100 0\n",
      "Dataset ID: Q99538\n",
      "\n",
      "Classification:<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Secondary<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.1. Define the formatting function for ChatML (Corrected for trl 0.19.1)\n",
    "def format_example(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert assistant for classifying research data citations. /no_think\"},\n",
    "        {\"role\": \"user\", \"content\": (\n",
    "            f\"Given the following article context and a specific data citation, classify if the data was generated as 'Primary' (newly generated for this study), 'Secondary' (reused from existing records), or 'Missing' (no data citation context given).\\n\\n\"\n",
    "            f\"Article DOI: {example['article_doi']}\\n\"\n",
    "            f\"Article Abstract: {example['article_abstract']}\\n\" \n",
    "            f\"Data Citation Context: {example['citation_context']}\\n\"\n",
    "            f\"Dataset ID: {example['dataset_id']}\\n\\n\"\n",
    "            f\"Classification:\"\n",
    "        )}\n",
    "    ]\n",
    "    # The target output for the model is just \"Primary\" or \"Secondary\"\n",
    "    messages.append({\"role\": \"assistant\", \"content\": example['label']})\n",
    "    \n",
    "    # Apply chat template and return the string directly\n",
    "    # <--- IMPORTANT CHANGE: Directly return the string, not a dictionary\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False, enable_thinking=False)\n",
    "\n",
    "# Apply the formatting to the dataset\n",
    "# IMPORTANT: When formatting_func returns a string directly, you typically don't\n",
    "# need to call .map() on the dataset beforehand if SFTTrainer handles it internally.\n",
    "# However, if you want to inspect the formatted text, you can still do this:\n",
    "# formatted_train_dataset = train_dataset.map(format_example)\n",
    "# But for SFTTrainer, you pass the original `train_dataset` and the `formatting_func`\n",
    "# and `dataset_text_field` (which will be ignored if formatting_func is used to generate the text).\n",
    "\n",
    "# Print an example to verify (you'll need to call format_example directly for this)\n",
    "print(\"\\nExample of formatted training data (string output):\")\n",
    "# You can't directly print from formatted_train_dataset if you don't map it first.\n",
    "# Let's print by calling the function on a sample:\n",
    "if len(train_dataset) > 0:\n",
    "    sample_formatted_text = format_example(train_dataset[0])\n",
    "    print(sample_formatted_text)\n",
    "else:\n",
    "    print(\"No training data to display example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d8fdd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd84cd99cb3f4e44a088082f80a53738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557aa8acdcd54112b2ccff021bd1a780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65aebf7615e247669ee2a641788f3e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9594033cf4646319561ac45d3c548e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cbe417c70e64a7bb02e42d08dcfb349",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to eval dataset:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c827abe0c04d21b4db2b28b28afa75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9ae5e353534bbeb7d171667c68a41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f7b058fa004b49bacbe84dbd4a916f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/58 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# This version uses the evaluation dataset in the SFTTrainer\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 7.1. Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\", # Adjust based on model architecture if needed\n",
    ")\n",
    "\n",
    "# 7.2. Configure Training Arguments (now using SFTConfig)\n",
    "training_args = SFTConfig(\n",
    "    output_dir=FINE_TUNED_MODEL_OUTPUT_DIR,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[],\n",
    "    \n",
    "    # SFTTrainer-specific parameters moved into SFTConfig\n",
    "    max_seq_length=256,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False},\n",
    "\n",
    "    # --- NEW: Evaluation Parameters ---\n",
    "    eval_strategy=\"steps\", # Evaluate every 'eval_steps'\n",
    "    eval_steps=500,              # How often to run evaluation (e.g., every 500 steps)\n",
    "                                 # You can also use \"epoch\" for evaluation_strategy\n",
    "    save_strategy=\"steps\",       # How often to save checkpoints\n",
    "    save_total_limit=1,          # Only keep the best model checkpoint\n",
    "    load_best_model_at_end=True, # Load the model with the best validation metric at the end of training\n",
    "    metric_for_best_model=\"eval_loss\", # Metric to monitor for best model (default for CLM)\n",
    "    greater_is_better=False,     # For loss, lower is better\n",
    ")\n",
    "\n",
    "# 7.3. Initialize SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, # <--- Pass the evaluation dataset here\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    "    formatting_func=format_example\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f90683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612c42297d1e41398bf21a46892eb71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc41c736f5a42d08bd9f7c003ac7441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8068b3693b584593b7012fe602578e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00bac153dd446b9a53dbb795a245c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# This version works but does NOT use the evaluation dataset\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# # 7.1. Configure LoRA\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     r=64,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=\"all-linear\", # Adjust based on model architecture if needed\n",
    "# )\n",
    "\n",
    "# # 7.2. Configure Training Arguments (now using SFTConfig)\n",
    "# # SFTConfig combines TrainingArguments with SFTTrainer-specific parameters\n",
    "# training_args = SFTConfig( # <--- IMPORTANT CHANGE: Use SFTConfig instead of TrainingArguments\n",
    "#     output_dir=FINE_TUNED_MODEL_OUTPUT_DIR,\n",
    "#     per_device_train_batch_size=1, # Adjust based on your GPU memory\n",
    "#     gradient_accumulation_steps=16,\n",
    "#     learning_rate=2e-4,\n",
    "#     num_train_epochs=3,\n",
    "#     logging_steps=10,\n",
    "#     save_steps=500,\n",
    "#     optim=\"paged_adamw_8bit\",\n",
    "#     fp16=True,  # <--- CHANGED: Try fp16 for broader compatibility and memory\n",
    "#     bf16=False, # <--- CHANGED: Disable bf16 if fp16 is used\n",
    "#     max_grad_norm=0.3,\n",
    "#     warmup_ratio=0.03,\n",
    "#     lr_scheduler_type=\"constant\",\n",
    "#     report_to=\"none\",\n",
    "#     disable_tqdm=False,\n",
    "#     remove_unused_columns=False, # Keep columns for formatting\n",
    "#     label_names=[], # Explicitly tell Trainer not to look for label columns in the dataset\n",
    "#     # Additional SFT-specific parameters    \n",
    "#     max_seq_length=512, # Max input sequence length (adjust based on context size)\n",
    "#     packing=False, # Set to True for more efficient training if your data is short\n",
    "#     dataset_text_field=\"text\", # The name of the column in your dataset containing the text\n",
    "#     # <--- NEW: Enable gradient checkpointing\n",
    "#     gradient_checkpointing=True,\n",
    "#     # This line is important for gradient checkpointing with PeftModel\n",
    "#     # It tells the model to use the Peft (LoRA) layers for checkpointing\n",
    "#     gradient_checkpointing_kwargs={'use_reentrant':False} # Recommended for newer PyTorch/Accelerate\n",
    "# )\n",
    "\n",
    "# # 7.3. Initialize SFTTrainer (Corrected for trl 0.19.1)\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     processing_class=tokenizer, \n",
    "#     train_dataset=train_dataset,\n",
    "#     peft_config=peft_config,\n",
    "#     args=training_args, # This is now an SFTConfig object\n",
    "#     formatting_func=format_example # This remains a direct argument to SFTTrainer\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45982673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='99' max='99' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [99/99 16:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Fine-tuned model saved to ./kaggle/working\\results\\final_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7.4. Start Training\n",
    "print(\"\\nStarting model training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the fine-tuned model (LoRA adapters)\n",
    "trainer.save_model(os.path.join(FINE_TUNED_MODEL_OUTPUT_DIR, \"final_model\"))\n",
    "print(f\"Fine-tuned model saved to {os.path.join(FINE_TUNED_MODEL_OUTPUT_DIR, 'final_model')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dba261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initiating GPU memory cleanup...\n",
      "GPU memory cleanup complete. Please check nvidia-smi to confirm.\n"
     ]
    }
   ],
   "source": [
    "# --- Explicit GPU Memory Cleanup ---\n",
    "print(\"\\nInitiating GPU memory cleanup...\")\n",
    "\n",
    "# 1. Explicitly delete large objects that consume GPU memory\n",
    "#    This removes references, allowing Python's garbage collector to act.\n",
    "if 'trainer' in locals() and trainer is not None:\n",
    "    del trainer\n",
    "if 'model' in locals() and model is not None:\n",
    "    del model\n",
    "if 'tokenizer' in locals() and tokenizer is not None:\n",
    "    del tokenizer\n",
    "# If you had other large tensors or datasets explicitly moved to GPU,\n",
    "# you would delete them here too. For Hugging Face datasets, they are usually\n",
    "# on CPU unless you manually call .to('cuda').\n",
    "\n",
    "# 2. Force Python's garbage collection\n",
    "#    This helps ensure that deleted objects are immediately cleaned up.\n",
    "gc.collect()\n",
    "\n",
    "# 3. Clear PyTorch's CUDA memory cache\n",
    "#    This tells PyTorch to release any cached memory back to the OS/driver.\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"GPU memory cleanup complete. Please check nvidia-smi to confirm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e3154",
   "metadata": {},
   "source": [
    "#### 8. Inference and Evaluation\n",
    "\n",
    "After training, load the best model (or the final one) and apply it to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "583fed9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded for inference.\n"
     ]
    }
   ],
   "source": [
    "# 8.1. Load the Trained Model (or merge LoRA adapters for full model)\n",
    "# If you saved LoRA adapters, you'll need to load the base model and then the adapters.\n",
    "# For inference, it's often easier to merge them.\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=nf4_config, # Use the same config as training\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# model = PeftModel.from_pretrained(model, os.path.join(FINE_TUNED_MODEL_OUTPUT_DIR, \"final_model\"))\n",
    "# model = model.merge_and_unload() # Merge LoRA adapters into the base model\n",
    "\n",
    "# For simplicity, if you just want to test the last saved checkpoint:\n",
    "# You can also load the model directly from the checkpoint if it's a full save\n",
    "# model = AutoModelForCausalLM.from_pretrained(os.path.join(FINE_TUNED_MODEL_OUTPUT_DIR, \"final_model\"), device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(os.path.join(FINE_TUNED_MODEL_OUTPUT_DIR, \"final_model\"))\n",
    "\n",
    "# If you want to load the base model and then the adapters for inference:\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=nf4_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, os.path.join(FINE_TUNED_MODEL_OUTPUT_DIR, \"final_model\"))\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "print(\"Model loaded for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2bc475f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files paths shape: (30, 4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "fb1e0aa2-bc3e-4e5c-a6b9-93731059b528",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_2017jc013030.xml",
         "test"
        ],
        [
         "5",
         "10.1002_chem.201903120",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.201903120.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.201903120.xml",
         "test"
        ],
        [
         "20",
         "10.1002_ecs2.1280",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.1280.pdf",
         "",
         "test"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1002_chem.201903120</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.1002_ecs2.1280</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                article_id                                      pdf_file_path  \\\n",
       "0     10.1002_2017jc013030  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "5   10.1002_chem.201903120  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "20       10.1002_ecs2.1280  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                        xml_file_path dataset_type  \n",
       "0   ./kaggle/input/make-data-count-finding-data-re...         test  \n",
       "5   ./kaggle/input/make-data-count-finding-data-re...         test  \n",
       "20                                                            test  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For testing, always set to the TEST_DATA_DIR\n",
    "base_file_dir = TEST_DATA_DIR\n",
    "\n",
    "# Load file paths for base directory\n",
    "test_file_paths_df = load_file_paths(base_file_dir)\n",
    "test_file_paths_df['xml_file_path'] = test_file_paths_df['xml_file_path'].fillna('')\n",
    "\n",
    "print(f\"Files paths shape: {test_file_paths_df.shape}\")\n",
    "display(test_file_paths_df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b06b8228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_potential_dataset_ids(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Finds potential dataset IDs in the given text using predefined regex patterns.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to search for dataset IDs.\n",
    "        \n",
    "    Returns:\n",
    "        Set[str]: A set of unique dataset IDs found in the text.\n",
    "    \"\"\"\n",
    "    dataset_ids = set()\n",
    "    for regex in COMPILED_DATASET_ID_REGEXES:\n",
    "        for match in re.finditer(regex, text):\n",
    "            dataset_id = match.group(0)\n",
    "            dataset_ids.add(dataset_id)\n",
    "    return list(dataset_ids)\n",
    "\n",
    "def invoke_model_for_inference(tokenizer, article_data: ArticleData) -> list[SubmissionData]:\n",
    "    submission_data_list = []\n",
    "    article_id = article_data.article_id\n",
    "    dataset_citations = article_data.dataset_citations\n",
    "    if not dataset_citations:\n",
    "        submission_data_list.append(SubmissionData(article_id, dataset_id=\"Missing\", type=\"Missing\"))\n",
    "        return submission_data_list\n",
    "\n",
    "    print(f\"Found {len(dataset_citations)} citations for {article_id}\")\n",
    "    for dc in dataset_citations:\n",
    "        # Create the prompt for inference\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert assistant for classifying research data citations. /no_think\"},\n",
    "            {\"role\": \"user\", \"content\": (\n",
    "                f\"Given the following article context and a specific data citation, classify if the data was generated as 'Primary' (newly generated for this study) or 'Secondary' (reused from existing records).\\n\\n\"\n",
    "                f\"Article Title: {article_data.title}\\n\"\n",
    "                f\"Article Abstract: {article_data.abstract}\\n\"\n",
    "                f\"Data Citation Context: {dc.citation_context}\\n\"\n",
    "                f\"Dataset ID: {dc.dataset_id}\\n\\n\"\n",
    "                f\"Classification:\"\n",
    "            )}\n",
    "        ]\n",
    "\n",
    "        # --- CHANGE STARTS HERE ---\n",
    "        # Tokenize and get both input_ids and attention_mask\n",
    "        input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs, # <--- Pass the entire dictionary (includes input_ids and attention_mask)\n",
    "                max_new_tokens=10, # Expecting \"Primary\" or \"Secondary\"\n",
    "                do_sample=False, # Use greedy decoding as per your preference\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        # --- CHANGE ENDS HERE ---        \n",
    "\n",
    "        generated_text = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip() # Use inputs['input_ids']\n",
    "        # print(f\"LLM Resp: {generated_text}\")        \n",
    "        \n",
    "        # Post-process the generated text to get the classification\n",
    "        predicted_type = \"Missing\"\n",
    "        if \"Primary\" in generated_text:\n",
    "            predicted_type = \"Primary\"\n",
    "        elif \"Secondary\" in generated_text:\n",
    "            predicted_type = \"Secondary\"\n",
    "        \n",
    "        submission_data_list.append(SubmissionData(article_id, dataset_id=dc.dataset_id, type=predicted_type, context=dc.citation_context))\n",
    "\n",
    "    return submission_data_list\n",
    "\n",
    "def extract_article_data_for_inference(article_id: str, filepath: str) -> ArticleData:\n",
    "    full_text = extract_text_from_file(filepath)\n",
    "    article_data = extract_article_data_from_text(full_text, article_id)\n",
    "    potential_dataset_ids = find_potential_dataset_ids(full_text)\n",
    "    if potential_dataset_ids:\n",
    "        doc = NLP_SPACY(full_text)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "        # Populate article_data with potentially valid dataset_citations\n",
    "        # the set_citation_context and add_dataset_citation methods do all of the appropriate filtering\n",
    "        for dataset_id in potential_dataset_ids:\n",
    "            citation = DatasetCitation(dataset_id=dataset_id)\n",
    "            context = extract_context_around_id(sentences, dataset_id)\n",
    "            citation.set_citation_context(context, check_data_related=True)\n",
    "            article_data.add_dataset_citation(citation)\n",
    "\n",
    "    return article_data\n",
    "\n",
    "def process_test_articles(tokenizer, file_paths_df: pd.DataFrame) -> list[SubmissionData]:\n",
    "    \"\"\"\n",
    "    Extracts article data for testing set without ground truth.\n",
    "    \n",
    "    Args:\n",
    "        file_paths_df (pd.DataFrame): DataFrame containing file paths and ground truth info.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, ArticleData]: Dictionary mapping article IDs to ArticleData objects.\n",
    "    \"\"\"\n",
    "    submission_data_list = []\n",
    "    for i, row in tqdm(file_paths_df.iterrows(), total=len(file_paths_df)):\n",
    "        article_id = row['article_id']\n",
    "        filepath = row['pdf_file_path'] if row['pdf_file_path'] else row['xml_file_path']\n",
    "        \n",
    "        # Extract article data\n",
    "        article_data = extract_article_data_for_inference(article_id, filepath)\n",
    "\n",
    "        # Invoke the model with the collected article_data\n",
    "        submission_data_list.extend(invoke_model_for_inference(tokenizer, article_data))\n",
    "\n",
    "    print(f\"Processed testing data for {len(submission_data_list)} article and dataset_id combos.\")\n",
    "    return submission_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee07a9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GSE45042',\n",
       " '10.1234/dryad.as2345',\n",
       " 'GSE28166',\n",
       " 'GSE37569',\n",
       " '10.25386/genetics.11365982',\n",
       " 'pdb 5yfp']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"adff 10.1234/dryad.as2345 accession numbers GSE1 GSE37569, GSE45042 , GSE28166  (pdb 5yfp) 10.25386/genetics.11365982 \"\n",
    "rsp1 = find_potential_dataset_ids(text)\n",
    "display(rsp1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "903dc0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "72e7477a-72f8-4705-bff1-5d022c8efc3d",
       "rows": [
        [
         "27",
         "10.1002_mp.14424",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_mp.14424.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_mp.14424.xml",
         "test"
        ],
        [
         "15",
         "10.1002_ece3.6144",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6144.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.6144.xml",
         "test"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10.1002_mp.14424</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.1002_ece3.6144</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           article_id                                      pdf_file_path  \\\n",
       "27   10.1002_mp.14424  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "15  10.1002_ece3.6144  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                        xml_file_path dataset_type  \n",
       "27  ./kaggle/input/make-data-count-finding-data-re...         test  \n",
       "15  ./kaggle/input/make-data-count-finding-data-re...         test  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_file_paths_df = test_file_paths_df.sample(2, random_state=42)\n",
    "sample_test_file_paths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "efde80b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ac79c01dcf449baafe0321d196e294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 citations for 10.1002_2017jc013030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.201916483.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202005531.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202007717.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.201902131.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.201903120.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202000235.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202001412.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202001668.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202003167.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_cssc.202201821.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 citations for 10.1002_cssc.202201821\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.3985.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.4466.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 citations for 10.1002_ece3.4466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.5260.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 citations for 10.1002_ece3.5260\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.5395.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 citations for 10.1002_ece3.5395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6144.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 citations for 10.1002_ece3.6144\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6303.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 citations for 10.1002_ece3.6303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6784.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.961.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.9627.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.1280.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 citations for 10.1002_ecs2.1280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.4619.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 citations for 10.1002_ecs2.4619\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejic.201900904.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejoc.202000139.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejoc.202000916.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_esp.5058.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 citations for 10.1002_esp.5058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_esp.5090.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 citations for 10.1002_esp.5090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_mp.14424.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 citations for 10.1002_mp.14424\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_nafm.10870.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 citations for 10.1002_nafm.10870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1007_jhep07(2018)134.pdf\n",
      "Processed testing data for 46 article and dataset_id combos.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SubmissionData(article_id='10.1002_2017jc013030', dataset_id='10.17882/49388', type='Primary', context='Xing, X. (2017). A global database of vertical profiles derived from Biogeochemical Argo float measurements for biogeochemical and bio-optical applications. SEANOE. https://doi.org/10.17882/49388'),\n",
       " SubmissionData(article_id='10.1002_2017jc013030', dataset_id='10.17882/47142', type='Secondary', context='Approches Numeriques); Pierre-Marie Poulain (National Institute of Oceanography and Experimental Geophysics, Italy; Argo-Italy); Sabrina Speich (Laboratoire de Meteorologie Dynamique, France; LEFE-GMMC); Virginie Thierry (Ifremer, France; LEFE-GMMC); Pascal Conan (Observatoire Oceanologique de Banyuls sur mer, France; LEFE-GMMC); Laurent Coppola (Laboratoire dOceanographie de Villefranche, France; LEFE-GMMC); Anne Petrenko (Mediterranean Institute of Oceanography, France; LEFE-GMMC); and Jean-Baptiste Sallee (Laboratoire dOceanographie et du Climat, France; LEFE-GMMC). Collin Roesler (Bowdoin College, USA) and Yannick Huot (University of Sherbrooke, Canada) are acknowledged for useful comments and fruitful discussion. We also thank the International Argo Program and the CORIOLIS project that contribute to make the data freely and publicly available. Data referring to Organelli et al. (2016a; https://doi.org/10.17882/47142) and Barbieux et al. (2017; https://doi.org/10.17882/49388) are freely available on SEANOE.'),\n",
       " SubmissionData(article_id='10.1002_2017jc013030', dataset_id='10.5194/os-11-759-2015', type='Primary', context='https://doi.org/10.1016/j.mio.2013.12.003 Sammartino, M., Di Cicco, A., Marullo, S., & Santoleri, R. (2015). Spatio-temporal variability of micro-, nano- and pico-phytoplankton in the Mediterranean Sea from satellite ocean colour data of SeaWiFS. Ocean Science, 11(5), 759-778. https://doi.org/10.5194/os-11-759-2015 Sathyendranath, S., Stuart, V., Nair, A., Oka, K., Nakane, T., Bouman, H., . . .'),\n",
       " SubmissionData(article_id='10.1002_anie.201916483', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_anie.202005531', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_anie.202007717', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_chem.201902131', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_chem.201903120', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_chem.202000235', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_chem.202001412', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_chem.202001668', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_chem.202003167', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_cssc.202201821', dataset_id='10.5281/zenodo.7074790.', type='Primary', context='Open Access funding enabled and organized by Projekt DEAL. Conflict of Interest The authors declare no conflict of interest. Data Availability Statement The data that support the findings of this study are openly available in zenodo at https://doi.org/10.5281/zenodo.7074790.'),\n",
       " SubmissionData(article_id='10.1002_ece3.3985', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_ece3.4466', dataset_id='10.1007/s00265-005-0106-8', type='Secondary', context='DATA ACCESSIBILITY The dataset supporting this article are available in Dryad https://doi.org/10.5061/dryad.r6nq870. ORCID Alexander Hansson http://orcid.org/0000-0002-7179-9361 Mats Olsson http://orcid.org/0000-0002-4130-1323 REFERENCES Aragn, P., Lpez, P., & Martn, J. (2006). Roles of male residence and relative size in the social behavior of Iberian rock lizards, Lacerta mon-ticola. Behavioral Ecology and Sociobiology, 59, 762-769. https://doi.org/10.1007/s00265-005-0106-8'),\n",
       " SubmissionData(article_id='10.1002_ece3.4466', dataset_id='10.5061/dryad.r6nq870.', type='Secondary', context='AUTHOR CONTRIBUTIONS AH and MO conceived the ideas and designed methodology; AH col-lected the data; AH and MO analyzed the data; AH led the writing 20457758, 2018, 19, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ece3.4466, Wiley Online Library on 31/03/2025. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 9832 | HANSSON and OLSSON of the manuscript. All authors contributed critically to the drafts and gave final approval for publication. DATA ACCESSIBILITY The dataset supporting this article are available in Dryad https://doi.org/10.5061/dryad.r6nq870. ORCID'),\n",
       " SubmissionData(article_id='10.1002_ece3.5260', dataset_id='10.5061/dryad.2f62927', type='Secondary', context='J.S.S., J.M.P., and D.R.M. ob- tained and provided the Illumina and Sanger sequence data, assem- bled the base genomes, and produced genomic assembly metrics. All authors contributed to writing of the manuscript. DATA AVAILABILITY DNA sequences: GenBank MK838494 to MK838511; Phyluce BED/BAM files for probe design experiments and Adephaga_2.9Kv1 final probe set fasta file: Dryad https://doi.org/10.5061/dryad.2f62927; Custom scripts: Alexander 2018a: Comparing_monolithic_UCE_fas- tas available from; https://github.com/laninsky/comparing_monol ithic_UCE_fastas; Alexander 2018b: Phase_target available from https://github.com/laninsky/reference_aligning_to_established_loci/tree/master/phase_target;'),\n",
       " SubmissionData(article_id='10.1002_ece3.5395', dataset_id='10.5441/001/1.c42j3js7', type='Primary', context=\"AUTHORS' CONTRIBUTION All authors contributed to study design, conception and gave ap- proval for publication. W.F. and A.F collected data. Y.C and A.F con- ducted the analysis and wrote the manuscript in collaboration with all. DATA ACCESSIBILITY The data used for this study are available through the Movebank Data Repository (https://www.movebank.org): with https://doi.org/10.5441 /001/1.v1cs4nn0, https://doi.org/10.5441/001/1.c42j3js7, https://doi.org/10.5441/001/1.4192t2j4, https://doi.org/10.5441/001/1.\"),\n",
       " SubmissionData(article_id='10.1002_ece3.5395', dataset_id='10.5441/001/1.71r7pp6q', type='Primary', context='W.F. and A.F collected data. Y.C and A.F con- ducted the analysis and wrote the manuscript in collaboration with all. DATA ACCESSIBILITY The data used for this study are available through the Movebank Data Repository (https://www.movebank.org): with https://doi.org/10.5441 /001/1.v1cs4nn0, https://doi.org/10.5441/001/1.c42j3js7, https://doi.org/10.5441/001/1.4192t2j4, https://doi.org/10.5441/001/1. ck04mn78, https://doi.org/10.5441/001/1.71r7pp6q (Fiedler, Flack, Schfle, et al., 2019; Fiedler, Flack, Schmid, Reinhard, & Wikelski, 2019; Fiedler, Hilsendegen, et al., 2019; Fiedler, Leppelsack, et al., 2019; Fiedler, Niederer, Schnenberger, Flack, & Wikelski, 2019).'),\n",
       " SubmissionData(article_id='10.1002_ece3.5395', dataset_id='10.5441/001/1.4192t2j4', type='Primary', context=\"AUTHORS' CONTRIBUTION All authors contributed to study design, conception and gave ap- proval for publication. W.F. and A.F collected data. Y.C and A.F con- ducted the analysis and wrote the manuscript in collaboration with all. DATA ACCESSIBILITY The data used for this study are available through the Movebank Data Repository (https://www.movebank.org): with https://doi.org/10.5441 /001/1.v1cs4nn0, https://doi.org/10.5441/001/1.c42j3js7, https://doi.org/10.5441/001/1.4192t2j4, https://doi.org/10.5441/001/1.\"),\n",
       " SubmissionData(article_id='10.1002_ece3.5395', dataset_id='10.5441/001/1.', type='Primary', context=\"AUTHORS' CONTRIBUTION All authors contributed to study design, conception and gave ap- proval for publication. W.F. and A.F collected data. Y.C and A.F con- ducted the analysis and wrote the manuscript in collaboration with all. DATA ACCESSIBILITY The data used for this study are available through the Movebank Data Repository (https://www.movebank.org): with https://doi.org/10.5441 /001/1.v1cs4nn0, https://doi.org/10.5441/001/1.c42j3js7, https://doi.org/10.5441/001/1.4192t2j4, https://doi.org/10.5441/001/1.\"),\n",
       " SubmissionData(article_id='10.1002_ece3.6144', dataset_id='10.5061/dryad.zw3r22854.', type='Primary', context='CONFLICT OF INTEREST None declared. AUTHOR CONTRIBUTIONS Elena Duke performed research, analyzed data, and wrote the paper. Ron Burton designed research, wrote the paper, and is the principal investigator. DATA AVAILABILITY STATEMENT Data are available at Dryad Digital Repository at: https://doi.org/10.5061/dryad.zw3r22854.'),\n",
       " SubmissionData(article_id='10.1002_ece3.6303', dataset_id='10.5061/dryad.37pvmcvgb.', type='Secondary', context='Feihai Yu: Conceptualization (supporting); Writing-review & editing (supporting). Junmin Li: Conceptualization (lead); Funding acquisition (lead); Investigation (lead); Methodology (lead); Project administration (lead); Validation (lead); Writing-original draft (lead); Writing-review & editing (lead). DATA AVAILABILITY STATEMENT DNA sequences have been deposited in the National Center for Biotechnology Information (NCBI) database (accession numbers: MN107084-MN107151, MN335287-MN335290). The data that support the findings of this study have been deposited in Dryad with doi:https://doi.org/10.5061/dryad.37pvmcvgb.'),\n",
       " SubmissionData(article_id='10.1002_ece3.6303', dataset_id='MN107151', type='Primary', context='The congener was collected from Kuocangshan Mountain in Linhai City, Zhejiang Province, China, and identified by Prof. Ming Jiang in Taizhou University, China. All phylogenetic analyses were performed with MEGA v 10.0.5 (Kumar et al., 2018). Sequences of the 34 haplotypes and the two outgroups have been 20457758, 2020, 12, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ece3.6303, Wiley Online Library on 31/03/2025. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 5622 | WAN et al. submitted to the National Center for Biotechnology Information (NCBI) database (accession numbers: MN107084-MN107151, MN335287-MN335290).'),\n",
       " SubmissionData(article_id='10.1002_ece3.6303', dataset_id='MN107084', type='Primary', context='The congener was collected from Kuocangshan Mountain in Linhai City, Zhejiang Province, China, and identified by Prof. Ming Jiang in Taizhou University, China. All phylogenetic analyses were performed with MEGA v 10.0.5 (Kumar et al., 2018). Sequences of the 34 haplotypes and the two outgroups have been 20457758, 2020, 12, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ece3.6303, Wiley Online Library on 31/03/2025. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 5622 | WAN et al. submitted to the National Center for Biotechnology Information (NCBI) database (accession numbers: MN107084-MN107151, MN335287-MN335290).'),\n",
       " SubmissionData(article_id='10.1002_ece3.6303', dataset_id='MN335290', type='Secondary', context='The congener was collected from Kuocangshan Mountain in Linhai City, Zhejiang Province, China, and identified by Prof. Ming Jiang in Taizhou University, China. All phylogenetic analyses were performed with MEGA v 10.0.5 (Kumar et al., 2018). Sequences of the 34 haplotypes and the two outgroups have been 20457758, 2020, 12, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ece3.6303, Wiley Online Library on 31/03/2025. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 5622 | WAN et al. submitted to the National Center for Biotechnology Information (NCBI) database (accession numbers: MN107084-MN107151, MN335287-MN335290).'),\n",
       " SubmissionData(article_id='10.1002_ece3.6303', dataset_id='MN335287', type='Secondary', context='The congener was collected from Kuocangshan Mountain in Linhai City, Zhejiang Province, China, and identified by Prof. Ming Jiang in Taizhou University, China. All phylogenetic analyses were performed with MEGA v 10.0.5 (Kumar et al., 2018). Sequences of the 34 haplotypes and the two outgroups have been 20457758, 2020, 12, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ece3.6303, Wiley Online Library on 31/03/2025. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 5622 | WAN et al. submitted to the National Center for Biotechnology Information (NCBI) database (accession numbers: MN107084-MN107151, MN335287-MN335290).'),\n",
       " SubmissionData(article_id='10.1002_ece3.6784', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_ece3.961', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_ece3.9627', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_ecs2.1280', dataset_id='10.1002/ecs2.1280/supinfo', type='Secondary', context='Williams, J. L., H. Auge, and J. L. Maron. 2010. Testing hypotheses for exotic plant success: parallel exper-iments in the native and introduced ranges. Ecolo-gy 91:1355-1366. Supporting Information Additional Supporting Information may be found online at: http://onlinelibrary.wiley.com/doi/10.1002/ecs2.1280/supinfo Data Availability Data associated with this paper have been deposited in Dryad: http://dx.doi.org/10.5061/dryad.p3fg9 21508925, 2016, 5, Downloaded from https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecs2.1280 by California Digital Library University Of California, Wiley Online Library on 31/03/2025.'),\n",
       " SubmissionData(article_id='10.1002_ecs2.1280', dataset_id='10.5061/dryad.p3fg9', type='Secondary', context='Williams, J. L., H. Auge, and J. L. Maron. 2010. Testing hypotheses for exotic plant success: parallel exper-iments in the native and introduced ranges. Ecolo-gy 91:1355-1366. Supporting Information Additional Supporting Information may be found online at: http://onlinelibrary.wiley.com/doi/10.1002/ecs2.1280/supinfo Data Availability Data associated with this paper have been deposited in Dryad: http://dx.doi.org/10.5061/dryad.p3fg9 21508925, 2016, 5, Downloaded from https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecs2.1280 by California Digital Library University Of California, Wiley Online Library on 31/03/2025.'),\n",
       " SubmissionData(article_id='10.1002_ecs2.4619', dataset_id='10.25349/D9QW5X.', type='Primary', context='The manuscript was greatly improved thanks to the attention and edits of John Melack, Tom Smith, and Jennifer King. CONFLICT OF INTEREST STATEMENT The authors declare no conflicts of interest. DATA AVAILABILITY STATEMENT Data and novel code (Owens et al., 2023) are available from Dryad: https://doi.org/10.25349/D9QW5X. ORCID Caroline H. Owens https://orcid.org/0000-0002-1675-5702 Michelle J. Lee'),\n",
       " SubmissionData(article_id='10.1002_ejic.201900904', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_ejoc.202000139', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_ejoc.202000916', dataset_id='Missing', type='Missing', context=''),\n",
       " SubmissionData(article_id='10.1002_esp.5058', dataset_id='10.5066/P9FW6E8K', type='Primary', context='CONFLICT OF INTEREST There is no conflict of interest in this paper. DATA AVAILABILITY STATEMENT The dataset of the bathymetric survey and structure from motion of the Ichilo River is openly available USGS data release at http://doi.org/10.5066/P9FW6E8K and Dryad https://doi.org/10.5061/dryad.jh9w0vt9t, respectively.'),\n",
       " SubmissionData(article_id='10.1002_esp.5058', dataset_id='10.5061/dryad.jh9w0vt9t', type='Primary', context='CONFLICT OF INTEREST There is no conflict of interest in this paper. DATA AVAILABILITY STATEMENT The dataset of the bathymetric survey and structure from motion of the Ichilo River is openly available USGS data release at http://doi.org/10.5066/P9FW6E8K and Dryad https://doi.org/10.5061/dryad.jh9w0vt9t, respectively.'),\n",
       " SubmissionData(article_id='10.1002_esp.5090', dataset_id='10.1130/0016-7606', type='Secondary', context='Bierman, P.R. & Caffee, M.W. (2002) Cosmogenic exposure and erosion history of Australian bedrock landforms. Bulletin of the Geological Society of America, 114(7), 787-803. https://doi.org/10.1130/0016-7606(2002)114<0787:CEAEHO>2.0.CO;2 Braun, J., Burbidge, D.R., Gesto, F.N., Sandiford, M., Gleadow, A.J.W., Kohn, B.P. & Cummins, P.R. (2009) Constraints on the current rate of deformation and surface uplift of the Australian continent from a new seismic database and low-T thermochronological data.'),\n",
       " SubmissionData(article_id='10.1002_esp.5090', dataset_id='10.1080/08120090802546977', type='Secondary', context='Bulletin of the Geological Society of America, 114(7), 787-803. https://doi.org/10.1130/0016-7606(2002)114<0787:CEAEHO>2.0.CO;2 Braun, J., Burbidge, D.R., Gesto, F.N., Sandiford, M., Gleadow, A.J.W., Kohn, B.P. & Cummins, P.R. (2009) Constraints on the current rate of deformation and surface uplift of the Australian continent from a new seismic database and low-T thermochronological data. Australian Journal of Earth Sciences, 56(2), 99-110. https://doi.org/10.1080/08120090802546977'),\n",
       " SubmissionData(article_id='10.1002_mp.14424', dataset_id='10.7937/K9/TCIA.2015.PF0M9REI', type='Primary', context='Data from NSCLC-Radio-mics Dataset. In: The Cancer Imaging Archive; 2019. https://doi.org/10.7937/K9/TCIA.2015.PF0M9REI 23.'),\n",
       " SubmissionData(article_id='10.1002_nafm.10870', dataset_id='10.5066/P94BH3W0.', type='Primary', context='2018. Developmental stages of Grass Carp (Ctenopharyngodon idella) eggs in the Sandusky River (version 3.0, July 2020). U.S. Geological Survey data release. Available: https://doi.org/10.5066/P94BH3W0.'),\n",
       " SubmissionData(article_id='10.1002_nafm.10870', dataset_id='10.5066/P90A4UGJ.', type='Primary', context='Long, J. M., and P. M. Kocovsky. 2022. Daily ages of young-\\xadof-\\xadyear Silver Chub from western Lake Erie, 2017-\\xad2018: U.S. Geological Survey data release. Available: https://doi.org/10.5066/P90A4UGJ.'),\n",
       " SubmissionData(article_id='10.1002_nafm.10870', dataset_id='10.5066/P9GTUMAY.', type='Primary', context='Water chemistry of Great Lakes tributaries, 2017-\\xad2018. U.S. Geological Survey data release. Available: https://doi.org/10.5066/P9GTUMAY.'),\n",
       " SubmissionData(article_id='10.1002_nafm.10870', dataset_id='10.5066/F75M63X0.', type='Primary', context='Great Lakes Science Center Great Lakes research vessel operations 1958-\\xad2018. (version 3.0, April 2019). U.S. Geological Survey data release. Available: https://doi.org/10.5066/F75M63X0.'),\n",
       " SubmissionData(article_id='10.1007_jhep07(2018)134', dataset_id='Missing', type='Missing', context='')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_sub = process_test_articles(tokenizer, test_file_paths_df)\n",
    "display(sample_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbddd2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.2. Preprocess Test Data (similar to training data)\n",
    "test_articles_data: Dict[str, ArticleData] = {}\n",
    "# Assuming test data structure is similar to train data (full text files)\n",
    "for article_file in os.listdir(TEST_DATA_DIR):\n",
    "    article_id = os.path.splitext(article_file)[0]\n",
    "    filepath = os.path.join(TEST_DATA_DIR, article_file)\n",
    "    \n",
    "    full_text = extract_text_from_file(filepath)\n",
    "    \n",
    "    # Extract title, author, abstract (same as training)\n",
    "    title_match = re.search(r\"Title:\\s*(.*)\", full_text, re.IGNORECASE)\n",
    "    title = title_match.group(1).strip() if title_match else \"Unknown Title\"\n",
    "    author_match = re.search(r\"Author(?:s)?:\\s*(.*)\", full_text, re.IGNORECASE)\n",
    "    author = author_match.group(1).strip() if author_match else \"Unknown Author\"\n",
    "    abstract_match = re.search(r\"Abstract\\s*(.*?)(?=\\n\\n|\\Z)\", full_text, re.IGNORECASE | re.DOTALL)\n",
    "    abstract = abstract_match.group(1).strip() if abstract_match else \"No Abstract\"\n",
    "\n",
    "    article_data = ArticleData(\n",
    "        article_id=article_id,\n",
    "        title=title,\n",
    "        author=author,\n",
    "        abstract=abstract\n",
    "    )\n",
    "    # For test data, we need to find *all* potential dataset IDs, not just ground truth\n",
    "    # This is the \"finding datasets\" part of your goal.\n",
    "    \n",
    "    # Use regex to find all potential dataset IDs in the full text\n",
    "    found_dataset_mentions = []\n",
    "    for pattern in ALL_ID_PATTERNS:\n",
    "        for match in re.finditer(pattern, full_text, re.IGNORECASE):\n",
    "            dataset_id = match.group(1) if pattern == DOI_PATTERN else match.group(0)\n",
    "            span_text = match.group(0) # The full matched text\n",
    "            \n",
    "            context = extract_context_around_id(full_text, span_text, window_size_sentences=3)\n",
    "            \n",
    "            if context:\n",
    "                dc = DatasetCitation()\n",
    "                dc.add_dataset_id(dataset_id)\n",
    "                dc.set_citation_context(context)\n",
    "                found_dataset_mentions.append(dc)\n",
    "                \n",
    "    article_data.dataset_citations = found_dataset_mentions # Assign found mentions\n",
    "    test_articles_data[article_id] = article_data\n",
    "\n",
    "print(f\"Prepared {len(test_articles_data)} test articles for inference.\")\n",
    "\n",
    "# 8.3. Generate Predictions\n",
    "predictions = []\n",
    "true_labels = [] # Only if you have a test_labels.json for evaluation\n",
    "\n",
    "for article_id, article_data in test_articles_data.items():\n",
    "    for dc in article_data.dataset_citations:\n",
    "        # Create the prompt for inference\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert assistant for classifying research data citations.\"},\n",
    "            {\"role\": \"user\", \"content\": (\n",
    "                f\"Given the following article context and a specific data citation, classify if the data was generated as 'Primary' (newly generated for this study) or 'Secondary' (reused from existing records).\\n\\n\"\n",
    "                f\"Article Title: {article_data.title}\\n\"\n",
    "                f\"Article Abstract: {article_data.abstract}\\n\"\n",
    "                f\"Data Citation Context: {dc.citation_context}\\n\"\n",
    "                f\"Dataset ID: {list(dc.dataset_ids)[0]}\\n\\n\" # Assuming one ID per citation\n",
    "                f\"Classification:\"\n",
    "            )}\n",
    "        ]\n",
    "        \n",
    "        input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=10, # Expecting \"Primary\" or \"Secondary\"\n",
    "                do_sample=False, # Use greedy decoding as per your preference\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Post-process the generated text to get the classification\n",
    "        predicted_type = \"Unknown\"\n",
    "        if \"Primary\" in generated_text:\n",
    "            predicted_type = \"Primary\"\n",
    "        elif \"Secondary\" in generated_text:\n",
    "            predicted_type = \"Secondary\"\n",
    "        \n",
    "        predictions.append({\n",
    "            \"article_id\": article_id,\n",
    "            \"dataset_id\": list(dc.dataset_ids)[0],\n",
    "            \"predicted_type\": predicted_type\n",
    "        })\n",
    "\n",
    "        # If you have test labels, you can collect true_labels here for evaluation\n",
    "        # For Kaggle, you'll typically submit predictions without knowing test labels.\n",
    "\n",
    "print(f\"Generated {len(predictions)} predictions.\")\n",
    "\n",
    "# 8.4. Evaluation (if test labels are available)\n",
    "# If you have a separate test_labels.json for local evaluation:\n",
    "# test_labels = load_labels(TEST_LABELS_PATH) # Load test labels\n",
    "#\n",
    "# # Match predictions to true labels and calculate metrics\n",
    "# # This part requires careful matching of dataset_id within article_id\n",
    "# # and might involve fuzzy matching for context if exact span isn't available.\n",
    "# # For simplicity, assuming exact match on article_id and dataset_id.\n",
    "#\n",
    "# y_true = []\n",
    "# y_pred = []\n",
    "#\n",
    "# for pred_entry in predictions:\n",
    "#     article_id = pred_entry[\"article_id\"]\n",
    "#     dataset_id = pred_entry[\"dataset_id\"]\n",
    "#     predicted_type = pred_entry[\"predicted_type\"]\n",
    "#\n",
    "#     # Find the true label for this specific dataset_id in this article\n",
    "#     found_true_label = False\n",
    "#     if article_id in test_labels:\n",
    "#         for gt_info in test_labels[article_id]:\n",
    "#             if gt_info[\"dataset_id\"] == dataset_id: # Exact match on ID\n",
    "#                 y_true.append(gt_info[\"citation_type\"])\n",
    "#                 y_pred.append(predicted_type)\n",
    "#                 found_true_label = True\n",
    "#                 break\n",
    "#     if not found_true_label:\n",
    "#         # Handle cases where a predicted ID might not be in ground truth\n",
    "#         # or where the ID extraction was imperfect.\n",
    "#         # For competition, this means your ID extraction needs to be precise.\n",
    "#         pass\n",
    "#\n",
    "# if y_true and y_pred:\n",
    "#     from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "#     # Map \"Primary\" to 1, \"Secondary\" to 0 for sklearn metrics\n",
    "#     label_map = {\"Primary\": 1, \"Secondary\": 0}\n",
    "#     y_true_mapped = [label_map.get(l, -1) for l in y_true]\n",
    "#     y_pred_mapped = [label_map.get(l, -1) for l in y_pred]\n",
    "#\n",
    "#     # Filter out -1 if there were unknown labels\n",
    "#     valid_indices = [i for i, val in enumerate(y_true_mapped) if val != -1 and y_pred_mapped[i] != -1]\n",
    "#     y_true_mapped = [y_true_mapped[i] for i in valid_indices]\n",
    "#     y_pred_mapped = [y_pred_mapped[i] for i in valid_indices]\n",
    "#\n",
    "#     if y_true_mapped:\n",
    "#         print(\"\\nEvaluation Results:\")\n",
    "#         print(f\"Accuracy: {accuracy_score(y_true_mapped, y_pred_mapped):.4f}\")\n",
    "#         print(f\"F1 Score (weighted): {f1_score(y_true_mapped, y_pred_mapped, average='weighted'):.4f}\")\n",
    "#         print(f\"Precision (weighted): {precision_score(y_true_mapped, y_pred_mapped, average='weighted'):.4f}\")\n",
    "#         print(f\"Recall (weighted): {recall_score(y_true_mapped, y_pred_mapped, average='weighted'):.4f}\")\n",
    "#     else:\n",
    "#         print(\"No matching true labels found for evaluation.\")\n",
    "# else:\n",
    "#     print(\"Not enough data to perform evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef4754",
   "metadata": {},
   "source": [
    "#### 9. Submission File Generation (Kaggle Specific)\n",
    "\n",
    "Finally, format your predictions into the required `submission.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1. Create Submission DataFrame\n",
    "\n",
    "submission_df = pd.DataFrame(predictions)\n",
    "# Rename columns to match Kaggle's expected format (e.g., 'id', 'class_label')\n",
    "# This will depend on the exact submission format specified by Kaggle.\n",
    "# Example:\n",
    "# submission_df = submission_df.rename(columns={\"article_id\": \"Id\", \"dataset_id\": \"DatasetId\", \"predicted_type\": \"Type\"})\n",
    "# submission_df[\"Id\"] = submission_df[\"Id\"] + \"_\" + submission_df[\"DatasetId\"] # If Id is a combination\n",
    "\n",
    "# Assuming the submission format is a list of dictionaries with 'article_id', 'dataset_id', 'citation_type'\n",
    "# You might need to adjust this based on the exact competition requirements.\n",
    "# For example, if it expects a single ID column like \"article_id_dataset_id\"\n",
    "final_submission_data = []\n",
    "for pred in predictions:\n",
    "    final_submission_data.append({\n",
    "        \"Id\": f\"{pred['article_id']}_{pred['dataset_id']}\", # Example: combine IDs\n",
    "        \"Type\": pred['predicted_type']\n",
    "    })\n",
    "\n",
    "final_submission_df = pd.DataFrame(final_submission_data)\n",
    "final_submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf893421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(tp, fp, fn):\n",
    "    return 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n",
    "    \n",
    "    \n",
    "# if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "pred_df = submission_df.copy()\n",
    "label_df = pd.read_csv(\"./kaggle/input/make-data-count-finding-data-references/sample_submission.csv\")\n",
    "label_df = label_df[label_df['type'] != 'Missing'].reset_index(drop=True)\n",
    "\n",
    "hits_df = label_df.merge(pred_df, on=[\"article_id\", \"dataset_id\", \"type\"])\n",
    "\n",
    "tp = hits_df.shape[0]\n",
    "fp = pred_df.shape[0] - tp\n",
    "fn = label_df.shape[0] - tp\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
