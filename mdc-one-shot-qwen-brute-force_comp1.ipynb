{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8ecd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2db1bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- 0. Environment Setup & Offline Preparation ---\n",
    "\n",
    "# Standard Imports\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import lxml.etree as etree\n",
    "from lxml.etree import _Element as Element # Type hinting for lxml.etree.Element\n",
    "import fitz # PyMuPDF for PDF processing\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from transformers.training_args import TrainingArguments\n",
    "import torch\n",
    "import kagglehub\n",
    "import spacy\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = \"cuda\" if torch and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b667ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this line once to download the model\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48e263db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for file paths and model configurations\n",
    "BASE_INPUT_DIR = './kaggle/input/make-data-count-finding-data-references'\n",
    "ARTICLE_TRAIN_DIR = os.path.join(BASE_INPUT_DIR, 'train')\n",
    "ARTICLE_TEST_DIR = os.path.join(BASE_INPUT_DIR, 'test')\n",
    "\n",
    "# Define directories for articles in train and test sets\n",
    "LABELED_TRAINING_DATA_CSV_PATH = os.path.join(BASE_INPUT_DIR, 'train_labels.csv')\n",
    "\n",
    "# Define the base model path\n",
    "QWEN_BASE_MODEL_PATH = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "\n",
    "# Output directory for the fine-tuned model and results\n",
    "BASE_OUTPUT_DIR = \"./kaggle/working\"\n",
    "FINE_TUNED_MODEL_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, \"qwen_finetuned_dataset_classifier\")\n",
    "FINAL_RESULTS_CSV_PATH = os.path.join(BASE_OUTPUT_DIR, \"article_dataset_classification.csv\")\n",
    "\n",
    "# Load a spaCy model (e.g., 'en_core_web_sm')\n",
    "# python -m spacy download en_core_web_sm \n",
    "NLP_SPACY = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0734f1",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f5e14",
   "metadata": {},
   "source": [
    "### Common dataset identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e50e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Information Extraction (IE) - Dataset Identification ---\n",
    "NON_STD_UNICODE_DASHES = re.compile(r'[\\u2010\\u2011\\u2012\\u2013\\u2014]')\n",
    "NON_STD_UNICODE_TICKS = re.compile(r'[\\u201c\\u201d]')\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input text by removing non-standard unicode dashes and extra whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Replace all non-standard unicode dashes with '-'\n",
    "    text = text.replace('\\u200b', '').replace('-\\n', '-').replace('_\\n', '_').replace('/\\n', '/')\n",
    "    text = NON_STD_UNICODE_DASHES.sub('-', text)\n",
    "    text = NON_STD_UNICODE_TICKS.sub(\"'\", text)\n",
    "    # Remove extra whitespace\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Regex patterns for common dataset identifiers\n",
    "# DOI_PATTERN = r'10\\.\\d{4,5}/[-._;()/:A-Za-z0-9\\u002D\\u2010\\u2011\\u2012\\u2013\\u2014\\u2015]+'\tDOI_PATTERN\n",
    "# DOI_PATTERN = r'10\\.\\s?\\d{4,5}\\/[-._()<>;\\/:A-Za-z0-9]+\\s?(?:(?![A-Z]+)(?!\\d{1,3}\\.))+[-._()<>;\\/:A-Za-z0-9]+'\n",
    "#DOI_PATTERN = r'\\bhttps://doi.org/10\\.\\d{4,5}\\/[-._\\/:A-Za-z0-9]+'\n",
    "DOI_PATTERN = r'\\b10\\.\\d{4,5}\\/[-._\\/:A-Za-z0-9]+'\n",
    "EPI_PATTERN = r'\\bEPI[-_A-Z0-9]{2,}'\n",
    "SAM_PATTERN = r'\\bSAMN[0-9]{2,}'          # SAMN07159041\n",
    "IPR_PATTERN = r'\\bIPR[0-9]{2,}'\n",
    "CHE_PATTERN = r'\\bCHEMBL[0-9]{2,}'\n",
    "PRJ_PATTERN = r'\\bPRJ[A-Z0-9]{2,}'\n",
    "E_G_PATTERN = r'\\bE-[A-Z]{4}-[0-9]{2,}'   # E-GEOD-19722 or E-PROT-100\n",
    "ENS_PATTERN = r'\\bENS[A-Z]{4}[0-9]{2,}'\n",
    "CVC_PATTERN = r'\\bCVCL_[A-Z0-9]{2,}'\n",
    "EMP_PATTERN = r'\\bEMPIAR-[0-9]{2,}'\n",
    "PXD_PATTERN = r'\\bPXD[0-9]{2,}'\n",
    "HPA_PATTERN = r'\\bHPA[0-9]{2,}'\n",
    "SRR_PATTERN = r'\\bSRR[0-9]{2,}'\n",
    "GSE_PATTERN = r'\\b(GSE|GSM|GDS|GPL)\\d{4,6}\\b' # Example for GEO accession numbers (e.g., GSE12345, GSM12345)\n",
    "GNB_PATTERN = r'\\b[A-Z]{1,2}\\d{5,6}\\b' # GenBank accession numbers (e.g., AB123456, AF000001)\n",
    "CAB_PATTERN = r'\\bCAB[0-9]{2,}'\n",
    "\n",
    "# Combine all patterns into a list\n",
    "DATASET_ID_PATTERNS = [\n",
    "    DOI_PATTERN,\n",
    "    EPI_PATTERN,\n",
    "    SAM_PATTERN,\n",
    "    IPR_PATTERN,\n",
    "    CHE_PATTERN,\n",
    "    PRJ_PATTERN,\n",
    "    E_G_PATTERN,\n",
    "    ENS_PATTERN,\n",
    "    CVC_PATTERN,\n",
    "    EMP_PATTERN,\n",
    "    PXD_PATTERN,\n",
    "    HPA_PATTERN,\n",
    "    SRR_PATTERN,\n",
    "    GSE_PATTERN,\n",
    "    GNB_PATTERN,\n",
    "    CAB_PATTERN,\n",
    "]\n",
    "\n",
    "# Compile all patterns for efficiency\n",
    "COMPILED_DATASET_ID_REGEXES = [re.compile(p) for p in DATASET_ID_PATTERNS]\n",
    "\n",
    "# Data related keywords to look for in the text\n",
    "# These keywords help to ensure that the text is relevant to datasets\n",
    "DATA_RELATED_KEYWORDS = ['data release', 'data associated', 'data availability', 'data access', 'download', 'program data', 'the data', 'dataset', 'database', 'repository', 'data source', 'data access', 'archive', 'arch.', 'digital']\n",
    "\n",
    "def is_text_data_related(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in DATA_RELATED_KEYWORDS)\n",
    "\n",
    "def text_has_dataset_id(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given text contains any dataset identifier.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to check for dataset identifiers.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if any dataset identifier is found, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    occurrences_with_context: list[str] = []\n",
    "    for regex in COMPILED_DATASET_ID_REGEXES:\n",
    "        if regex.search(text):\n",
    "            text_lower = text.lower()\n",
    "            # Check for specific keywords in the text\n",
    "            if any(keyword in text_lower for keyword in DATA_RELATED_KEYWORDS):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def extract_dataset_ids(text: str, context_chars: int = 250) -> list[dict[str, list[str] | str]]:\n",
    "    \"\"\"\n",
    "    Extract dataset identifiers with context from the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to search for dataset identifiers.\n",
    "        context_chars (int): Number of characters to include before and after the match for context.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: A list of extracted dataset identifiers with context.\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    is_small_context = len(text) < context_chars * 2\n",
    "    dataset_ids: list[str] = []\n",
    "    occurrences_with_context: list[dict[str, list[str] | str]] = []\n",
    "    if is_text_data_related(text):\n",
    "        for regex in COMPILED_DATASET_ID_REGEXES:\n",
    "            matches = regex.finditer(text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                dataset_id = text[match.start() : match.end()]\n",
    "                if is_small_context:\n",
    "                    dataset_ids.append(dataset_id)\n",
    "                else:\n",
    "                    citation_context = text[max(0, match.start() - context_chars): match.end() + context_chars ]\n",
    "                    citation_context = citation_context.replace('\\n', '').replace('[', '').replace(']', '')\n",
    "                    citation_context = re.sub(r'\\s+', ' ', citation_context).strip()\n",
    "                    if is_text_data_related(citation_context):\n",
    "                        occurrences_with_context.append({\"dataset_ids\": [dataset_id], \"citation_context\": citation_context})\n",
    "                        #occurrences_with_context.append(\"{\" + f'\"dataset_ids\": {[dataset_id]}, citation_context: \"{citation_context}\"' + \"}\")\n",
    "        if dataset_ids:\n",
    "            occurrences_with_context.append({\"dataset_ids\": dataset_ids, \"citation_context\": text})\n",
    "    \n",
    "    # # If no occurrences found, return an empty string\n",
    "    # # Otherwise, join the occurrences with a specific separator\n",
    "    # if not occurrences_with_context:\n",
    "    #     return \"\"\n",
    "    # return json.dumps(occurrences_with_context, separators=(',', ':'))\n",
    "    return occurrences_with_context\n",
    "\n",
    "# Use NLP to get sentences from the given text\n",
    "\n",
    "def get_sentences_from_text(text: str, nlp=NLP_SPACY) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Replace all non-standard unicode dashes with '-'\n",
    "    text = clean_text(text)\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    doc_spacy = nlp(text)\n",
    "    return \" \".join([sent.text for sent in doc_spacy.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2382661e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: Seventy-eight pleural effusions. CT scans acquired from The Cancer Imaging Archive 'NSCLC Radiomics' data collection. All expert-vetted segmentations are publicly available in NIfTI format through The Cancer Imaging Archive at https://doi.org/10.7937/tcia.2020.6c7y-gq39\n"
     ]
    }
   ],
   "source": [
    "s = \"Seventy\\u2010eight pleural effusions. CT scans acquired from The Cancer \\nImaging Archive \\u201cNSCLC Radiomics\\u201d data collection. All expert\\u2010vetted segmentations are publicly available in NIfTI format through The Cancer Imaging Archive at  https://doi.org/10.7937/tcia.2020.6c7y\\u2010gq39\"\n",
    "print(f\"Cleaned text: {get_sentences_from_text(s)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e6435",
   "metadata": {},
   "source": [
    "### XML Element Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7a53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_element_text(element: Element | None) -> str:\n",
    "    if element is not None:\n",
    "        # Use itertext() to get all text content from the <p> tag and its descendants\n",
    "        # and join them into a single string.\n",
    "        all_text = \" \".join(element.itertext(tag=None)).replace('\\u200b', '').strip()\n",
    "        return all_text[:2000]\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def extract_next_sibling_text(elements: list[Element] | None, sibling_xpath: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from the next sibling of the given XML element.\n",
    "    \n",
    "    Args:\n",
    "        element (Element | None): The XML element whose next sibling's text is to be extracted.\n",
    "        sibling_xpath (str): The XPath expression to find the next sibling element. (eg. \"following-sibling::passage[1]\")\n",
    "        \n",
    "    Returns:\n",
    "        str: A string containing the text from the next sibling element, or an empty string if no sibling exists.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if the provided elements list is None or empty\n",
    "    if not elements:\n",
    "        return \"\"\n",
    "    \n",
    "    # Assuming there's only one such element, take the first one found\n",
    "    # and find the element immediately following based on the given sibling_xpath.\n",
    "    first_element = elements[0]\n",
    "    sibling_elements = first_element.xpath(sibling_xpath)\n",
    "\n",
    "    if not sibling_elements:\n",
    "        # print(\"DEBUG: No following <passage> element found.\") # Uncomment for debugging\n",
    "        return \"\"\n",
    "    \n",
    "    next_sibling = sibling_elements[0]\n",
    "    if next_sibling is None:\n",
    "        return \"\"\n",
    "    \n",
    "    return extract_element_text(next_sibling)\n",
    "\n",
    "def extract_elements_text(elements: list[Element] | None, sep: str = \" \") -> str:\n",
    "    elements_text = []\n",
    "    if elements is None:\n",
    "        return \"\"\n",
    "    \n",
    "    for element in elements:\n",
    "        text = extract_element_text(element)\n",
    "        if text:\n",
    "            elements_text.append(text)\n",
    "\n",
    "    return sep.join(elements_text).strip()\n",
    "\n",
    "def extract_elements_text_from_xpath_list(root: Element | None, xpath_list: list[str], ns: dict[str, str] | None = None) -> str:\n",
    "    elements_text = \"\"\n",
    "    if root is None or not xpath_list:\n",
    "        return \"\"\n",
    "    \n",
    "    for xpath in xpath_list:\n",
    "        element = root.find(xpath, namespaces=ns)\n",
    "        elements_text += extract_element_text(element)\n",
    "    return elements_text\n",
    "\n",
    "def extract_text_from_elements_within_element(element: Element | None, child_xpaths: list[str] = [], ns: dict[str, str] | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from elements within a given XML element that match the specified tag names.\n",
    "    \n",
    "    Args:\n",
    "        element (Element | None): The XML element to search within.\n",
    "        tag_names (list[str]): A list of tag names to search for.\n",
    "        \n",
    "    Returns:\n",
    "        str: A string containing the extracted text from the matching elements.\n",
    "    \"\"\"\n",
    "    if element is None:\n",
    "        return \"\"\n",
    "    \n",
    "    if not child_xpaths:\n",
    "        # If no child tag names are provided, return the text of the element itself\n",
    "        return extract_element_text(element)\n",
    "    \n",
    "    extracted_text = []\n",
    "    for xpath in child_xpaths:\n",
    "        for child in element.findall(xpath, namespaces=ns):\n",
    "            text = extract_element_text(child)\n",
    "            if text:\n",
    "                extracted_text.append(text)\n",
    "    \n",
    "    return \"|\".join(extracted_text)\n",
    "\n",
    "def extract_data_related_elements_text(elements: list[Element] | None, child_xpaths: list[str] = [], ns: dict[str, str] | None = None) -> list[dict[str, list[str] | str]]:\n",
    "    elements_text = []\n",
    "    if elements is None:\n",
    "        return elements_text\n",
    "    \n",
    "    for element in elements:\n",
    "        text = extract_dataset_ids(extract_text_from_elements_within_element(element, child_xpaths, ns))\n",
    "        if text:\n",
    "            elements_text.extend(text)\n",
    "\n",
    "    return elements_text\n",
    "\n",
    "def extract_data_related_elements_text_from_xpath_list(root: Element | None, xpath_list: list[str], ns: dict[str, str] | None = None) -> list[dict[str, list[str] | str]]:\n",
    "    \"\"\"\n",
    "    Extracts text from elements in the XML tree that match the provided XPath expressions.\n",
    "    \n",
    "    Args:\n",
    "        root (Element | None): The root element of the XML tree.\n",
    "        xpath_list (list[str]): A list of XPath expressions to search for elements.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: A list of extracted text from the matching elements.\n",
    "    \"\"\"\n",
    "    elements_text = []\n",
    "    if root is None or not xpath_list:\n",
    "        return elements_text\n",
    "    \n",
    "    for xpath in xpath_list:\n",
    "        primary_xpath, *child_xpath_text = xpath.split('||')\n",
    "        child_xpaths = child_xpath_text[0].split(',') if child_xpath_text else []\n",
    "        elements = root.findall(primary_xpath, namespaces=ns)\n",
    "        if elements:\n",
    "            elements_text.extend(extract_data_related_elements_text(elements, child_xpaths, ns))\n",
    "    return elements_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab6146",
   "metadata": {},
   "source": [
    "### PDF File Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02f6ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_author_names(full_text: str, nlp=NLP_SPACY) -> str:\n",
    "    \"\"\"\n",
    "    Extracts potential author names from the beginning of a research article's text\n",
    "    using spaCy's Named Entity Recognition. It attempts to isolate the author section\n",
    "    and applies heuristics to filter out non-author entities.\n",
    "\n",
    "    Args:\n",
    "        full_text (str): The complete text content of the research article,\n",
    "                         typically extracted from a PDF.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of unique strings, each representing a potential author name,\n",
    "                   sorted alphabetically. Returns an empty list if no authors are found.\n",
    "    \"\"\"\n",
    "    if not full_text or not full_text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    full_text = full_text.replace('1\\n,', ',').replace('1,', ',').replace('\\u2019', \"'\")\n",
    "\n",
    "    # 1. Isolate the potential author section\n",
    "    # Authors are typically at the very beginning, before the abstract or introduction.\n",
    "    # We'll search for common section headers to define the end of the author block.\n",
    "    # Using regex for case-insensitive search and handling various newline/spacing.\n",
    "    header_patterns = [\n",
    "        r\"\\n\\s*Abstract\\s*\\n\",\n",
    "        r\"\\n\\s*Introduction\\s*\\n\",\n",
    "        r\"\\n\\s*Summary\\s*\\n\",\n",
    "        r\"\\n\\s*Keywords\\s*\\n\",\n",
    "        r\"\\n\\s*Graphical Abstract\\s*\\n\",\n",
    "        r\"\\n\\s*1\\.\\s*Introduction\\s*\\n\", # Common for numbered sections\n",
    "        r\"\\n\\s*DOI:\\s*\\n\" # Sometimes DOI appears before abstract\n",
    "    ]\n",
    "\n",
    "    author_section_end_index = len(full_text)\n",
    "    for pattern in header_patterns:\n",
    "        match = re.search(pattern, full_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            # Take text up to the start of the found header\n",
    "            author_section_end_index = min(author_section_end_index, match.start())\n",
    "            break\n",
    "    \n",
    "    # As a fallback or if no header is found early, limit the search to the first\n",
    "    # 2500 characters. This prevents processing the entire document for authors.\n",
    "    author_section_text = full_text[:min(author_section_end_index, 2500)]\n",
    "\n",
    "    if not author_section_text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    # 2. Process the isolated author section with spaCy\n",
    "    doc = nlp(author_section_text)\n",
    "\n",
    "    # 3. Extract PERSON entities and apply initial filtering\n",
    "    potential_authors: list[str] = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            name = ent.text.strip()\n",
    "            # Basic filtering to reduce false positives:\n",
    "            # - Exclude very short strings (e.g., single letters, common conjunctions)\n",
    "            # - Exclude common stop words (e.g., \"The\", \"And\")\n",
    "            # - Exclude all-uppercase strings that might be acronyms (e.g., \"WHO\", \"NASA\")\n",
    "            # - Ensure it contains at least one space (e.g., \"John Doe\") or is a capitalized\n",
    "            #   single word that's longer than 2 characters (e.g., \"Smith\").\n",
    "            if (len(name) > 1 and\n",
    "                name.lower() not in nlp.Defaults.stop_words and\n",
    "                not name.isupper() and\n",
    "                (' ' in name or (name[0].isupper() and len(name) > 2))):\n",
    "                \n",
    "                potential_authors.append(name)\n",
    "\n",
    "    # 4. Apply more advanced heuristics to filter out non-author names\n",
    "    # This step is crucial for accuracy and often requires tuning.\n",
    "    filtered_authors = []\n",
    "    for author in potential_authors:\n",
    "        # Heuristic 1: Filter out names that contain common affiliation keywords.\n",
    "        # This is a simple check; more robust solutions might use spaCy's dependency\n",
    "        # parsing to check if a PERSON entity is part of an ORG entity.\n",
    "        affiliation_keywords = [\"univ\", \"observ\", \"institute\", \"department\", \"center\", \"lab\",\n",
    "                                \"hospital\", \"college\", \"school\", \"inc.\", \"ltd.\", \"company\",\n",
    "                                \"corp.\", \"group\", \"foundation\", \"research\"]\n",
    "        if any(keyword in author.lower() for keyword in affiliation_keywords):\n",
    "            continue # Skip if it looks like an affiliation\n",
    "\n",
    "        # Heuristic 2: Filter out names that contain email patterns or ORCID patterns.\n",
    "        if '@' in author or re.search(r'\\b\\d{4}-\\d{4}-\\d{4}-\\d{3}[\\dX]\\b', author):\n",
    "            continue # Skip if it contains an email or ORCID\n",
    "\n",
    "        # Heuristic 3: Filter out names that are likely just initials or very short.\n",
    "        # This is partially covered by initial filtering, but can be refined.\n",
    "        # E.g., \"J. D.\" might be an author, but \"J.\" alone is unlikely.\n",
    "        if len(author.split()) == 1 and len(author) <= 2 and author.isupper():\n",
    "            continue # Skip single-letter or two-letter uppercase (e.g., \"JD\")\n",
    "\n",
    "        filtered_authors.append(author)\n",
    "\n",
    "    # Convert to list and sort for consistent output\n",
    "    return filtered_authors[0] if filtered_authors else \"\"\n",
    "\n",
    "def extract_pdf_doc_text(pdf_doc: fitz.Document)  -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    \"\"\"\n",
    "    Extracts all text from a PDF document using PyMuPDF.\n",
    "    \n",
    "    Args:\n",
    "        pdf_doc (fitz.Document): The PDF document to extract text from.\n",
    "        \n",
    "    Returns:\n",
    "        str: A JSON string of the article_dict containing specific elements extracted from the PDF.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the article dictionary with empty strings\n",
    "    article_dict = {\n",
    "        'title': '',\n",
    "        'author': '',\n",
    "        'abstract': '',\n",
    "        'data_availability': '',\n",
    "        'other_dataset_citations': []\n",
    "    }\n",
    "\n",
    "    # Initialize variables for text extraction\n",
    "    p1 = None  # Placeholder for the first page text\n",
    "    other_dataset_citations = []\n",
    "    for page in pdf_doc:\n",
    "        # Extract text from the page\n",
    "        textpage = page.get_textpage()\n",
    "        if page.number == 0:\n",
    "            p1_txt = textpage.extractTEXT()\n",
    "            p1 = get_sentences_from_text(p1_txt)\n",
    "            p1 = p1[:int(len(p1)/2)]\n",
    "            article_dict['author'] = extract_author_names(p1_txt, nlp=NLP_SPACY)\n",
    "\n",
    "        # Extract text from all blocks that have an abstract or dataset id's\n",
    "        blocks = textpage.extractBLOCKS()\n",
    "        for block in blocks:\n",
    "            block_text = get_sentences_from_text(block[4])\n",
    "            block_text_lower = block_text.lower()\n",
    "            if page.number == 0 and len(block_text) > 100 and \"abstract\" in block_text_lower:\n",
    "                # Add the abstract block text to the article dictionary\n",
    "                article_dict['abstract'] = block_text\n",
    "            elif \"data availability\" in block_text_lower or \"data accessibility\" in block_text_lower or \"acknowledgments\" in block_text_lower:\n",
    "                # Add the data availability block text to the article dictionary\n",
    "                article_dict['data_availability'] = block_text\n",
    "            else:\n",
    "                context_chars = min(250, len(block_text))  # Use a minimum\n",
    "                dataset_ids_found = extract_dataset_ids(block_text, context_chars)  # Extract dataset IDs from the block text\n",
    "                if dataset_ids_found:\n",
    "                    # print(f\"DEBUG: Found dataset IDs in block: {dataset_ids_found}\")  # Debugging output\n",
    "                    # print(f\"DEBUG: block_text: {block_text}\")  # Debugging output\n",
    "                    if len(article_dict['data_availability']) > 0 and len(article_dict['data_availability']) < 25:\n",
    "                        # If data availability text is only a few characters, append the next block text to it\n",
    "                        # This is a heuristic to ensure that we capture relevant dataset IDs\n",
    "                        article_dict['data_availability'] = block_text\n",
    "                    else:\n",
    "                        # Append the dataset IDs found in the block to the other_dataset_citations\n",
    "                        other_dataset_citations.extend(dataset_ids_found)\n",
    "\n",
    "    article_dict['other_dataset_citations'] = other_dataset_citations if other_dataset_citations else []\n",
    "    \n",
    "    # If an abstract was not found, use the first page text as the abstract\n",
    "    if not article_dict['abstract'] and p1:\n",
    "        article_dict['abstract'] = p1\n",
    "\n",
    "    # Return the article dictionary as a JSON string\n",
    "    return article_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2690ab4",
   "metadata": {},
   "source": [
    "### XML File Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d341bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_xml_text_jats(root: Element) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Find the title, abstract, and data availablity info for Journal Archiving and Interchange DTD (JATS)\n",
    "    # The \".//\" ensures it searches anywhere in the document, not just direct children of root.\n",
    "    ns = None  # No namespaces for JATS\n",
    "\n",
    "    xpath_title = \".//article-title\"\n",
    "    xpath_authors_1 = \".//contrib-group/contrib[@contrib-type='author']/name\"\n",
    "    xpath_authors_2 = \".//biblstruct/analytic/author[@role='corresp']/persname\"\n",
    "    author = extract_element_text(root.find(xpath_authors_1, namespaces=ns))\n",
    "    if not author:\n",
    "        author = extract_element_text(root.find(xpath_authors_2, namespaces=ns))\n",
    "    xpath_abstract = \".//abstract\"\n",
    "    xpath_data_avails = [\".//notes[@notes-type='data-availability']\", \".//sec[@sec-type='data-availability']\"]\n",
    "    xpath_citations = [\".//element-citation||.article-title,.source,.pub-id\", \".//mixed-citation\"]  # List of XPath expressions for citations\n",
    "\n",
    "    return {\n",
    "        'title': extract_element_text(root.find(xpath_title, ns)),\n",
    "        'author': author,\n",
    "        'abstract': get_sentences_from_text(extract_element_text(root.find(xpath_abstract, ns))),\n",
    "        'data_availability': extract_elements_text_from_xpath_list(root, xpath_data_avails, ns=ns),\n",
    "        'other_dataset_citations': extract_data_related_elements_text_from_xpath_list(root, xpath_citations, ns=ns),\n",
    "    }\n",
    "\n",
    "def extract_xml_text_tei(root: Element) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Find the title, abstract, and data availability info for Text Encoding Initiative (TEI)\n",
    "    # Set the namespace for TEI\n",
    "    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    xpath_title = \".//tei:title\"\n",
    "    xpath_authors = \".//tei:sourcedesc/tei:biblstruct/tei:analytic/tei:author/tei:persname\"\n",
    "    xpath_abstract = \".//tei:abstract\"\n",
    "    xpath_data_avail = \"\" #\".//tei:biblstruct\"\n",
    "    xpath_citations = [\".//tei:biblstruct||.//tei:title,.//tei:idno,.//tei:notes\"]  # List of XPath expressions for citations\n",
    "        \n",
    "    return {\n",
    "        'title': extract_element_text(root.find(xpath_title, namespaces=ns)),\n",
    "        'author': extract_element_text(root.find(xpath_authors, namespaces=ns)),\n",
    "        'abstract': get_sentences_from_text(extract_element_text(root.find(xpath_abstract, namespaces=ns))),\n",
    "        'data_availability': xpath_data_avail,  # No direct extraction for TEI data_availability\n",
    "        'other_dataset_citations': extract_data_related_elements_text_from_xpath_list(root, xpath_citations, ns=ns),\n",
    "    }\n",
    "\n",
    "def extract_xml_text_wiley(root: Element) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Find the title, abstract, and data availability info for Wiley XML format\n",
    "    # Set the namespace for Wiley\n",
    "    ns = {'ns': 'http://www.wiley.com/namespaces/wiley'}\n",
    "\n",
    "    xpath_title = \".//ns:publicationMeta[@level='part']/ns:titleGroup\"    #<publicationMeta level=\"part\"><titleGroup><title type=\"main\">\n",
    "    xpath_authors = \".//selfCitationGroup/citation[@type='self']/author\"\n",
    "    xpath_abstract = \".//ns:abstract[@type='main']\"  #<abstract type=\"main\"\n",
    "    xpath_data_avail = \".//ns:section[@type='dataAvailability']\"  #<section numbered=\"no\" type=\"dataAvailability\"\n",
    "    xpath_citations = [\".//ns:citation||.//ns:articleTitle,.//ns:journalTitle,.//ns:url\"]  # List of XPath expressions for citations\n",
    "        \n",
    "    return {\n",
    "        'title': extract_elements_text(root.findall(xpath_title, namespaces=ns)),\n",
    "        'author': extract_element_text(root.find(xpath_authors, namespaces=ns)),\n",
    "        'abstract': get_sentences_from_text(extract_element_text(root.find(xpath_abstract, namespaces=ns))),\n",
    "        'data_availability': extract_element_text(root.find(xpath_data_avail, namespaces=ns)),\n",
    "        'other_dataset_citations': extract_data_related_elements_text_from_xpath_list(root, xpath_citations, ns=ns),\n",
    "    }\n",
    "\n",
    "def extract_xml_text_biorxiv(root: Element) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Find the title, abstract, and data availability info for BioRxiv XML format\n",
    "    # Set the namespace for BioRxiv\n",
    "    ns = {'biorxiv': 'http://www.biorxiv.org'}\n",
    "\n",
    "    xpath_title = \".//biorxiv:title\"\n",
    "    xpath_authors = \".//biorxiv:contrib[@contrib-type='author']/biorxiv:name\"\n",
    "    xpath_abstract = \".//biorxiv:abstract\"\n",
    "    xpath_data_avail = \".//biorxiv:sec[@sec-type='data-availability']\"  #<sec sec-type=\"data-availability\"\n",
    "    xpath_citations = [\".//biorxiv:biblio||.//biorxiv:title,.//biorxiv:source,.//biorxiv:pub-id\"]  # List of XPath expressions for citations\n",
    "        \n",
    "    return {\n",
    "        'title': extract_element_text(root.find(xpath_title, namespaces=ns)),\n",
    "        'author': extract_element_text(root.find(xpath_authors, namespaces=ns)),\n",
    "        'abstract': get_sentences_from_text(extract_element_text(root.find(xpath_abstract, namespaces=ns))),\n",
    "        'data_availability': extract_element_text(root.find(xpath_data_avail, namespaces=ns)),\n",
    "        'other_dataset_citations': extract_data_related_elements_text_from_xpath_list(root, xpath_citations, ns=ns),\n",
    "    }\n",
    "\n",
    "def extract_xml_text_bioc(root: Element) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Find the title, abstract, and data availability info for BioC-API XML format\n",
    "    ns = None  # No namespaces for BioC\n",
    "\n",
    "    xpath_title = \"string(.//passage[infon[@key='section_type' and text()='TITLE']]/text)\"\n",
    "    xpath_authors = \"string(.//infon[@key='name_0'] | .//infon[@key='name_1'])\"\n",
    "    xpath_abstract = \"string(.//passage[infon[@key='section_type' and text()='ABSTRACT']]/text)\"\n",
    "    xpath_data_avail = \".//passage[text[text()='DATA ACCESSIBILITY:']]\"\n",
    "    xpath_data_avail_sibling = \"following-sibling::passage[1]\"\n",
    "    xpath_citations = []\n",
    "        \n",
    "    return {\n",
    "        'title': root.xpath(xpath_title, namespaces=ns),\n",
    "        'author': root.xpath(xpath_authors, namespaces=ns).strip().replace('surname:', '').replace(';given-names:', ' '),\n",
    "        'abstract': get_sentences_from_text(root.xpath(xpath_abstract, namespaces=ns)[:2000]),  # Limit to 2000 characters\n",
    "        'data_availability': extract_next_sibling_text(root.xpath(xpath_data_avail, namespaces=ns), xpath_data_avail_sibling),\n",
    "        'other_dataset_citations': xpath_citations,\n",
    "    }\n",
    "\n",
    "def extract_xml_text_taxonx(root: Element) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Find the title, abstract, and data availability info for TaxonX format\n",
    "    ns = None  # No namespaces for Taxonomic Treatment Publishing DTD\n",
    "\n",
    "    xpath_title = \"string(.//article-meta/title-group/article-title)\"\n",
    "    xpath_authors = \"\"\n",
    "    xpath_abstract = \"string(.//article-meta/abstract)\"\n",
    "    xpath_data_avail = \"\"\n",
    "    xpath_citations = []\n",
    "        \n",
    "    return {\n",
    "        'title': root.xpath(xpath_title, namespaces=ns),\n",
    "        'author': xpath_authors,\n",
    "        'abstract': get_sentences_from_text(root.xpath(xpath_abstract, namespaces=ns)[:2000]),  # Limit to 2000 characters\n",
    "        'data_availability': xpath_data_avail,  # No direct extraction for TaxonX data_availability\n",
    "        'other_dataset_citations': xpath_citations,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229f8e8",
   "metadata": {},
   "source": [
    "## File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15e0697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dictionary mapping XML types to their respective extraction functions\n",
    "XML_TYPE_EXTRACTORS = {\n",
    "    'jats': extract_xml_text_jats,\n",
    "    'tei': extract_xml_text_tei,\n",
    "    'wiley': extract_xml_text_wiley,\n",
    "    'bioc': extract_xml_text_bioc,\n",
    "    'taxonx': extract_xml_text_taxonx,\n",
    "}\n",
    "\n",
    "# --- Data Loading ---\n",
    "def get_file_extension(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the file extension of the given file path.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "        \n",
    "    Returns:\n",
    "        str: The file extension, or an empty string if no extension is found.\n",
    "    \"\"\"\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "    return ext.lower() if ext else \"\"\n",
    "\n",
    "def read_first_line_of_xml(file_path: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Reads and returns the first line of an XML file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "\n",
    "    Returns:\n",
    "        str | None: The first line of the file, stripped of leading/trailing whitespace,\n",
    "                    or None if the file cannot be read or is empty.\n",
    "    \"\"\"\n",
    "    if not file_path and not os.path.exists(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline().replace('<?xml version=\"1.0\" encoding=\"UTF-8\"?>', '').replace('<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>', '').strip()\n",
    "            # If the first line is empty, read the next line\n",
    "            if not first_line:\n",
    "                first_line = f.readline()\n",
    "            return first_line.strip()[:90] if first_line else None\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
    "                first_line = f.readline().replace('<?xml version=\"1.0\" encoding=\"UTF-8\"?>', '').replace('<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>', '').strip()\n",
    "                if not first_line:\n",
    "                    first_line = f.readline()\n",
    "                return first_line.strip()[:90] if first_line else None\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file '{file_path}': {e}\")\n",
    "        return None\n",
    "    \n",
    "def identify_xml_type(first_line: str) -> str:\n",
    "    \"\"\"\n",
    "    Identifies the XML type based on the first line of the XML file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "        \n",
    "    Returns:\n",
    "        str: The identified XML type ('jats', 'tei', 'wiley', 'bioc', or 'unknown').\n",
    "    \"\"\"\n",
    "    if not first_line:\n",
    "        return \"unknown\"\n",
    "    first_line_lower = first_line.lower()\n",
    "    # Check for specific patterns in the first line\n",
    "    if 'journal archiving and interchange dtd' in first_line_lower:\n",
    "        return \"jats\"\n",
    "    elif 'xmlns=\"http://www.tei-c.org/ns/1.0\"' in first_line_lower:\n",
    "        return \"tei\"\n",
    "    elif 'xmlns=\"http://www.wiley.com/namespaces/wiley\"' in first_line_lower:\n",
    "        return \"wiley\"\n",
    "    elif 'bioc.dtd' in first_line_lower or 'bioc-api' in first_line_lower:\n",
    "        return \"bioc\"\n",
    "    elif 'taxonomic treatment publishing dtd' in first_line_lower:\n",
    "        return \"taxonx\"\n",
    "    \n",
    "    return \"unknown\"    \n",
    "\n",
    "def get_xml_type(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Determines the XML type of a file based on its first line.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "        \n",
    "    Returns:\n",
    "        str: The identified XML type ('jats', 'tei', 'wiley', 'bioc', 'taxonx', or 'unknown').\n",
    "    \"\"\"\n",
    "    first_line = \"\"\n",
    "    if \".xml\" == get_file_extension(file_path):\n",
    "        # If the file is an XML file, read the first line and identify the type\n",
    "        first_line = read_first_line_of_xml(file_path)\n",
    "    return identify_xml_type(first_line) if first_line else \"unknown\"\n",
    "\n",
    "def load_file_paths(dataset_type_dir: str) -> pd.DataFrame: \n",
    "    pdf_path = os.path.join(dataset_type_dir, 'PDF')\n",
    "    xml_path = os.path.join(dataset_type_dir, 'XML')\n",
    "    dataset_type = os.path.basename(dataset_type_dir)\n",
    "    pdf_files = [f for f in os.listdir(pdf_path) if f.endswith('.pdf')]\n",
    "    xml_files = [f for f in os.listdir(xml_path) if f.endswith('.xml')]\n",
    "    df_pdf = pd.DataFrame({\n",
    "        'article_id': [f.replace('.pdf', '') for f in pdf_files],\n",
    "        'pdf_file_path': [os.path.join(pdf_path, f) for f in pdf_files]\n",
    "    })\n",
    "    df_xml = pd.DataFrame({\n",
    "        'article_id': [f.replace('.xml', '') for f in xml_files],\n",
    "        'xml_file_path': [os.path.join(xml_path, f) for f in xml_files]\n",
    "    })\n",
    "    merge_df = pd.merge(df_pdf, df_xml, on='article_id', how='outer', suffixes=('_pdf', '_xml'), validate=\"one_to_many\")\n",
    "    merge_df['dataset_type'] = dataset_type\n",
    "    return merge_df\n",
    "\n",
    "def extract_pdf_text(file_path: str, xml_type: str | None = None) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    \"\"\"Extracts all text from a PDF file using PyMuPDF.\"\"\"\n",
    "    article_dict = {}\n",
    "    if file_path and os.path.exists(file_path):\n",
    "        try:\n",
    "            with fitz.open(file_path) as doc:\n",
    "                article_dict = extract_pdf_doc_text(doc)  # Extract text from the PDF document\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"PDF file not found: {file_path}\")\n",
    "    \n",
    "    return article_dict\n",
    "\n",
    "def extract_xml_text(file_path: str, xml_type: str) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    \"\"\"Reads and extracts text from an XML file based on the specified XML type.\n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "        xml_type (str): The type of XML format (e.g., 'jats', 'tei', 'wiley', 'bioc', 'taxonx').\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted text from the XML file.\n",
    "    \"\"\"\n",
    "    # Initialize the article dictionary\n",
    "    article_dict = {}\n",
    "    if file_path and os.path.exists(file_path):\n",
    "        # Disable external entity resolution for security\n",
    "        parser = etree.XMLParser(resolve_entities=False, no_network=True)\n",
    "        try:\n",
    "            tree = etree.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "            # Use the appropriate extraction function based on the xml_type\n",
    "            extract_function = XML_TYPE_EXTRACTORS.get(xml_type, extract_xml_text_jats)  \n",
    "            article_dict = extract_function(root)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading XML {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"XML file not found: {file_path}\")    \n",
    "    return article_dict\n",
    "\n",
    "def dedupe_other_dataset_citations(article_dict: dict[str, str | list[dict[str, list[str] | str]]]) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    \"\"\"\n",
    "    Deduplicates dataset IDs in the article dictionary.\n",
    "    \n",
    "    Args:\n",
    "        article_dict (dict): The article dictionary containing dataset IDs.\n",
    "        \n",
    "    Returns:\n",
    "        dict: The updated article dictionary with deduplicated dataset IDs.\n",
    "    \"\"\"\n",
    "    unique_dataset_ids = set()\n",
    "    unique_dataset_id_citations = []\n",
    "    if 'other_dataset_citations' in article_dict and isinstance(article_dict['other_dataset_citations'], list):\n",
    "        dataset_citations = article_dict['other_dataset_citations']\n",
    "        for citation_dict in dataset_citations:\n",
    "            dataset_ids = citation_dict['dataset_ids']\n",
    "            if isinstance(dataset_ids, list):\n",
    "                if len(dataset_ids) == 1:\n",
    "                    dataset_id = dataset_ids[0]\n",
    "                    if dataset_id not in article_dict['data_availability'] and dataset_id not in unique_dataset_ids:\n",
    "                        unique_dataset_ids.add(dataset_id)\n",
    "                        unique_dataset_id_citations.append(citation_dict)\n",
    "                else:\n",
    "                    unique_dataset_id_citations.append(citation_dict)\n",
    "        article_dict['other_dataset_citations'] = unique_dataset_id_citations\n",
    "    \n",
    "    return article_dict \n",
    "\n",
    "def process_unsupported_file(file_path: str, xml_type: str | None = None) -> dict:\n",
    "    return {\n",
    "        'title': f\"Unsupported file type for: {file_path}\",\n",
    "        'data_availability': \"\",\n",
    "        'other_dataset_citations': [],\n",
    "    }\n",
    "\n",
    "# Dictionary mapping file extensions to loading functions\n",
    "FILE_EXTRACTORS = {\n",
    "    '.xml': extract_xml_text,\n",
    "    '.pdf': extract_pdf_text,\n",
    "}\n",
    "\n",
    "def extract_article_dict(file_path: str) -> dict[str, str | list[dict[str, list[str] | str]]]:\n",
    "    # Get the file extension (e.g., '.xml', '.pdf')\n",
    "    file_extension = get_file_extension(file_path)\n",
    "\n",
    "    # Get the XML type if the file is an XML file\n",
    "    xml_type = get_xml_type(file_path)\n",
    "\n",
    "    # Get the appropriate function from the dictionary,\n",
    "    # or fall back to a default 'unsupported' function if not found.\n",
    "    extract_function = FILE_EXTRACTORS.get(file_extension, process_unsupported_file)\n",
    "\n",
    "    # Call the selected function\n",
    "    article_dict = extract_function(file_path, xml_type=xml_type)\n",
    "    article_dict = dedupe_other_dataset_citations(article_dict)\n",
    "    \n",
    "    return article_dict\n",
    "\n",
    "def extract_article_text(arg: str| dict) -> str:\n",
    "    \"\"\"\n",
    "    Overloaded function: Accepts either a file_path (str) or an article_dict (dict).\n",
    "    Returns the article text as a JSON string.\n",
    "    \"\"\"\n",
    "    if isinstance(arg, dict):\n",
    "        # If it's already a dict, just serialize it\n",
    "        return json.dumps(arg, separators=(',', ':'))\n",
    "    elif isinstance(arg, str):\n",
    "        # If it's a file path, process as before\n",
    "        file_path = arg\n",
    "        file_extension = get_file_extension(file_path)\n",
    "        xml_type = get_xml_type(file_path)\n",
    "        extract_function = FILE_EXTRACTORS.get(file_extension, process_unsupported_file)\n",
    "        article_dict = extract_function(file_path, xml_type=xml_type)\n",
    "        article_dict = dedupe_other_dataset_citations(article_dict)\n",
    "        text_content = json.dumps(article_dict, separators=(',', ':'))\n",
    "        print(f\"Extracted text from {file_path}. Length: {len(text_content)} characters, xml_type: {xml_type}\")\n",
    "        return text_content\n",
    "    else:\n",
    "        raise TypeError(\"extract_article_text expects a file path (str) or article_dict (dict)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f45bff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\train\\XML\\10.3897_zoologia.35.e23481.xml. Length: 1915 characters, xml_type: taxonx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1915"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'{\"title\":\"Is the spider a good biological control agent for Plutella xylostella (Lepidoptera: Plutellidae)?\",\"author\":\"\",\"abstract\":\"Abstract Spiders, as predators of insects and other invertebrates, are an important part of the natural enemies, and they are recognized as an important biological control agent. Plutella xylostella (Linnaeus, 1758), the diamondback moth (DBM), is a well-known and destructive insect pest of brassicaceous crops worldwide. Here, we analyzed the functional responses of four spiders (Araneae) - Ebrechtella tricuspidata (Fabricius, 1775) (Thomisidae), Pardosa laura (Karsch, 1879) (Lycosidae), Pardosa astrigera (Koch, 1878) (Lycosidae), and Pardosa pseudoannulata (B\\\\u00f6senberg & Strand, 1906) (Lycosidae) - on P. xylostella larvae. We also analyzed intraspecific disturbances in the predation reaction and the intensity of scrambling competition of the spiders to P. xylostella larvae. Our results demonstrated that the functional responses of four spiders of different genera were in line with the Holling II model. Two Lycosidae spiders (P. astrigera and P. pseudoannulata) had the potential to control P. xylostella, and female and male spiders that belonged to the same species had different functional responses to P. xylostella. The functional responses of female predation of P. astrigena, P. laura, and P. pseudoannulata was stronger than the males, but male E. tricuspidatus had stronger functional responses to predation than females. We used the Hassell model to describe the intraspecific disturbance experiments of four spiders. There were intraspecific disturbances in the predation reactions of spiders, indicating that the predation ratio of spiders decreased in relation to the increase of its density, and with the increase of spider density, the intensity of scrambling competition of the spider increased.\",\"data_availability\":\"\",\"other_dataset_citations\":[]}'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test extracting text from various PDF and XML files\n",
    "pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1002_2017jc013030.pdf')\n",
    "pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1002_ece3.6784.pdf')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1002_2017jc013030.xml')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TEST_DIR, 'PDF', '10.1002_ecs2.1280.pdf')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1017_rdc.2022.19.pdf')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1017_s0007123423000601.pdf')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.3389_fcimb.2024.1292467.pdf')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1002_esp.5058.pdf') # This one is big\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1002_esp.5059.pdf') # This one is big\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1002_ece3.4466.pdf') # dryad\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1002_ece3.4466.xml') # dryad\n",
    "pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1002_mp.14424.xml')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1007_s00259-022-06053-8.xml')    # jats\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1007_s00382-022-06361-7.xml')    # tei\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1111_1365-2435.13431.xml')       # wiley\n",
    "pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1111_mec.16977.xml')             # bioc\n",
    "pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.3897_zoologia.35.e23481.xml')      # taxonx\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1002_ece3.6144.xml')               # jats\n",
    "article_text = extract_article_text(pdf_file_path)\n",
    "display(len(article_text))\n",
    "article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c1d655b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: No data availability or other dataset citations found in ./kaggle/input/make-data-count-finding-data-references\\train\\XML\\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'Unsupported file type for: ./kaggle/input/make-data-count-finding-data-references\\\\train\\\\XML\\\\',\n",
       " 'data_availability': '',\n",
       " 'other_dataset_citations': []}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_dict = extract_article_dict(pdf_file_path)\n",
    "\n",
    "if not article_dict['data_availability'] and not article_dict['other_dataset_citations']:\n",
    "    print(f\"DEBUG: No data availability or other dataset citations found in {pdf_file_path}\")\n",
    "\n",
    "article_dict    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dbdf50",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "29d7f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labeled training data from: ./kaggle/input/make-data-count-finding-data-references\\train_labels.csv\n",
      "Training labels shape: (1028, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "09bffd3b-c24a-470a-ae40-34d472f93718",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "https://doi.org/10.17882/49388",
         "Primary"
        ],
        [
         "1",
         "10.1002_anie.201916483",
         "Missing",
         "Missing"
        ],
        [
         "2",
         "10.1002_anie.202005531",
         "Missing",
         "Missing"
        ],
        [
         "3",
         "10.1002_anie.202007717",
         "Missing",
         "Missing"
        ],
        [
         "4",
         "10.1002_chem.201902131",
         "Missing",
         "Missing"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/49388</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1002_anie.202007717</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1002_chem.201902131</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                      dataset_id     type\n",
       "0    10.1002_2017jc013030  https://doi.org/10.17882/49388  Primary\n",
       "1  10.1002_anie.201916483                         Missing  Missing\n",
       "2  10.1002_anie.202005531                         Missing  Missing\n",
       "3  10.1002_anie.202007717                         Missing  Missing\n",
       "4  10.1002_chem.201902131                         Missing  Missing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the labeled training data CSV file\n",
    "print(f\"Loading labeled training data from: {LABELED_TRAINING_DATA_CSV_PATH}\")\n",
    "train_labels_df = pd.read_csv(LABELED_TRAINING_DATA_CSV_PATH)\n",
    "\n",
    "print(f\"Training labels shape: {train_labels_df.shape}\")\n",
    "display(train_labels_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "560a47e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example grouped training data for article_id '10.1002_2017jc013030': [{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]\n"
     ]
    }
   ],
   "source": [
    "# Group training data by article_id to get all datasets for each article\n",
    "# This creates a dictionary where keys are article_ids and values are lists of dataset dicts\n",
    "grouped_training_data = {}\n",
    "for article_id, group_df in train_labels_df.groupby('article_id'):\n",
    "    grouped_training_data[article_id] = group_df[['dataset_id', 'type']].to_dict('records')\n",
    "\n",
    "# Example usage of grouped_training_data\n",
    "print(f\"Example grouped training data for article_id '10.1002_2017jc013030': {grouped_training_data['10.1002_2017jc013030']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5d2b7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base file dir for the articles to be processed\n",
    "base_file_dir = ARTICLE_TEST_DIR \\\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN') \\\n",
    "    else ARTICLE_TRAIN_DIR\n",
    "\n",
    "# Just for testing, always set to the ARTICLE_TEST_DIR\n",
    "base_file_dir = ARTICLE_TEST_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "34f61a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files paths shape: (30, 6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_info",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "xml_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "5a4402a1-a90b-446b-84ce-7e4432929c42",
       "rows": [
        [
         "21",
         "10.1002_ecs2.4619",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.4619.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ecs2.4619.xml",
         "test",
         "[{'dataset_id': 'https://doi.org/10.25349/d9qw5x', 'type': 'Primary'}]",
         "tei"
        ],
        [
         "8",
         "10.1002_chem.202001668",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202001668.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.202001668.xml",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "jats"
        ],
        [
         "0",
         "10.1002_2017jc013030",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_2017jc013030.xml",
         "test",
         "[{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]",
         "tei"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>dataset_info</th>\n",
       "      <th>xml_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.1002_ecs2.4619</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.25349/d9qw...</td>\n",
       "      <td>tei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.1002_chem.202001668</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>jats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.17882/4938...</td>\n",
       "      <td>tei</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                article_id                                      pdf_file_path  \\\n",
       "21       10.1002_ecs2.4619  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "8   10.1002_chem.202001668  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "0     10.1002_2017jc013030  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                        xml_file_path dataset_type  \\\n",
       "21  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "8   ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "0   ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "\n",
       "                                         dataset_info xml_type  \n",
       "21  [{'dataset_id': 'https://doi.org/10.25349/d9qw...      tei  \n",
       "8      [{'dataset_id': 'Missing', 'type': 'Missing'}]     jats  \n",
       "0   [{'dataset_id': 'https://doi.org/10.17882/4938...      tei  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load file paths for training and testing datasets\n",
    "file_paths_df = load_file_paths(base_file_dir)\n",
    "file_paths_df['xml_file_path'] = file_paths_df['xml_file_path'].fillna('')\n",
    "\n",
    "# Merge the file paths with the grouped_training_data\n",
    "file_paths_df['dataset_info'] = file_paths_df['article_id'].map(grouped_training_data)\n",
    "\n",
    "# Get the xml type for each file based on the first line of the XML file\n",
    "file_paths_df['xml_type'] = file_paths_df['xml_file_path'].apply(get_xml_type)\n",
    "\n",
    "print(f\"Files paths shape: {file_paths_df.shape}\")\n",
    "display(file_paths_df.sample(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e30a9aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf. Length: 5502 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.201916483.pdf. Length: 1325 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202005531.pdf. Length: 1609 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202007717.pdf. Length: 1697 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.201902131.pdf. Length: 647 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.201903120.pdf. Length: 871 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202000235.pdf. Length: 683 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202001412.pdf. Length: 969 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202001668.pdf. Length: 425 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202003167.pdf. Length: 912 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_cssc.202201821.pdf. Length: 3836 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.3985.pdf. Length: 3479 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.4466.pdf. Length: 2181 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.5260.pdf. Length: 2315 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.5395.pdf. Length: 3453 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6144.pdf. Length: 2857 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6303.pdf. Length: 3148 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6784.pdf. Length: 6586 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.961.pdf. Length: 2736 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.9627.pdf. Length: 2699 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.1280.pdf. Length: 3250 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.4619.pdf. Length: 2871 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejic.201900904.pdf. Length: 617 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejoc.202000139.pdf. Length: 895 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejoc.202000916.pdf. Length: 297 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_esp.5058.pdf. Length: 4419 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_esp.5090.pdf. Length: 5250 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_mp.14424.pdf. Length: 4423 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_nafm.10870.pdf. Length: 4930 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1007_jhep07(2018)134.pdf. Length: 873 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_2017jc013030.xml. Length: 1426 characters, xml_type: tei\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.201916483.xml. Length: 993 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.202005531.xml. Length: 233 characters, xml_type: bioc\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.202007717.xml. Length: 1369 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.201902131.xml. Length: 1181 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.201903120.xml. Length: 1815 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.202000235.xml. Length: 757 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.202001412.xml. Length: 1087 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.202001668.xml. Length: 811 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.202003167.xml. Length: 1816 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_cssc.202201821.xml. Length: 1532 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.3985.xml. Length: 1644 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.4466.xml. Length: 1747 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.5260.xml. Length: 2461 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.5395.xml. Length: 2331 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.6144.xml. Length: 2164 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.6303.xml. Length: 2529 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.6784.xml. Length: 2725 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.961.xml. Length: 1699 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.9627.xml. Length: 2405 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ecs2.4619.xml. Length: 1990 characters, xml_type: tei\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ejic.201900904.xml. Length: 971 characters, xml_type: wiley\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ejoc.202000916.xml. Length: 570 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_esp.5090.xml. Length: 2586 characters, xml_type: wiley\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_mp.14424.xml. Length: 2704 characters, xml_type: jats\n",
      "File paths DataFrame shape: (30, 8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_info",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "xml_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "3099408c-bb47-4923-8974-0a427c1033b0",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_2017jc013030.xml",
         "test",
         "[{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]",
         "tei",
         "{\"title\":\"\",\"author\":\"Marie Barbieux\",\"abstract\":\"Abstract Characterizing phytoplankton distribution and dynamics in the world\\u2019s open oceans requires in situ observations over a broad range of space and time scales. In addition to temperature/salinity measure-ments, Biogeochemical-Argo (BGC-Argo) profiling floats are capable of autonomously observing at high-frequency bio-optical properties such as the chlorophyll fluorescence, a proxy of the chlorophyll a concen-tration (Chla), the particulate backscattering coefficient (bbp), a proxy of the stock of particulate organic car-bon, and the light available for photosynthesis. We analyzed an unprecedented BGC-Argo database of more than 8,500 multivariable profiles collected in various oceanic conditions, from subpolar waters to subtropical gyres. Our objective is to refine previously established Chla versus bbp relationships and gain insights into the sources of vertical, seasonal, and regional variability in this relationship. Despite some regional, seasonal and vertical variations, a general covariation occurs at a global scale. We distinguish two main contrasted situations: (1) concomitant changes in Chla and bbp that correspond to actual variations in phytoplankton biomass, e.g., in subpolar regimes; (2) a decoupling between the two variables attributed to photoacclima-tion or changes in the relative abundance of nonalgal particles, e.g., in subtropical regimes. The variability in the bbp:Chla ratio in the surface layer appears to be essentially influenced by the type of particles and by photoacclimation processes. The large BGC-Argo database helps identifying the spatial and temporal scales at which this ratio is predominantly driven by one or the other of these two factors.\",\"data_availability\":\"Acknowledgments This paper represents a contribution to the following research projects: remOcean (funded by the European Research Council, grant 246777), NAOS (funded by the Agence Nationale de la Recherche in the frame of the French \\u2018\\u2018Equipement d\\u2019avenir\\u2019\\u2019 program, grant ANR J11R107-F), the SOCLIM (Southern Ocean and climate) project supported by the French research program LEFE-CYBER of INSU-CNRS, the Climate Initiative of the foundation BNP Paribas and the French polar institute (IPEV), AtlantOS (funded by the European Union\\u2019s Horizon 2020 Research and Innovation program, grant 2014- 633211), E-AIMS (funded by the European Commission\\u2019s FP7 project, grant 312642), U.K. Bio-Argo (funded by the British Natural Environment Research Council-NERC, grant NE/L012855/1), REOPTIMIZE (funded by the European Union\\u2019s Horizon 2020 Research and Innovation program, Marie Sk\\u0142odowska-Curie grant 706781), Argo-Italy (funded by the Italian Ministry of Education, University and Research -MIUR), and the French Bio-Argo program (BGC-Argo France; funded by CNES-TOSCA, LEFE Cyber, and GMMC). We thank the PIs of several BGC-Argo floats missions and projects: Giorgio Dall\\u2019Olmo (Plymouth Marine Laboratory, United Kingdom; E-AIMS and U.K. Bio-Argo); Kjell-Arne Mork (Institute of Marine Research, Norway; E-AIMS); Violeta Slabakova (Bulgarian Academy of Sciences, Bulgaria; E-AIMS); Emil Stanev (University of Oldenburg, Germany; E-AIMS); Claire Lo Monaco (Laboratoire d\\u2019Oc\\ufffdeanographie et du Climat: Exp\\ufffderimentations et Approches Num\\ufffderiques); Pierre-Marie Poulain (National Institute of Oceanography and Experimental Geophysics, Italy; Argo-Italy); Sabrina Speich (Laboratoire de M\\ufffdet\\ufffdeorologie Dynamique, France; LEFE-GMMC); Virginie Thierry (Ifremer, France; LEFE-GMMC); Pascal Conan (Observatoire Oc\\ufffdeanologique de Banyuls sur mer, France; LEFE-GMMC); Laurent Coppola (Laboratoire d\\u2019Oc\\ufffdeanographie de Villefranche, France; LEFE-GMMC); Anne Petrenko (Mediterranean Institute of Oceanography, France; LEFE-GMMC); and Jean-Baptiste Sall\\ufffdee (Laboratoire d\\u2019Oc\\ufffdeanographie et du Climat, France; LEFE-GMMC). Collin Roesler (Bowdoin College, USA) and Yannick Huot (University of Sherbrooke, Canada) are acknowledged for useful comments and fruitful discussion. We also thank the International Argo Program and the CORIOLIS project that contribute to make the data freely and publicly available. Data referring to Organelli et al. (2016a; https://doi.org/10.17882/47142) and Barbieux et al. (2017; https://doi.org/10.17882/49388) are freely available on SEANOE.\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.1002/2017JC013030\"],\"citation_context\":\"Citation: Barbieux, M., Uitz, J., Bricaud, A., Organelli, E., Poteau, A., Schmechtig, C., . . . Claustre, H. (2018). Assessing the variability in the relationship between the particulate backscattering coefficient and the chlorophyll a concentration from a global Biogeochemical-Argo database. Journal of Geophysical Research: Oceans, 123, 1229-1250. https://doi. org/10.1002/2017JC013030\"},{\"dataset_ids\":[\"10.1016/j.rse.\"],\"citation_context\":\", Alvain, S., Vantrepotte, V., & Huertas, I. E. (2014). Identification of dominant phytoplankton functional types in the Mediterra-nean Sea based on a regionalized remote sensing approach. Remote Sensing of Environment, 152, 557-575. https://doi.org/10.1016/j.rse. 2014.06.029 Organelli, E., Barbieux, M., Claustre, H., Schmechtig, C., Poteau, A., Bricaud, A., . . . Dall\\u2019Olmo, G. (2016a). A global bio-optical database derived from Biogeochemical Argo float measurements within the layer of interest for field and\"}]}",
         "{\"title\":\"Assessing the variability in the relationship between the particulate backscattering coefficient and the chlorophyll a concentration from a global Biogeochemical-Argo database\",\"author\":\"Marie Barbieux\",\"abstract\":\"The particulate backscattering coefficient vs chlorophyll a concentration relationship varies along the water column, according to seasons and oceanic regions. -The b bp -to-Chla ratio is a valuable biogeochemical proxy for assessing the nature of the particulate assemblage and revealing photoacclimation processes. -The BGC-Argo float network yields an unprecedented amount of quality data for studying biogeochemical processes at a global scale and along the vertical dimension.\",\"data_availability\":\"\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.17882/49388\"],\"citation_context\":\"A global database of vertical profiles derived from Biogeochemical Argo float measurements for biogeochemical and bio-optical applications|SEANOE|10.17882/49388\"},{\"dataset_ids\":[\"10.17882/47142\"],\"citation_context\":\"A global bio-optical database derived from Biogeochemical Argo float measurements within the layer of interest for field and remote ocean colour applications|SEANOE|10.17882/47142\"},{\"dataset_ids\":[\"10.5194/essd-2017-58\"],\"citation_context\":\"Two databases derived from BGC-Argo float measurements for marine biogeochemical and biooptical applications|Earth Syst. Sci. Data|10.5194/essd-2017-58\"}]}"
        ],
        [
         "1",
         "10.1002_anie.201916483",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.201916483.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.201916483.xml",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "jats",
         "{\"title\":\"\",\"author\":\"Daniel Werner\",\"abstract\":\"Abstract: The homoleptic pyrazolate complexes [CeIII 4-(Me2pz)12] and [CeIV(Me2pz)4]2 quantitatively insert CO2 to give [CeIII 4(Me2pz\\u00b7CO2)12] and [CeIV(Me2pz\\u00b7CO2)4], respec-tively (Me2pz = 3,5-dimethylpyrazolato). This process is rever-sible for both complexes, as observed by in situ IR and NMR spectroscopy in solution and by TGA in the solid state. By adjusting the molar ratio, one molecule of CO2 per [CeIV-(Me2pz)4] complex could be inserted to give trimetallic [Ce3-(Me2pz)9(Me2pz\\u00b7CO2)3(thf)]. Both the cerous and ceric in-sertion products catalyze the formation of cyclic carbonates from epoxides and CO2 under mild conditions. In the absence of epoxide, the ceric catalyst is prone to reduction by the co-catalyst tetra-n-butylammonium bromide (TBAB).\",\"data_availability\":\"\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.1002/anie.201916483\"],\"citation_context\":\"15213773, 2020, 14, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/anie.201916483 by California Digital Library University Of California, Wiley Online Library on [09/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"}]}",
         "{\"title\":\"Effective and Reversible Carbon Dioxide Insertion into Cerium Pyrazolates\",\"author\":\"Bayer Uwe\",\"abstract\":\"Abstract The homoleptic pyrazolate complexes [Ce III 4 (Me 2 pz) 12 ] and [Ce IV (Me 2 pz) 4 ] 2 quantitatively insert CO 2 to give [Ce III 4 (Me 2 pz\\u22c5CO 2 ) 12 ] and [Ce IV (Me 2 pz\\u22c5CO 2 ) 4 ], respectively (Me 2 pz=3,5-dimethylpyrazolato). This process is reversible for both complexes, as observed by in situ IR and NMR spectroscopy in solution and by TGA in the solid state. By adjusting the molar ratio, one molecule of CO 2 per [Ce IV (Me 2 pz) 4 ] complex could be inserted to give trimetallic [Ce 3 (Me 2 pz) 9 (Me 2 pz\\u22c5CO 2 ) 3 (thf)]. Both the cerous and ceric insertion products catalyze the formation of cyclic carbonates from epoxides and CO 2 under mild conditions. In the absence of epoxide, the ceric catalyst is prone to reduction by the co-catalyst tetra- n -butylammonium bromide (TBAB).\",\"data_availability\":\"\",\"other_dataset_citations\":[]}"
        ],
        [
         "2",
         "10.1002_anie.202005531",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202005531.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.202005531.xml",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "bioc",
         "{\"title\":\"\",\"author\":\"Dong Wang\",\"abstract\":\"Abstract: Trifluoromethyl sulfoxides are a new class of trifluoromethylthiolating reagent. The sulfoxides engage in metal-free C\\ufffdH trifluoromethylthiolation with a range of (hetero)arenes. The method is also applicable to the function-alization of important compound classes, such as ligand derivatives and polyaromatics, and in the late-stage trifluor-omethylthiolation of medicines and agrochemicals. The iso-lation and characterization of a sulfonium salt intermediate supports an interrupted Pummerer reaction mechanism. Incorporating fluorine into organic compounds is a useful tool in drug design and development. The fluoro group is well known to improve the pharmacokinetic properties of a mol-ecule and fluorine-18 is an important radioisotope in molec-ular imaging.[1,2] Trifluoromethylthio (SCF3) groups are commonly found in drug molecules and veterinary medi-cines.[3,4] By combining a fluorinated moiety with a hetero-atom, many have turned to the SCF3 group to impart useful properties, such as high lipophilicity, to a compound of interest.[5]\",\"data_availability\":\"\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.1002/anie.202005531\"],\"citation_context\":\"15213773, 2020, 37, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/anie.202005531 by California Digital Library University Of California, Wiley Online Library on [09/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"}]}",
         "{\"title\":\"Trifluoromethyl Sulfoxides: Reagents for Metal\\u00e2\\u0080\\u0090Free C\\u00e2\\u0088\\u0092H Trifluoromethylthiolation\",\"author\":\"Wang Dong;prefix:Dr.\",\"abstract\":\"Abstract\",\"data_availability\":\"\",\"other_dataset_citations\":[]}"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>dataset_info</th>\n",
       "      <th>xml_type</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>xml_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.17882/4938...</td>\n",
       "      <td>tei</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Marie Barbieux\",\"abstrac...</td>\n",
       "      <td>{\"title\":\"Assessing the variability in the rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>jats</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Daniel Werner\",\"abstract...</td>\n",
       "      <td>{\"title\":\"Effective and Reversible Carbon Diox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>bioc</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Dong Wang\",\"abstract\":\"A...</td>\n",
       "      <td>{\"title\":\"Trifluoromethyl Sulfoxides: Reagents...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                                      pdf_file_path  \\\n",
       "0    10.1002_2017jc013030  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "1  10.1002_anie.201916483  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "2  10.1002_anie.202005531  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                       xml_file_path dataset_type  \\\n",
       "0  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "1  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "2  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "\n",
       "                                        dataset_info xml_type  \\\n",
       "0  [{'dataset_id': 'https://doi.org/10.17882/4938...      tei   \n",
       "1     [{'dataset_id': 'Missing', 'type': 'Missing'}]     jats   \n",
       "2     [{'dataset_id': 'Missing', 'type': 'Missing'}]     bioc   \n",
       "\n",
       "                                            pdf_text  \\\n",
       "0  {\"title\":\"\",\"author\":\"Marie Barbieux\",\"abstrac...   \n",
       "1  {\"title\":\"\",\"author\":\"Daniel Werner\",\"abstract...   \n",
       "2  {\"title\":\"\",\"author\":\"Dong Wang\",\"abstract\":\"A...   \n",
       "\n",
       "                                            xml_text  \n",
       "0  {\"title\":\"Assessing the variability in the rel...  \n",
       "1  {\"title\":\"Effective and Reversible Carbon Diox...  \n",
       "2  {\"title\":\"Trifluoromethyl Sulfoxides: Reagents...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the PDF text content for each article\n",
    "file_paths_df['pdf_text'] = file_paths_df['pdf_file_path'].apply(lambda x: extract_article_text(x) if x else \"\")\n",
    "# Load the XML text content for each article\n",
    "file_paths_df['xml_text'] = file_paths_df['xml_file_path'].apply(lambda x: extract_article_text(x) if x else \"\")\n",
    "# Fill NaN values in the 'xml_type' and 'xml_text' columns with empty strings\n",
    "file_paths_df['xml_file_path'] = file_paths_df['xml_file_path'].fillna('')\n",
    "file_paths_df['xml_type'] = file_paths_df['xml_type'].fillna('')\n",
    "file_paths_df['xml_text'] = file_paths_df['xml_text'].fillna('')\n",
    "# Display the first few rows of the training file paths DataFrame\n",
    "print(f\"File paths DataFrame shape: {file_paths_df.shape}\")\n",
    "display(file_paths_df.head(3))\n",
    "# Save the file paths DataFrame to CSV files\n",
    "file_paths_df.to_csv(os.path.join(BASE_OUTPUT_DIR, 'file_paths.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5033e1",
   "metadata": {},
   "source": [
    "## Define the Qwen evaluation model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2640c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QwenModelEval Class ---\n",
    "# kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "#max_new_tokens=32768\n",
    "class QwenModelEval:\n",
    "    def __init__(self, model_name, sys_prompt, enable_thinking=True, max_new_tokens=1024, max_input_length=8200):\n",
    "        print(f\"Loading Qwen model and tokenizer from: {model_name}\")\n",
    "        self.model_name = model_name\n",
    "        self.sys_prompt = sys_prompt\n",
    "        self.enable_thinking = enable_thinking  # Enable or disable thinking mode\n",
    "        self.max_new_tokens = max_new_tokens  # Set the maximum number of new tokens to generate\n",
    "        self.max_input_length = max_input_length  # Set the maximum input length for the model\n",
    "        # Load the tokenizer and model\n",
    "        # Using trust_remote_code=True to allow custom model code execution\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\", trust_remote_code=True)\n",
    "        self.model.eval() # Set the model to evaluation mode here.\n",
    "\n",
    "    def generate_response(self, user_input):  \n",
    "        inputs = self._get_inputs(user_input)\n",
    "        # Disable gradient calculation during inference\n",
    "        # Generate the response using the model\n",
    "        with torch.no_grad(): \n",
    "            generated_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.convert_tokens_to_ids(\"<|im_end|>\"),\n",
    "                do_sample=False, # <--- Set to False for deterministic output\n",
    "                num_beams=1,     # <--- Keep at 1 for greedy decoding\n",
    "                # temperature, top_p, top_k, min_p will have no effect when do_sample=False\n",
    "                # temperature=0.6 if self.enable_thinking else 0.7,\n",
    "                # top_p=0.95 if self.enable_thinking else 0.8,\n",
    "                # top_k=20,\n",
    "                # min_p=0,\n",
    "            )\n",
    "        # Parse the response and thinking content\n",
    "        return self._parse_response(inputs, generated_ids)\n",
    "\n",
    "    def _get_inputs(self, user_input):\n",
    "        \"\"\"Prepare the input for the model based on user input.\"\"\"\n",
    "        # Trim the user input to a maximum length for better performance\n",
    "        user_input = user_input[:self.max_input_length]  # Limit input length to 4096 characters\n",
    "        print(f\"Preparing input with length: {len(user_input)}\")\n",
    "        # Create the messages for the chat template\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=self.enable_thinking\n",
    "        )\n",
    "        return self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "    \n",
    "    def _parse_response(self, inputs, generated_ids):\n",
    "        # Extract the output IDs from the generated IDs\n",
    "        output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist()\n",
    "        try:\n",
    "            index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        thinking_content = self.tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "        raw_response = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "        response = self._parse_json(raw_response)\n",
    "        print(f\"Parsed response: {response}\")\n",
    "        return response, thinking_content\n",
    "    \n",
    "    def _parse_json(self, raw_response: str) -> list[dict[str,str]]:\n",
    "        # Remove code block markers and leading/trailing whitespace\n",
    "        cleaned = raw_response.strip()\n",
    "        if cleaned.startswith(\"```json\"):\n",
    "            cleaned = cleaned[len(\"```json\"):].strip()\n",
    "        if cleaned.endswith(\"```\"):\n",
    "            cleaned = cleaned[:-3].strip()\n",
    "\n",
    "        # Now parse as JSON\n",
    "        try:\n",
    "            return json.loads(cleaned)\n",
    "        except json.JSONDecodeError as e:\n",
    "            return []        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2dee95",
   "metadata": {},
   "source": [
    "## Define the System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "74e5a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the one-shot reasoning and task prompt\n",
    "# This prompt is designed to guide the model through a structured reasoning process\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are an advanced AI research assistant specialized in identifying and classifying datasets used within academic research papers.\n",
    "Your primary goal is to accurately extract and categorize dataset identifiers (dataset_ids) from provided paper sections.\n",
    "\n",
    "---\n",
    "\n",
    "### Input Data Structure\n",
    "\n",
    "You will receive a JSON string representing key sections of an academic paper, structured as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"title\": \"Title of the paper\",\n",
    "    \"author\": \"The primary author of the paper, e.g., 'Author A'\",\n",
    "    \"abstract\": \"Abstract of the paper\",\n",
    "    \"data_availability\": \"Data availability information\",\n",
    "    \"other_dataset_citations\": [\n",
    "        {\"dataset_ids\": [\"10.12345/12345\"], \"citation_context\": \"Dataset citation context 1\"},\n",
    "        {\"dataset_ids\": [\"10.1234/xxxx.1x1x-xx11\", \"EPI_ISL_12345678\"], \"citation_context\": \"Dataset citation context 2\"},\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Guidance on Input Sections:**\n",
    "*   **`title`**: Provides general context for the paper's topic.\n",
    "*   **`author`**: The primary author of the paper. This is crucial for determining if the dataset's *raw data* was *originally generated by this author*.\n",
    "*   **`abstract`**: **CRITICAL** for understanding the research scope and, most importantly, for determining if a dataset's *raw data* was *originally generated by the authors of *this* paper*.\n",
    "*   **`data_availability`**: This section provides information on datasets. Its content must be evaluated to determine if the data was *originally generated by the authors of this paper* (Primary) or *acquired from an existing source* (Secondary).\n",
    "*   **`other_dataset_citations`**: A list of potential dataset citations. The `citation_context` is vital to confirm if a `dataset_id` truly refers to a dataset and to aid in classifying its origin (Primary or Secondary).\n",
    "\n",
    "---\n",
    "\n",
    "### Core Objective & Critical Exclusion\n",
    "\n",
    "Your overarching objective is to identify and classify **only valid, data-related `dataset_id`s**.\n",
    "\n",
    "**CRITICAL EXCLUSION**: You **MUST NOT** extract any `dataset_id`s that refer to other academic papers, articles, or the paper itself. Focus strictly on identifiers for *datasets* only found within the `abstract`, `data_availability` and `other_dataset_citations` sections. DO NOT make up any dataset_ids.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Definitions\n",
    "\n",
    "*   **`dataset_id`**: A unique, persistent identifier for a dataset. There are two main types:\n",
    "\n",
    "    1.  **Digital Object Identifier (DOI)**:\n",
    "        *   **Format**: `[prefix]/[suffix]`. The prefix always starts with \"10.\" and is followed by a 4 or 5 digit number. The suffix can contain letters, numbers, and special characters.\n",
    "        *   DO NOT look for Accession IDs within the suffix of a DOI.\n",
    "        *   May or may not start with \"https://doi.org/\" or \"doi:\".\n",
    "        *   **IMPORTANT DOI VALIDATION RULE**:\n",
    "            *   Only identify DOIs that are explicitly used as `dataset_id`s.\n",
    "            *   **DO NOT extract DOIs for academic papers/articles.**\n",
    "            *   **If a DOI is presented as a reference to a publication (e.g., \"as described in [DOI]\", \"cited in [DOI]\", \"see [DOI] for details on the method\"), it is NOT a dataset_id.**\n",
    "            *   A DOI is a `dataset_id` ONLY if the surrounding `citation_context` or `data_availability` section clearly indicates it refers to a dataset, data repository, data archive, or similar data-specific entity.\n",
    "\n",
    "    2.  **Accession ID**:\n",
    "        *   DO NOT look for Accession IDs within a DOI. If a dataset_id has a DOI format, no portion of the DOI should be identified as an Accession ID.\n",
    "        *   They always start with two or more alpha characters, including underscores (\"_\"), followed by three or more digits.\n",
    "        *   These identifiers are often used in biological databases, chemical databases, or other scientific data repositories.\n",
    "        *   Can be found in both `data_availability` and `other_dataset_citations` sections.\n",
    "        *   *Examples*: `\"EPI_ISL_12345678\"` (e.g., a genomic sequence dataset), `\"IPR000264\"` (e.g., a protein family identifier), `\"SAMN07159041\"` (e.g., a biological sample record), `\"CHEMBL1782574\"` (e.g., a chemical compound entry)\n",
    "\n",
    "*   **Dataset Type Classification**: **This is the MOST CRITICAL distinction. Focus ONLY on the *ORIGIN* of the *SPECIFIC DATA PRODUCT* that the `dataset_id` refers to.**\n",
    "\n",
    "    *   **Primary**: **New data** (e.g., measurements, annotations, segmentations, etc.) that was **ORIGINALLY GENERATED, COLLECTED, PROCESSED, or CREATED by the AUTHORS OF *THIS SPECIFIC PAPER*** as a direct output of their research. This data is central to the new findings presented in this paper.\n",
    "        *   *Keywords often associated with Primary*: \"generated\", \"sequenced\", \"collected\", \"created\", \"produced\", \"developed\", \"annotated\".\n",
    "        *   **CRITICAL CLARIFICATION**: If a `dataset_id` refers to a novel data product (like annotations or segmentations) that was *created by the authors* (Primary) but was *derived from* or *applied to* existing, external data (Secondary input), the `dataset_id` for the *novel data product* is still **Primary**. The act of making this novel data product publicly available does not change its origin.\n",
    "        *   *Example*: If the paper describes *new* patient scans or *new* experimental results that *these authors* produced, and then provides a DOI for *those new scans/results*, that's Primary.\n",
    "\n",
    "    *   **Secondary**: Existing data that was **ACQUIRED, DERIVED, USED, REUSED, or RE-ANALYZED from EXISTING RECORDS or PREVIOUSLY PUBLISHED DATASETS** and that was *not originally generated by the authors of this specific paper* or *not directly related to the research of this paper*.\n",
    "        *   *Keywords often associated with Secondary*: \"previously published\", \"existing\", \"external\", \"re-analyzed\", \"obtained from\", \"acquired from\", \"derived from\", \"sourced from\".\n",
    "        *   **IMPORTANT CLARIFICATION**: If the `citation_context` associated with the `dataset_id` refers to one or more people (\"[People], et. al.\") and the first person is NOT the author of this article, then it is **Secondary**.\n",
    "        *   *Example*: If the `citation_context` contains text similar to \"Aerts H, Wee L, Velazquez E*, et al.*\", it is referring to people associated with the dataset. If the first person listed, \"Aerts H\" in this example, is NOT the author of this article, then the `dataset_id` is **Secondary**. If the first person IS the author of this article, then it is **Primary**.\n",
    "\n",
    "---\n",
    "\n",
    "### Classification Logic Flow (for each identified `dataset_id`):\n",
    "\n",
    "To classify a `dataset_id` as Primary or Secondary, follow these steps strictly:\n",
    "\n",
    "1.  **For the specific `dataset_id` being evaluated, examine the `abstract` and associated `citation_context` to:**\n",
    "    *   Identify the *data product* it refers to. (e.g., \"images\", \"scans\", \"segmentations\", \"genomic sequences\", \"raw data\").\n",
    "    *   Determine if the *specific data product* is directly related to and created for the research topic of this paper.\n",
    "    *   Determine if the associated `citation_context` refers to specific people (e.g., \"Aerts H, Wee L, Velazquez E*, et al.*\"). If it does, check if the first person listed, \"Aerts H\" in this example, is the author of this article.\n",
    "2.  **STEP 1: Check for Primary Indicators (Original Generation of the *Specific Data Product* by Authors):**\n",
    "    *   Does the text indicate that this *specific data product* is *directly related to* and *created for* the research topic of this paper?\n",
    "        *   **IF YES**: Classify as **Primary**.\n",
    "        *   **REMEMBER**: Even if this *Primary* data product was derived from *Secondary* input data, it is still **Primary** because the *data product itself* is a novel creation of these authors.\n",
    "3.  **STEP 2: Check for Secondary Indicators (Acquisition/Reuse of the *Specific Data Product* from External Source):**\n",
    "    *   Does the text indicate that this *specific data product* is an existing, external dataset that was *USED* in the research of this paper OR (if you can determine) that the dataset was not created by the author of this paper*?\n",
    "        *   **IF YES**: Classify as **Secondary**.\n",
    "        *   *Example*: If the abstract states \"scans **acquired from The Cancer Imaging Archive 'NSCLC Radiomics' data collection**\". This indicates the raw CT scans were acquired from an external source, making this **Secondary**.\n",
    "4.  **STEP 3: Fallback Rule:**\n",
    "    *   If, after applying Step 1 and Step 2, the origin of the *specific data product* remains truly ambiguous, then default to \"Primary\".\n",
    "\n",
    "---\n",
    "\n",
    "### Tasks: Step-by-Step Instructions\n",
    "\n",
    "Follow these three tasks in order:\n",
    "\n",
    "**SHORT-CIRCUIT RULE:**\n",
    "**IF** the `data_availability` section is an empty string (`\"\"`) **AND** the `other_dataset_citations` section is an empty list (`[]`), **THEN** immediately **skip all other tasks** and proceed directly to **Task 3** to output the \"Missing\" JSON structure. Do not perform any further analysis or reasoning.\n",
    "\n",
    "**Task 1: Identify Valid Dataset IDs**\n",
    "\n",
    "1.  **Search Priority**: Begin by searching the `data_availability` section if it is NOT an empty string (`\"\"`). Then, search the `other_dataset_citations` section if it is NOT an empty list (`[]`).\n",
    "2.  **Validation**: For each potential `dataset_id` (DOI or Accession ID), confirm it is truly data-related.\n",
    "    *   **For DOIs**: Strictly apply the **IMPORTANT DOI VALIDATION RULE** defined above. If it refers to a publication, **DO NOT** extract it.\n",
    "    *   **For all IDs**: Look for surrounding terms like \"data release\", \"data availability\", \"dataset\", \"database\", \"repository\", \"data source\", \"data access\", or \"data archive\" within the `data_availability` section or the `citation_context`.\n",
    "3.  **Deduplication**: If the same `dataset_id` is found multiple times, **only process the first instance encountered**.\n",
    "4.  **Conditional Proceeding**:\n",
    "    *   If **no valid `dataset_id`s are found** after searching both sections, **skip directly to Task 3** and output the \"Missing\" JSON structure.\n",
    "    *   If one or more valid `dataset_id`s are found, proceed to Task 2.\n",
    "\n",
    "**Task 2: Classify Dataset Types**\n",
    "\n",
    "1.  For each valid `dataset_id` identified in Task 1, classify its type as either \"Primary\" or \"Secondary\".\n",
    "2.  **STRICTLY APPLY THE \"CLASSIFICATION LOGIC FLOW\" ABOVE for each `dataset_id`.** Use the `author` section, the `abstract` section, and the `citation_context` to determine if the *specific data product* associated with the `dataset_id` was *originally generated by the authors of *this* paper* (Primary) or *acquired/reused from an existing source* (Secondary).\n",
    "3.  Apply the \"Key Definitions\" for Primary and Secondary types, paying close attention to the associated keywords and the provided examples.\n",
    "4.  Remember the \"Fallback Rule\": Default to \"Primary\" if the classification remains truly ambiguous regarding the *original generation* of the raw data.\n",
    "\n",
    "**Task 3: Format and Return Results**\n",
    "\n",
    "Return your final results as a JSON array of objects.\n",
    "\n",
    "1.  **Scenario A: No Valid Datasets Found**\n",
    "    If Task 1 resulted in no valid `dataset_id`s, return a single JSON object with the following structure:\n",
    "    ```json\n",
    "    [\n",
    "        {\n",
    "            \"dataset_id\": \"Missing\",\n",
    "            \"type\": \"Missing\"\n",
    "        }\n",
    "    ]\n",
    "    ```\n",
    "2.  **Scenario B: One or More Valid Datasets Found**\n",
    "    If Task 1 identified one or more valid `dataset_id`s, return every valid dataset found in a JSON array of objects, where each object has the following structure:\n",
    "    ```json\n",
    "    [\n",
    "        {\n",
    "            \"dataset_id\": \"example_id_1\",\n",
    "            \"type\": \"Primary\"\n",
    "        },\n",
    "        {\n",
    "            \"dataset_id\": \"example_id_2\",\n",
    "            \"type\": \"Secondary\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    ```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe217a6",
   "metadata": {},
   "source": [
    "## Evaluate the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f8a8a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen model and tokenizer from: C:\\Users\\jim\\.cache\\kagglehub\\models\\qwen-lm\\qwen-3\\transformers\\0.6b\\1\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the QwenModelEval class with the model path and system prompt\n",
    "inference_model = QwenModelEval(QWEN_BASE_MODEL_PATH, sys_prompt=SYS_PROMPT, enable_thinking=True, max_new_tokens=1576)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ca3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_articles(file_paths_df: pd.DataFrame, model) -> pd.DataFrame:\n",
    "    results = []\n",
    "    # tqdm(os.listdir(pdf_directory), total=len(os.listdir(pdf_directory)))\n",
    "    for i, row in tqdm(file_paths_df.iterrows(), total=len(file_paths_df)):\n",
    "        article_id = row['article_id']\n",
    "        dataset_info = row['dataset_info']\n",
    "        pdf_text = row['pdf_text']\n",
    "        xml_text = row['xml_text']\n",
    "        text_type = \"XML\" if xml_text else \"PDF\"\n",
    "        text_content = xml_text if xml_text else pdf_text\n",
    "        response = [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
    "        thinking_content = \"\"\n",
    "\n",
    "        # Prepare the user input for the model\n",
    "        user_input = f\"Text Content: {text_content}\\n\"\n",
    "        \n",
    "        if text_content:\n",
    "            print(f\"Processing article {i}/{len(file_paths_df)}: {article_id}, type: {text_type}\")\n",
    "            # Generate response from the model\n",
    "            response, thinking_content = model.generate_response(user_input)\n",
    "\n",
    "        results.append({\n",
    "            'article_id': article_id,\n",
    "            'dataset_info': dataset_info,\n",
    "            'text_type': text_type,\n",
    "            'llm_input': user_input,\n",
    "            'llm_response': response,\n",
    "            'llm_thinking_content': thinking_content\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(by=[\"article_id\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dc937f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File paths DataFrame shape: (30, 8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_info",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "82017230-b1df-4176-9162-70064d599074",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_2017jc013030.xml",
         "test",
         "[{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]",
         "tei",
         "{\"title\":\"\",\"author\":\"Marie Barbieux\",\"abstract\":\"Abstract Characterizing phytoplankton distribution and dynamics in the world\\u2019s open oceans requires in situ observations over a broad range of space and time scales. In addition to temperature/salinity measure-ments, Biogeochemical-Argo (BGC-Argo) profiling floats are capable of autonomously observing at high-frequency bio-optical properties such as the chlorophyll fluorescence, a proxy of the chlorophyll a concen-tration (Chla), the particulate backscattering coefficient (bbp), a proxy of the stock of particulate organic car-bon, and the light available for photosynthesis. We analyzed an unprecedented BGC-Argo database of more than 8,500 multivariable profiles collected in various oceanic conditions, from subpolar waters to subtropical gyres. Our objective is to refine previously established Chla versus bbp relationships and gain insights into the sources of vertical, seasonal, and regional variability in this relationship. Despite some regional, seasonal and vertical variations, a general covariation occurs at a global scale. We distinguish two main contrasted situations: (1) concomitant changes in Chla and bbp that correspond to actual variations in phytoplankton biomass, e.g., in subpolar regimes; (2) a decoupling between the two variables attributed to photoacclima-tion or changes in the relative abundance of nonalgal particles, e.g., in subtropical regimes. The variability in the bbp:Chla ratio in the surface layer appears to be essentially influenced by the type of particles and by photoacclimation processes. The large BGC-Argo database helps identifying the spatial and temporal scales at which this ratio is predominantly driven by one or the other of these two factors.\",\"data_availability\":\"Acknowledgments This paper represents a contribution to the following research projects: remOcean (funded by the European Research Council, grant 246777), NAOS (funded by the Agence Nationale de la Recherche in the frame of the French \\u2018\\u2018Equipement d\\u2019avenir\\u2019\\u2019 program, grant ANR J11R107-F), the SOCLIM (Southern Ocean and climate) project supported by the French research program LEFE-CYBER of INSU-CNRS, the Climate Initiative of the foundation BNP Paribas and the French polar institute (IPEV), AtlantOS (funded by the European Union\\u2019s Horizon 2020 Research and Innovation program, grant 2014- 633211), E-AIMS (funded by the European Commission\\u2019s FP7 project, grant 312642), U.K. Bio-Argo (funded by the British Natural Environment Research Council-NERC, grant NE/L012855/1), REOPTIMIZE (funded by the European Union\\u2019s Horizon 2020 Research and Innovation program, Marie Sk\\u0142odowska-Curie grant 706781), Argo-Italy (funded by the Italian Ministry of Education, University and Research -MIUR), and the French Bio-Argo program (BGC-Argo France; funded by CNES-TOSCA, LEFE Cyber, and GMMC). We thank the PIs of several BGC-Argo floats missions and projects: Giorgio Dall\\u2019Olmo (Plymouth Marine Laboratory, United Kingdom; E-AIMS and U.K. Bio-Argo); Kjell-Arne Mork (Institute of Marine Research, Norway; E-AIMS); Violeta Slabakova (Bulgarian Academy of Sciences, Bulgaria; E-AIMS); Emil Stanev (University of Oldenburg, Germany; E-AIMS); Claire Lo Monaco (Laboratoire d\\u2019Oc\\ufffdeanographie et du Climat: Exp\\ufffderimentations et Approches Num\\ufffderiques); Pierre-Marie Poulain (National Institute of Oceanography and Experimental Geophysics, Italy; Argo-Italy); Sabrina Speich (Laboratoire de M\\ufffdet\\ufffdeorologie Dynamique, France; LEFE-GMMC); Virginie Thierry (Ifremer, France; LEFE-GMMC); Pascal Conan (Observatoire Oc\\ufffdeanologique de Banyuls sur mer, France; LEFE-GMMC); Laurent Coppola (Laboratoire d\\u2019Oc\\ufffdeanographie de Villefranche, France; LEFE-GMMC); Anne Petrenko (Mediterranean Institute of Oceanography, France; LEFE-GMMC); and Jean-Baptiste Sall\\ufffdee (Laboratoire d\\u2019Oc\\ufffdeanographie et du Climat, France; LEFE-GMMC). Collin Roesler (Bowdoin College, USA) and Yannick Huot (University of Sherbrooke, Canada) are acknowledged for useful comments and fruitful discussion. We also thank the International Argo Program and the CORIOLIS project that contribute to make the data freely and publicly available. Data referring to Organelli et al. (2016a; https://doi.org/10.17882/47142) and Barbieux et al. (2017; https://doi.org/10.17882/49388) are freely available on SEANOE.\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.1002/2017JC013030\"],\"citation_context\":\"Citation: Barbieux, M., Uitz, J., Bricaud, A., Organelli, E., Poteau, A., Schmechtig, C., . . . Claustre, H. (2018). Assessing the variability in the relationship between the particulate backscattering coefficient and the chlorophyll a concentration from a global Biogeochemical-Argo database. Journal of Geophysical Research: Oceans, 123, 1229-1250. https://doi. org/10.1002/2017JC013030\"},{\"dataset_ids\":[\"10.1016/j.rse.\"],\"citation_context\":\", Alvain, S., Vantrepotte, V., & Huertas, I. E. (2014). Identification of dominant phytoplankton functional types in the Mediterra-nean Sea based on a regionalized remote sensing approach. Remote Sensing of Environment, 152, 557-575. https://doi.org/10.1016/j.rse. 2014.06.029 Organelli, E., Barbieux, M., Claustre, H., Schmechtig, C., Poteau, A., Bricaud, A., . . . Dall\\u2019Olmo, G. (2016a). A global bio-optical database derived from Biogeochemical Argo float measurements within the layer of interest for field and\"}]}",
         "{\"title\":\"Assessing the variability in the relationship between the particulate backscattering coefficient and the chlorophyll a concentration from a global Biogeochemical-Argo database\",\"author\":\"Marie Barbieux\",\"abstract\":\"The particulate backscattering coefficient vs chlorophyll a concentration relationship varies along the water column, according to seasons and oceanic regions. -The b bp -to-Chla ratio is a valuable biogeochemical proxy for assessing the nature of the particulate assemblage and revealing photoacclimation processes. -The BGC-Argo float network yields an unprecedented amount of quality data for studying biogeochemical processes at a global scale and along the vertical dimension.\",\"data_availability\":\"\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.17882/49388\"],\"citation_context\":\"A global database of vertical profiles derived from Biogeochemical Argo float measurements for biogeochemical and bio-optical applications|SEANOE|10.17882/49388\"},{\"dataset_ids\":[\"10.17882/47142\"],\"citation_context\":\"A global bio-optical database derived from Biogeochemical Argo float measurements within the layer of interest for field and remote ocean colour applications|SEANOE|10.17882/47142\"},{\"dataset_ids\":[\"10.5194/essd-2017-58\"],\"citation_context\":\"Two databases derived from BGC-Argo float measurements for marine biogeochemical and biooptical applications|Earth Syst. Sci. Data|10.5194/essd-2017-58\"}]}"
        ],
        [
         "1",
         "10.1002_anie.201916483",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.201916483.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.201916483.xml",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "jats",
         "{\"title\":\"\",\"author\":\"Daniel Werner\",\"abstract\":\"Abstract: The homoleptic pyrazolate complexes [CeIII 4-(Me2pz)12] and [CeIV(Me2pz)4]2 quantitatively insert CO2 to give [CeIII 4(Me2pz\\u00b7CO2)12] and [CeIV(Me2pz\\u00b7CO2)4], respec-tively (Me2pz = 3,5-dimethylpyrazolato). This process is rever-sible for both complexes, as observed by in situ IR and NMR spectroscopy in solution and by TGA in the solid state. By adjusting the molar ratio, one molecule of CO2 per [CeIV-(Me2pz)4] complex could be inserted to give trimetallic [Ce3-(Me2pz)9(Me2pz\\u00b7CO2)3(thf)]. Both the cerous and ceric in-sertion products catalyze the formation of cyclic carbonates from epoxides and CO2 under mild conditions. In the absence of epoxide, the ceric catalyst is prone to reduction by the co-catalyst tetra-n-butylammonium bromide (TBAB).\",\"data_availability\":\"\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.1002/anie.201916483\"],\"citation_context\":\"15213773, 2020, 14, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/anie.201916483 by California Digital Library University Of California, Wiley Online Library on [09/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"}]}",
         "{\"title\":\"Effective and Reversible Carbon Dioxide Insertion into Cerium Pyrazolates\",\"author\":\"Bayer Uwe\",\"abstract\":\"Abstract The homoleptic pyrazolate complexes [Ce III 4 (Me 2 pz) 12 ] and [Ce IV (Me 2 pz) 4 ] 2 quantitatively insert CO 2 to give [Ce III 4 (Me 2 pz\\u22c5CO 2 ) 12 ] and [Ce IV (Me 2 pz\\u22c5CO 2 ) 4 ], respectively (Me 2 pz=3,5-dimethylpyrazolato). This process is reversible for both complexes, as observed by in situ IR and NMR spectroscopy in solution and by TGA in the solid state. By adjusting the molar ratio, one molecule of CO 2 per [Ce IV (Me 2 pz) 4 ] complex could be inserted to give trimetallic [Ce 3 (Me 2 pz) 9 (Me 2 pz\\u22c5CO 2 ) 3 (thf)]. Both the cerous and ceric insertion products catalyze the formation of cyclic carbonates from epoxides and CO 2 under mild conditions. In the absence of epoxide, the ceric catalyst is prone to reduction by the co-catalyst tetra- n -butylammonium bromide (TBAB).\",\"data_availability\":\"\",\"other_dataset_citations\":[]}"
        ],
        [
         "2",
         "10.1002_anie.202005531",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202005531.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.202005531.xml",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "bioc",
         "{\"title\":\"\",\"author\":\"Dong Wang\",\"abstract\":\"Abstract: Trifluoromethyl sulfoxides are a new class of trifluoromethylthiolating reagent. The sulfoxides engage in metal-free C\\ufffdH trifluoromethylthiolation with a range of (hetero)arenes. The method is also applicable to the function-alization of important compound classes, such as ligand derivatives and polyaromatics, and in the late-stage trifluor-omethylthiolation of medicines and agrochemicals. The iso-lation and characterization of a sulfonium salt intermediate supports an interrupted Pummerer reaction mechanism. Incorporating fluorine into organic compounds is a useful tool in drug design and development. The fluoro group is well known to improve the pharmacokinetic properties of a mol-ecule and fluorine-18 is an important radioisotope in molec-ular imaging.[1,2] Trifluoromethylthio (SCF3) groups are commonly found in drug molecules and veterinary medi-cines.[3,4] By combining a fluorinated moiety with a hetero-atom, many have turned to the SCF3 group to impart useful properties, such as high lipophilicity, to a compound of interest.[5]\",\"data_availability\":\"\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.1002/anie.202005531\"],\"citation_context\":\"15213773, 2020, 37, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/anie.202005531 by California Digital Library University Of California, Wiley Online Library on [09/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"}]}",
         "{\"title\":\"Trifluoromethyl Sulfoxides: Reagents for Metal\\u00e2\\u0080\\u0090Free C\\u00e2\\u0088\\u0092H Trifluoromethylthiolation\",\"author\":\"Wang Dong;prefix:Dr.\",\"abstract\":\"Abstract\",\"data_availability\":\"\",\"other_dataset_citations\":[]}"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>dataset_info</th>\n",
       "      <th>xml_type</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>xml_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.17882/4938...</td>\n",
       "      <td>tei</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Marie Barbieux\",\"abstrac...</td>\n",
       "      <td>{\"title\":\"Assessing the variability in the rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>jats</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Daniel Werner\",\"abstract...</td>\n",
       "      <td>{\"title\":\"Effective and Reversible Carbon Diox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>bioc</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Dong Wang\",\"abstract\":\"A...</td>\n",
       "      <td>{\"title\":\"Trifluoromethyl Sulfoxides: Reagents...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                                      pdf_file_path  \\\n",
       "0    10.1002_2017jc013030  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "1  10.1002_anie.201916483  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "2  10.1002_anie.202005531  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                       xml_file_path dataset_type  \\\n",
       "0  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "1  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "2  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "\n",
       "                                        dataset_info xml_type  \\\n",
       "0  [{'dataset_id': 'https://doi.org/10.17882/4938...      tei   \n",
       "1     [{'dataset_id': 'Missing', 'type': 'Missing'}]     jats   \n",
       "2     [{'dataset_id': 'Missing', 'type': 'Missing'}]     bioc   \n",
       "\n",
       "                                            pdf_text  \\\n",
       "0  {\"title\":\"\",\"author\":\"Marie Barbieux\",\"abstrac...   \n",
       "1  {\"title\":\"\",\"author\":\"Daniel Werner\",\"abstract...   \n",
       "2  {\"title\":\"\",\"author\":\"Dong Wang\",\"abstract\":\"A...   \n",
       "\n",
       "                                            xml_text  \n",
       "0  {\"title\":\"Assessing the variability in the rel...  \n",
       "1  {\"title\":\"Effective and Reversible Carbon Diox...  \n",
       "2  {\"title\":\"Trifluoromethyl Sulfoxides: Reagents...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the file paths DataFrame from the CSV file\n",
    "file_paths_df = pd.read_csv(os.path.join(BASE_OUTPUT_DIR, 'file_paths.csv'))\n",
    "# Fill NaN values in the 'xml_type' and 'xml_text' columns with empty strings\n",
    "file_paths_df['xml_file_path'] = file_paths_df['xml_file_path'].fillna('')\n",
    "file_paths_df['xml_text'] = file_paths_df['xml_text'].fillna('')\n",
    "# Display the first few rows of the file paths DataFrame\n",
    "print(f\"File paths DataFrame shape: {file_paths_df.shape}\")\n",
    "display(file_paths_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "76e021a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_info",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "c1bd1d4b-2f28-417d-8ef7-f6f2a8b01f22",
       "rows": [
        [
         "0",
         "10.1002_mp.14424",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_mp.14424.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_mp.14424.xml",
         "test",
         "[{'dataset_id': 'https://doi.org/10.7937/k9/tcia.2015.pf0m9rei', 'type': 'Secondary'}, {'dataset_id': 'https://doi.org/10.7937/tcia.2020.6c7y-gq39', 'type': 'Primary'}]",
         "jats",
         "{\"title\":\"\",\"author\":\"Kendall J. Kisera\",\"abstract\":\"PleThora: Pleural effusion and thoracic cavity segmentations in diseased lungs for benchmarking chest CT processing pipelines Kendall J. Kisera) John P. and Kathrine G. McGovern Medical School, Houston, TX, USA Center for Precision Health, UTHealth School of Biomedical Informatics, Houston, TX, USA Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA Sara Ahmed and Sonja Stieb Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA Abdallah S. R. Mohamed Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA MD Anderson Cancer Center-UTHealth Graduate School of Biomedical Sciences, Houston, TX, USA Hesham Elhalawani Department of Radiation Oncology, Cleveland Clinic Taussig Cancer Center, Cleveland, OH, USA Peter Y. S. Park, Nathan S. Doyle, and Brandon J. Wang Department of Diagnostic and Interventional Imaging, John P. and Kathrine G. McGovern Medical School, Houston, TX, USA Arko Barman, Zhao Li, and W. Jim Zheng Center for Precision Health, UTHealth School of Biomedical Informatics, Houston, TX, USA Clifton D. Fullera) Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA MD Anderson Cancer Center-UTHealth Graduate School of Biomedical Sciences, Houston, TX, USA Luca Giancardoa) Center for Precision Health, UTHealth School of Biomedical Informatics, Houston, TX, USA Department of Radiation Oncology, Cleveland Clinic Taussig Cancer Center, Cleveland, OH, USA (Received 7 April 2020; revised 22 July 2020; accepted for publication 27 July 2020; published 28 August 2020) This manuscript describes a dataset of thoracic cavity segmentations and discrete pleural effusion segmentations we have annotated on 402 computed tomography (CT) scans acquired from patients with non-small cell lung cancer. The segmentation of these anatomic regions precedes fundamental tasks in image analysis pipelines such as lung structure segmentation, lesion detection, and radiomics feature extraction. Bilateral thoracic cavity volumes and pleural effusion volumes were manually seg-me\",\"data_availability\":\"24734209, 2020, 11, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.14424 by California Digital Library University Of California, Wiley Online Library on [31/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.7937/tcia.2020.6c7y-gq39.51\"],\"citation_context\":\"We describe PleThora, a dataset of 402 expert-vetted tho-racic cavity segmentations, 78 expert-vetted pleural effusion segmentations, and corresponding clinical and technical metadata made available to the public through TCIA at https://doi.org/10.7937/tcia.2020.6c7y-gq39.51 These seg-mentations have value for preprocessing steps in image analy-sis pipelines built for fundamental quantitative imaging tasks, including but not limited to pathologic lung segmentation, lesion detection, and radiomics feature extraction.\"},{\"dataset_ids\":[\"10.7937/K9/TCIA.2015.PF0M9REI\"],\"citation_context\":\"chive (TCIA): maintaining and operating a public information repository. J Digit Imag-ing. 2013;26:1045-1057. 22. Aerts HJWL, Wee L, Rios Velazquez E, et al. Data from NSCLC-Radio-mics Dataset. In: The Cancer Imaging Archive; 2019. https://doi.org/10.7937/K9/TCIA.2015.PF0M9REI 23. Aerts HJ, Velazquez ER, Leijenaar RT, et al. Decoding tumour pheno-type by noninvasive imaging using a quantitative radiomics approach. Nat Commun. 2014;5:4006. 24. Li X, Morgan PS, Ashburner J, Smith J, Rorden C. dcm2niix.exe com-puter program\"},{\"dataset_ids\":[\"10.7937/tcia.2020.6c7y-gq39\"],\"citation_context\":\"51. Kiser KJ, Ahmed S, Stieb SM, et al. Data from the thoracic volume and pleural effusion segmentations in diseased lungs for benchmarking chest CT processing pipelines Dataset. In: The Cancer Imaging Archive; 2020. https://doi.org/10.7937/tcia.2020.6c7y-gq39 52. Ronneberger O, Fischer P, Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation. Paper presented at: Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015; 5-9 October, 2015; Munich, Germany. https://doi.org/\"}]}",
         "{\"title\":\"PleThora: Pleural effusion and thoracic cavity segmentations in diseased lungs for benchmarking chest CT processing pipelines\",\"author\":\"Kiser Kendall J.\",\"abstract\":\"This manuscript describes a dataset of thoracic cavity segmentations and discrete pleural effusion segmentations we have annotated on 402 computed tomography (CT) scans acquired from patients with non-small cell lung cancer. The segmentation of these anatomic regions precedes fundamental tasks in image analysis pipelines such as lung structure segmentation, lesion detection, and radiomics feature extraction. Bilateral thoracic cavity volumes and pleural effusion volumes were manually segmented on CT scans acquired from The Cancer Imaging Archive 'NSCLC Radiomics' data collection. Four hundred and two thoracic segmentations were first generated automatically by a U-Net based algorithm trained on chest CTs without cancer, manually corrected by a medical student to include the complete thoracic cavity (normal, pathologic, and atelectatic lung parenchyma, lung hilum, pleural effusion, fibrosis, nodules, tumor, and other anatomic anomalies), and revised by a radiation oncologist or a radiologist. Seventy-eight pleural effusions were manually segmented by a medical student and revised by a radiologist or radiation oncologist. Interobserver agreement between the radiation oncologist and radiologist corrections was acceptable. All expert-vetted segmentations are publicly available in NIfTI format through The Cancer Imaging Archive at https://doi.org/10.7937/tcia.2020.6c7y-gq39 . Tabular data detailing clinical and technical metadata linked to segmentation cases are also available. Thoracic cavity segmentations will be valuable for developing image analysis pipelines on pathologic lungs - where current automated algorithms struggle most. In conjunction with gross tumor volume segmentations already available from 'NSCLC Radiomics,' pleural effusion segmentations may be valuable for investigating radiomics profile differences between effusion and primary tumor or training algorithms to discriminate between them.\",\"data_availability\":\"\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.7937/K9/TCIA.2015.PF0M9REI\"],\"citation_context\":\"Aerts HJWL , Wee L , Rios Velazquez E , et al. Data from NSCLC-Radiomics [Dataset] . In: The Cancer Imaging Archive ; 2019 10.7937/K9/TCIA.2015.PF0M9REI\"},{\"dataset_ids\":[\"10.7937/tcia.2020.6c7y-gq39\"],\"citation_context\":\"Kiser KJ , Ahmed S , Stieb SM , et al. Data from the thoracic volume and pleural effusion segmentations in diseased lungs for benchmarking chest CT processing pipelines [Dataset] . In: The Cancer Imaging Archive ; 2020 10.7937/tcia.2020.6c7y-gq39\"}]}"
        ],
        [
         "1",
         "10.1002_ece3.6144",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6144.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.6144.xml",
         "test",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.zw3r22854', 'type': 'Primary'}]",
         "jats",
         "{\"title\":\"\",\"author\":\"Beng\",\"abstract\":\"Ecology and Evolution. 2020;10:3463-3476. \\ufeff\\b | 3463 www.ecolevol.org 1 | INTRODUCTION Marine ecosystems can change rapidly in response to both natural and anthropogenic changes in the environment. Monitoring changes in the abundance and distribution of marine organisms is a chal-lenging but critical component of conservation efforts and natural resource management. Human impacts such as overfishing, coastal development, and pollution can rapidly increase stress on ecosys-tems resulting in an urgent need to develop efficient methods for monitoring spatial and temporal dynamics of biodiversity (Beng et al., 2016). Fisheries management has long used ichthyoplankton sur-veys as a component of monitoring because sampling the early life stages of fish reflects the reproductive activities of fish Received: 17 July 2019 | Revised: 31 January 2020 | Accepted: 4 February 2020 DOI: 10.1002/ece3.6144 O R I G I N A L R E S E A R C H Efficacy of metabarcoding for identification of fish eggs evaluated with mock communities Elena M. Duke | Ronald S. Burton This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. \\u00a9 2020 The Authors. Ecology and Evolution published by John Wiley & Sons Ltd. Marine Biology Research Division, Scripps Institution of Oceanography, University of California, San Diego, La Jolla, California Correspondence Elena M. Duke, Marine Biology Research Division, Scripps Institution of Oceanography, University of California, San Diego, La Jolla, CA. Email: emduke@ucsd.edu Abstract There is urgent need for effective and efficient monitoring of marine fish popula-tions. Monitor\",\"data_availability\":\"DATA AVAILABILITY STATEMENT\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.1002/ece3.6144\"],\"citation_context\":\"20457758, 2020, 7, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ece3.6144, Wiley Online Library on [31/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"},{\"dataset_ids\":[\"10.1371/journal.pone.0134647\"],\"citation_context\":\"Hermsmeier, M. C., Rogowski, P. A., Terrill, E., Burton, R. S., & Bernardi, G. (2015). Monitoring spawning ac-tivity in a Southern California marine protected area using molec-ular identification of fish eggs. PLoS ONE, 10(8), 1-21. https://doi. org/10.1371/journal.pone.0134647 Hastings, P. A., & Burton, R. S. (2008). Establishing a DNA Sequence Database for the Marine Fish Fauna of California. Retrieved from https://escholarship.org/uc/item/1ck9b3qs Hatzenbuhler, C., Kelly, J. R., Martinson, J., Okum, S., & Pilgrim, E. (2\"}]}",
         "{\"title\":\"Efficacy of metabarcoding for identification of fish eggs evaluated with mock communities\",\"author\":\"Duke Elena M.\",\"abstract\":\"Abstract There is urgent need for effective and efficient monitoring of marine fish populations. Monitoring eggs and larval fish may be more informative than that traditional fish surveys since ichthyoplankton surveys reveal the reproductive activities of fish populations, which directly impact their population trajectories. Ichthyoplankton surveys have turned to molecular methods (DNA barcoding & metabarcoding) for identification of eggs and larval fish due to challenges of morphological identification. In this study, we examine the effectiveness of using metabarcoding methods on mock communities of known fish egg DNA. We constructed six mock communities with known ratios of species. In addition, we analyzed two samples from a large field collection of fish eggs and compared metabarcoding results with traditional DNA barcoding results. We examine the ability of our metabarcoding methods to detect species and relative proportion of species identified in each mock community. We found that our metabarcoding methods were able to detect species at very low input proportions; however, levels of successful detection depended on the markers used in amplification, suggesting that the use of multiple markers is desirable. Variability in our quantitative results may result from amplification bias as well as interspecific variation in mitochondrial DNA copy number. Our results demonstrate that there remain significant challenges to using metabarcoding for estimating proportional species composition; however, the results provide important insights into understanding how to interpret metabarcoding data. This study will aid in the continuing development of efficient molecular methods of biological monitoring for fisheries management.\",\"data_availability\":\"DATA AVAILABILITY STATEMENT Data are available at Dryad Digital Repository at:  https://doi.org/10.5061/dryad.zw3r22854 . Two sequences used in our reference library are on GenBank (accession numbers MH714866 and MH718435).\",\"other_dataset_citations\":[]}"
        ],
        [
         "2",
         "10.1002_ejoc.202000139",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejoc.202000139.pdf",
         "",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "unknown",
         "{\"title\":\"\",\"author\":\"Ann Christin Reiers\\u00f8lmoen[a\",\"abstract\":\"Abstract: Studies on gold(III) coordination of a series of pre-pared polydentate pyridine and quinoline based ligands are re-ported. Characterization (1H, 13C, 15N NMR, and XRD) of the novel gold(III) complexes, prepared in 31-98 % yield, revealed different coordination ability of the pyridine and quinoline nitrogen atoms. Testing of catalytic activity in cyclopropanation\",\"data_availability\":\"10990690, 2020, 19, Downloaded from https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ejoc.202000139 by California Digital Library University Of California, Wiley Online Library on [09/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\",\"other_dataset_citations\":[]}",
         ""
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>dataset_info</th>\n",
       "      <th>xml_type</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>xml_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_mp.14424</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.7937/k9/tc...</td>\n",
       "      <td>jats</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Kendall J. Kisera\",\"abst...</td>\n",
       "      <td>{\"title\":\"PleThora: Pleural effusion and thora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_ece3.6144</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "      <td>jats</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Beng\",\"abstract\":\"Ecolog...</td>\n",
       "      <td>{\"title\":\"Efficacy of metabarcoding for identi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_ejoc.202000139</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>unknown</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Ann Christin Reiers\\u00f...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                                      pdf_file_path  \\\n",
       "0        10.1002_mp.14424  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "1       10.1002_ece3.6144  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "2  10.1002_ejoc.202000139  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                       xml_file_path dataset_type  \\\n",
       "0  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "1  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "2                                                            test   \n",
       "\n",
       "                                        dataset_info xml_type  \\\n",
       "0  [{'dataset_id': 'https://doi.org/10.7937/k9/tc...     jats   \n",
       "1  [{'dataset_id': 'https://doi.org/10.5061/dryad...     jats   \n",
       "2     [{'dataset_id': 'Missing', 'type': 'Missing'}]  unknown   \n",
       "\n",
       "                                            pdf_text  \\\n",
       "0  {\"title\":\"\",\"author\":\"Kendall J. Kisera\",\"abst...   \n",
       "1  {\"title\":\"\",\"author\":\"Beng\",\"abstract\":\"Ecolog...   \n",
       "2  {\"title\":\"\",\"author\":\"Ann Christin Reiers\\u00f...   \n",
       "\n",
       "                                            xml_text  \n",
       "0  {\"title\":\"PleThora: Pleural effusion and thora...  \n",
       "1  {\"title\":\"Efficacy of metabarcoding for identi...  \n",
       "2                                                     "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_file_paths_df = file_paths_df.copy().sample(3, random_state=42).reset_index(drop=True)\n",
    "sample_file_paths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4d2524bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847cc2380a664a5083fb44f628007a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article 0/30: 10.1002_2017jc013030, type: XML\n",
      "Preparing input with length: 1441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': '10.17882/49388', 'type': 'Primary'}, {'dataset_id': '10.17882/47142', 'type': 'Primary'}, {'dataset_id': '10.5194/essd-2017-58', 'type': 'Primary'}]\n",
      "Processing article 1/30: 10.1002_anie.201916483, type: XML\n",
      "Preparing input with length: 1008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 2/30: 10.1002_anie.202005531, type: XML\n",
      "Preparing input with length: 248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 3/30: 10.1002_anie.202007717, type: XML\n",
      "Preparing input with length: 1384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 4/30: 10.1002_chem.201902131, type: XML\n",
      "Preparing input with length: 1196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 5/30: 10.1002_chem.201903120, type: XML\n",
      "Preparing input with length: 1830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 6/30: 10.1002_chem.202000235, type: XML\n",
      "Preparing input with length: 772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 7/30: 10.1002_chem.202001412, type: XML\n",
      "Preparing input with length: 1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 8/30: 10.1002_chem.202001668, type: XML\n",
      "Preparing input with length: 826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 9/30: 10.1002_chem.202003167, type: XML\n",
      "Preparing input with length: 1831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 10/30: 10.1002_cssc.202201821, type: XML\n",
      "Preparing input with length: 1547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': '10.5281/zenodo.7074790', 'type': 'Primary'}]\n",
      "Processing article 11/30: 10.1002_ece3.3985, type: XML\n",
      "Preparing input with length: 1659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 12/30: 10.1002_ece3.4466, type: XML\n",
      "Preparing input with length: 1762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 13/30: 10.1002_ece3.5260, type: XML\n",
      "Preparing input with length: 2476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': '10.5061/dryad.2f62927', 'type': 'Primary'}]\n",
      "Processing article 14/30: 10.1002_ece3.5395, type: XML\n",
      "Preparing input with length: 2346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': '10.12345/12345', 'type': 'Primary'}, {'dataset_id': '10.5441/001/1.v1cs4nn0', 'type': 'Primary'}, {'dataset_id': '10.5441/001/1.c42j3js7', 'type': 'Primary'}, {'dataset_id': '10.5441/001/1.4192t2j4', 'type': 'Primary'}, {'dataset_id': '10.5441/001/1.ck04mn78', 'type': 'Primary'}]\n",
      "Processing article 15/30: 10.1002_ece3.6144, type: XML\n",
      "Preparing input with length: 2179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': '10.5061/dryad.zw3r22854', 'type': 'Primary'}]\n",
      "Processing article 16/30: 10.1002_ece3.6303, type: XML\n",
      "Preparing input with length: 2544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: []\n",
      "Processing article 17/30: 10.1002_ece3.6784, type: XML\n",
      "Preparing input with length: 2740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': '10.1007/s00435-017-0386-3', 'type': 'Primary'}, {'dataset_id': '10.1007/s00435-0386-3', 'type': 'Primary'}]\n",
      "Processing article 18/30: 10.1002_ece3.961, type: XML\n",
      "Preparing input with length: 1714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 19/30: 10.1002_ece3.9627, type: XML\n",
      "Preparing input with length: 2420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': '10.5061/dryad.b8gtht7h3', 'type': 'Primary'}]\n",
      "Processing article 20/30: 10.1002_ecs2.1280, type: PDF\n",
      "Preparing input with length: 3265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': '10.5061/dryad.p3fg9', 'type': 'Primary'}, {'dataset_id': '10.1002/ecs2.1280', 'type': 'Primary'}]\n",
      "Processing article 21/30: 10.1002_ecs2.4619, type: XML\n",
      "Preparing input with length: 2005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': '10.25349/D9QW5X', 'type': 'Secondary'}]\n",
      "Processing article 22/30: 10.1002_ejic.201900904, type: XML\n",
      "Preparing input with length: 986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 23/30: 10.1002_ejoc.202000139, type: PDF\n",
      "Preparing input with length: 910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: []\n",
      "Processing article 24/30: 10.1002_ejoc.202000916, type: XML\n",
      "Preparing input with length: 585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processing article 25/30: 10.1002_esp.5058, type: PDF\n",
      "Preparing input with length: 4434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': '10.1002/esp.5058', 'type': 'Primary'}, {'dataset_id': '10.1002/esp.4637', 'type': 'Primary'}, {'dataset_id': '10.5066/P9FW6E8K', 'type': 'Primary'}, {'dataset_id': '10.1029/2005JF000416', 'type': 'Primary'}]\n",
      "Processing article 26/30: 10.1002_esp.5090, type: XML\n",
      "Preparing input with length: 2601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': '10.1080/08120090802546977', 'type': 'Secondary'}, {'dataset_id': '10.1177/0959683613515732', 'type': 'Secondary'}]\n",
      "Processing article 27/30: 10.1002_mp.14424, type: XML\n",
      "Preparing input with length: 2719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': '10.7937/K9/TCIA.2015.PF0M9REI', 'type': 'Primary'}, {'dataset_id': '10.7937/tcia.2020.6c7y-gq39', 'type': 'Primary'}]\n",
      "Processing article 28/30: 10.1002_nafm.10870, type: PDF\n",
      "Preparing input with length: 4945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed response: [{'dataset_id': '10.5066/P9GTUMAY.', 'type': 'Primary'}, {'dataset_id': '10.5066/P94BH3W0.', 'type': 'Secondary'}, {'dataset_id': '10.5066/P90A4UGJ.', 'type': 'Primary'}, {'dataset_id': '10.5066/F75M63X0.', 'type': 'Secondary'}]\n",
      "Processing article 29/30: 10.1007_jhep07(2018)134, type: PDF\n",
      "Preparing input with length: 888\n",
      "Parsed response: [{'dataset_id': 'Missing', 'type': 'Missing'}]\n",
      "Processed articles DataFrame shape: (30, 6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_info",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "text_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "llm_input",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "llm_response",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "llm_thinking_content",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "98e86e78-51a6-4249-bf7c-1fbe7d49973f",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "[{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]",
         "XML",
         "Text Content: {\"title\":\"Assessing the variability in the relationship between the particulate backscattering coefficient and the chlorophyll a concentration from a global Biogeochemical-Argo database\",\"author\":\"Marie Barbieux\",\"abstract\":\"The particulate backscattering coefficient vs chlorophyll a concentration relationship varies along the water column, according to seasons and oceanic regions. -The b bp -to-Chla ratio is a valuable biogeochemical proxy for assessing the nature of the particulate assemblage and revealing photoacclimation processes. -The BGC-Argo float network yields an unprecedented amount of quality data for studying biogeochemical processes at a global scale and along the vertical dimension.\",\"data_availability\":\"\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.17882/49388\"],\"citation_context\":\"A global database of vertical profiles derived from Biogeochemical Argo float measurements for biogeochemical and bio-optical applications|SEANOE|10.17882/49388\"},{\"dataset_ids\":[\"10.17882/47142\"],\"citation_context\":\"A global bio-optical database derived from Biogeochemical Argo float measurements within the layer of interest for field and remote ocean colour applications|SEANOE|10.17882/47142\"},{\"dataset_ids\":[\"10.5194/essd-2017-58\"],\"citation_context\":\"Two databases derived from BGC-Argo float measurements for marine biogeochemical and biooptical applications|Earth Syst. Sci. Data|10.5194/essd-2017-58\"}]}\n",
         "[{'dataset_id': '10.17882/49388', 'type': 'Primary'}, {'dataset_id': '10.17882/47142', 'type': 'Primary'}, {'dataset_id': '10.5194/essd-2017-58', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with a paper section and wants me to extract dataset IDs and classify them as Primary or Secondary. \n\nFirst, I need to check the data_availability section. The input says it's empty, so I should look at the other_dataset_citations. There are three entries. \n\nLooking at each dataset_id, I need to validate them. The first one is \"10.17882/49388\". The citation context mentions a global database for vertical profiles from Biogeochemical Argo. Since the data_availability is empty, and the citations are there, this seems like a valid dataset. The DOI is valid and refers to a dataset. So this should be Primary.\n\nNext, the second dataset_id is \"10.17882/47142\". The citation context also mentions a global bio-optical database. Again, the data_availability is empty, so this is a valid dataset. Primary.\n\nThird, \"10.5194/essd-2017-58\" has a citation context about two databases derived from BGC-Argo. Since the data_availability is empty, this is also a valid dataset. Primary.\n\nNow, I need to check if there are any Secondary indicators. The citation contexts mention \"global database\" and \"global bio-optical database\", which are likely Secondary. The data_availability is empty, so no Secondary data here. \n\nSo all three dataset_ids are Primary. The output should be a JSON array with each dataset as an object. The user's example shows that if there are multiple, they list them. Since there are three, the JSON array should include all three. \n\nI need to make sure not to include any other data. The data_availability is empty, so no more entries. Also, check for any possible duplicates, but since each is unique, no need to deduplicate. \n\nFinally, format the JSON correctly. The structure should have the dataset_ids and their types. All three are Primary, so the output is as specified.\n</think>"
        ],
        [
         "1",
         "10.1002_anie.201916483",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"Effective and Reversible Carbon Dioxide Insertion into Cerium Pyrazolates\",\"author\":\"Bayer Uwe\",\"abstract\":\"Abstract The homoleptic pyrazolate complexes [Ce III 4 (Me 2 pz) 12 ] and [Ce IV (Me 2 pz) 4 ] 2 quantitatively insert CO 2 to give [Ce III 4 (Me 2 pz\\u22c5CO 2 ) 12 ] and [Ce IV (Me 2 pz\\u22c5CO 2 ) 4 ], respectively (Me 2 pz=3,5-dimethylpyrazolato). This process is reversible for both complexes, as observed by in situ IR and NMR spectroscopy in solution and by TGA in the solid state. By adjusting the molar ratio, one molecule of CO 2 per [Ce IV (Me 2 pz) 4 ] complex could be inserted to give trimetallic [Ce 3 (Me 2 pz) 9 (Me 2 pz\\u22c5CO 2 ) 3 (thf)]. Both the cerous and ceric insertion products catalyze the formation of cyclic carbonates from epoxides and CO 2 under mild conditions. In the absence of epoxide, the ceric catalyst is prone to reduction by the co-catalyst tetra- n -butylammonium bromide (TBAB).\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with a paper section and wants me to extract dataset IDs. First, I need to check the data_availability and other_dataset_citations sections.\n\nLooking at the input, the data_availability is an empty string, so I should skip Task 1. Then, the other_dataset_citations is also an empty list, so I skip Task 1 either. Since both are empty, I go to Task 3, which is the final step. \n\nIn Task 3, since there are no valid dataset IDs found, I need to return a JSON array with a \"Missing\" entry. The structure should have one object with dataset_id as \"Missing\" and type as \"Missing\". \n\nI need to make sure I'm not adding any dataset IDs and that the classification is correctly set to \"Missing\" because there's no data provided. The key points to check are the absence of any dataset IDs in the given sections, which leads to the conclusion that no valid dataset was found.\n</think>"
        ],
        [
         "2",
         "10.1002_anie.202005531",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"Trifluoromethyl Sulfoxides: Reagents for Metal\\u00e2\\u0080\\u0090Free C\\u00e2\\u0088\\u0092H Trifluoromethylthiolation\",\"author\":\"Wang Dong;prefix:Dr.\",\"abstract\":\"Abstract\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with some paper sections. The task is to extract valid dataset IDs and classify them as Primary or Secondary.\n\nFirst, I need to check the data_availability section. The input here is an empty string, so Task 1 starts there. Since there are no valid dataset IDs in data_availability, I proceed to Task 2.\n\nLooking at the other_dataset_citations, there's an empty list. So Task 1 also skips. Now, Task 2 requires classifying each dataset. But since there are no dataset IDs, I need to handle the \"Missing\" scenario.\n\nThe user's instruction says to return a JSON array with \"Missing\" if no dataset is found. So the final output should be a single object with dataset_id as \"Missing\" and type as \"Missing\".\n</think>"
        ],
        [
         "3",
         "10.1002_anie.202007717",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"Metal\\u2013Ligand Cooperativity of the Calix[4]pyrrolato Aluminate: Triggerable C\\u2212C Bond Formation and Rate Control in Catalysis\",\"author\":\"Ebner Fabian\",\"abstract\":\"Abstract Metal-ligand cooperativity (MLC) had a remarkable impact on transition metal chemistry and catalysis. By use of the calix[4]pyrrolato aluminate, [ 1 ] \\u2212 , which features a square-planar Al III , we transfer this concept into the p -block and fully elucidate its mechanisms by experiment and theory. Complementary to transition metal-based MLC ( aromatization upon substrate binding), substrate binding in [ 1 ] \\u2212 occurs by dearomatization of the ligand. The aluminate trapps carbonyls by the formation of C\\u2212C and Al\\u2212O bonds, but the products maintain full reversibility and outstanding dynamic exchange rates. Remarkably, the C\\u2212C bonds can be formed or cleaved by the addition or removal of lithium cations, permitting unprecedented control over the system's constitutional state. Moreover, the metal-ligand cooperative substrate interaction allows to twist the kinetics of catalytic hydroboration reactions in a unique sense. Ultimately, this work describes the evolution of an anti-van't Hoff/Le Bel species from their being as a structural curiosity to their application as a reagent and catalyst.\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with a paper section and wants me to extract dataset IDs. First, I need to check the data_availability and other_dataset_citations sections.\n\nLooking at the data_availability, it's an empty string. So I should skip Task 1. Then, Task 2 comes next. But since there are no dataset IDs in either section, I need to return the \"Missing\" JSON structure. The user's example shows that if no dataset IDs are found, the output is a single object with \"dataset_id\" and \"type\" both missing. \n\nWait, the input data has \"data_availability\": \"\" and \"other_dataset_citations\": []. So according to Task 1, no valid dataset IDs are found. Therefore, Task 3 should output the \"Missing\" scenario. I need to make sure that the JSON structure is correctly formatted as per the user's example. The final answer should be a JSON array with one object, both dataset_id and type missing.\n</think>"
        ],
        [
         "4",
         "10.1002_chem.201902131",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"A Synthetic Route Toward Tetrazoles: The Thermolysis of Geminal Diazides\",\"author\":\"Holzschneider Kristina\",\"abstract\":\"Abstract A new synthetic route toward the tetrazole core is described, which is based on a general fragmentation pattern that was found in a range of compounds featuring geminal diazido units. Through a simple two-step procedure, the synthesis of structurally diverse target compounds containing a tetrazole, such as tetrazoloquinoxalinones, benzoylaryltetrazoles, tetrazolotriazinones, and tetrazoloazepinones, was easily accomplished, starting from broadly accessible substrates (i.e., oxindoles, diarylethanones, pyrazolones, and phenanthrols). The initial oxidative diazidation reaction with iodine and sodium azide under mild conditions is followed by the thermal fragmentation under microwave irradiation, leading to the tetrazole products. Noteworthy, an experimental solution is presented in which the potentially hazardous diazide intermediates are not isolated and the concentration of crude reaction mixtures containing diazides is not required to achieve the tetrazoles in good yields.\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with a paper section and wants me to extract dataset IDs. First, I need to check the data_availability and other_dataset_citations sections.\n\nLooking at the input, the data_availability is an empty string, so I should skip Task 1. Then, the other_dataset_citations is also an empty list, so I skip Task 1 either. Since both are empty, I go to Task 3, which is the fallback. The output should be a JSON array with \"Missing\" entries for both dataset_ids. But wait, the user's input has no dataset IDs, so both are missing. So the final answer is an empty array with those entries.\n</think>"
        ],
        [
         "5",
         "10.1002_chem.201903120",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"Structure\\u2013Solubility Relationship of 1,4\\u2010Dioxane Complexes of Di(hydrocarbyl)magnesium\",\"author\":\"Fischer Reinald Dr.\",\"abstract\":\"Abstract Systematic variation of the 1,4-dioxane (dx) concentration during the precipitation of sparingly soluble [MgBr 2 (dx) 2 ] from ethereal Grignard solutions of RMgBr has allowed the structural investigation of crystallized [R 2 Mg(dx) n ] ( n =1, 1.5, 2, and 3), which form during this dioxane method, depending on the bulkiness of R. The numbering of the complexes explored in this study is based on the number n of dioxane molecules per magnesium atom, followed by the substituent R; an apostrophe denotes coordination polymers. The following derivatives were studied by X-ray crystal-structure determination and NMR spectroscopy: n =1: [Me 2 Mg(\\u03bc-dx)] \\u221e ( 1\\u2032-Me ) and [ n Pr 2 Mg(\\u03bc-dx)] \\u221e ( 1\\u2032- n Pr ); n =1.5: [{ i Pr 2 Mg(dx)} 2 (\\u03bc-dx)] ( 1.5- i Pr ), [{ o Tol 2 Mg(dx)} 2 (\\u03bc-dx)] ( 1.5- o Tol ), and [(Me 3 Si-C\\u2261C) 2 Mg(dx) 1.5 ] \\u221e ( 1.5\\u2032-C 2 SiMe 3 ); n =2: [ t Bu 2 Mg(dx) 2 ] ( 2- t Bu ) and [ o Tol 2 Mg(dx) 2 ] ( 2- o Tol ); n =3: [Ph 2 Mg(dx) 3 ] ( 3-Ph ). In the structure types 1\\u2032 , 1.5 , and 2 , the magnesium atom exhibits the coordination number 4, whereas pentacoordinate metal atoms are observed in types 3 and 1.5\\u2032 . The structure type 2\\u2032 is realized for [(Ph-C\\u2261C) 2 Mg(dx) 2 ] \\u221e ( 2\\u2032-C 2 Ph ), [MgCl 2 (dx) 2 ] \\u221e ( 2\\u2032-Cl ), and [MgBr 2 (dx) 2 ] \\u221e ( 2\\u2032-Br ) with hexacoordinate metal atoms. The solubility of the dioxane adducts in common organic solvents strongly depends on the degree of aggregation with the solubility decreasing from molecular to strand to layer structures.\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a paper section and wants me to extract dataset IDs and classify them as Primary or Secondary. First, I need to check the data_availability and other_dataset_citations sections.\n\nLooking at the data_availability, it's empty. So I'll check the other_dataset_citations. The other_dataset_citations array is empty too. So according to Task 1, no valid dataset IDs were found. Then, since both sections are empty, I should skip to Task 3 and output the \"Missing\" JSON structure.\n\nThe structure should have a single object with dataset_id as \"Missing\" and type as \"Missing\". I need to make sure that the JSON is correctly formatted and that there are no typos. Let me double-check the JSON syntax to ensure it's valid.\n</think>"
        ],
        [
         "6",
         "10.1002_chem.202000235",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"An Isolable Bis(Silanone\\u2013Borane) Adduct\",\"author\":\"Luecke Marcel\\u2010Philip\",\"abstract\":\"Abstract The reaction of bis(silylenyl)-substituted ferrocene 1 with two molar equivalents of BPh 3 yields the corresponding bis(silylene-borane) Lewis adduct 2 . The latter is capable to activate CO 2 to furnish the borane-stabilized bis(silanone) 3 through mono-oxygenation of the dative Si II \\u2192B silicon centers under release of CO. Removal of BPh 3 from 3 with PMe 3 affords the corresponding 1,3,2,4-cyclodisiloxane and the Me 3 P\\u2212BPh 3 adduct. All isolated new compounds were characterized and their molecular structures were determined by single-crystal X-ray diffraction analyses.\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with a paper section and wants me to extract dataset IDs. First, I need to check the data_availability and other_dataset_citations sections.\n\nLooking at the input, the data_availability is an empty string, so I should check the other_dataset_citations. The other_dataset_citations array is empty too. According to the instructions, if both are empty, I skip all tasks. So, no dataset IDs are found here. \n\nNow, I need to format the output. Since there are no valid dataset IDs, I should return a JSON array with a single object indicating \"Missing\". The dataset_id is \"Missing\" and the type is \"Missing\". \n\nI need to make sure I'm following the rules correctly. The key points are to only consider data-related dataset IDs, validate DOIs, and apply the classification logic. Since there are no valid IDs, the output is as specified.\n</think>"
        ],
        [
         "7",
         "10.1002_chem.202001412",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"Iron(0)\\u2010Mediated Stereoselective (3+2)\\u2010Cycloaddition of Thiochalcones via a Diradical Intermediate\",\"author\":\"Buday Philipp\",\"abstract\":\"Abstract Reactions of \\u03b1,\\u03b2-unsaturated aromatic thioketones 1 (thiochalcones) with Fe 3 (CO) 12 leading to \\u03b7 4 -1-thia-1,3-diene iron tricarbonyl complexes 2 , [FeFe] hydrogenase mimics 3 , and the thiopyrane adduct 4 are described. Obtained products have been characterized by X-ray crystallography and by computational methods. Completely regio- and diastereoselective formation of the five-membered ring system in products 3 , containing four stereogenic centers, can be explained by an unprecedented, stepwise (3+2)-cycloaddition of two thiochalcone molecules mediated by Fe 3 (CO) 12 . Quantum chemical calculations aimed at elucidation of the reaction mechanism, suggest that the formal (3+2)-cycloaddition proceeds via sequential intramolecular radical transfer events upon homolytic cleavage of one carbon-sulfur bond leading to a diradical intermediate.\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with a paper section and wants me to extract dataset IDs. First, I need to check the data_availability and other_dataset_citations sections.\n\nLooking at the data_availability, it's an empty string. So I should skip Task 1. Then, Task 2 comes next. But since there are no dataset IDs in either section, I need to return the \"Missing\" JSON structure. The user's example shows that if no dataset IDs are found, the output is a single object with \"dataset_id\" and \"type\" both missing. \n\nWait, the input data has no dataset IDs in the given sections. The data_availability is empty, and other_dataset_citations is also empty. So according to the instructions, I should output the missing JSON. That makes sense. I need to make sure that the output is formatted correctly as per the user's example. So the final answer should be a JSON array with one object where both dataset_id and type are missing.\n</think>"
        ],
        [
         "8",
         "10.1002_chem.202001668",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"The First Use of a ReX 5  Synthon to Modulate Fe III  Spin Crossover via Supramolecular Halogen\\u22c5\\u22c5\\u22c5Halogen Interactions\",\"author\":\"Busch Rebecca Dr.\",\"abstract\":\"Abstract We have added the {Re IV X 5 } \\u2212 (X=Br, Cl) synthon to a pocket-based ligand to provide supramolecular design using halogen\\u22c5\\u22c5\\u22c5halogen interactions within an Fe III system that has the potential to undergo spin crossover (SCO). By removing the solvent from the crystal lattice, we 'switch on' halogen\\u22c5\\u22c5\\u22c5halogen interactions between neighboring molecules, providing a supramolecular cooperative pathway for SCO. Furthermore, changes to the halogen-based interaction allow us to modify the temperature and nature of the SCO event.\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with some text content, and I need to extract dataset IDs from it. The task is to identify and classify them as Primary or Secondary based on the given rules.\n\nFirst, I'll check the data_availability section. The text here is empty, so I'll move to the other_dataset_citations. The other_dataset_citations array is also empty, so according to Task 1, there are no valid dataset IDs. Then, since both sections are empty, I need to skip to Task 3, which is the final step.\n\nSo, the output should be a JSON array with a single object indicating that there are no valid dataset IDs. The structure should be as specified, with \"dataset_id\" as \"Missing\" and \"type\" as \"Missing\".\n</think>"
        ],
        [
         "9",
         "10.1002_chem.202003167",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"Metal\\u2010only Lewis Pairs of Rhodium with  s ,  p  and  d \\u2010Block Metals\",\"author\":\"Bajo Sonia Dr.\",\"abstract\":\"Abstract Metal-only Lewis pairs (MOLPs) in which the two metal fragments are solely connected by a dative M\\u2192M bond represent privileged architectures to acquire fundamental understanding of bimetallic bonding. This has important implications in many catalytic processes or supramolecular systems that rely on synergistic effects between two metals. However, a systematic experimental/computational approach on a well-defined class of compounds is lacking. Here we report a family of MOLPs constructed around the Rh I precursor [( \\u03b7 5 -C 5 Me 5 )Rh(PMe 3 ) 2 ] ( 1 ) with a series of s , p and d -block metals, mostly from the main group elements, and investigate their bonding by computational means. Among the new MOLPs, we have structurally characterized those formed by dative bonding between 1 and MgMeBr, AlMe 3 , GeCl 2 , SnCl 2 , ZnMe 2 and Zn(C 6 F 5 ) 2, as well as spectroscopically identified the ones resulting from coordination to MBAr F (M=Na, Li; BAr F \\u2212 =[B(C 6 H 2 -3,5-(CF 3 ) 2 ) 4 ] \\u2212 ) and CuCl. Some of these compounds represent unique examples of bimetallic structures, such as the first unambiguous cases of Rh\\u2192Mg dative bonding or base-free rhodium bound germylene and stannylene species. Multinuclear NMR spectroscopy, including 103 Rh NMR, is used to probe the formation of Rh\\u2192M bonds. A comprehensive theoretical analysis of those provides clear trends. As anticipated, greater bond covalency is found for the more electronegative acids, whereas ionic character dominates for the least electronegative nuclei, though some degree of electron sharing is identified in all cases.\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON string representing a paper section, and I need to extract dataset IDs and classify them as Primary or Secondary. Let me start by understanding the input.\n\nFirst, looking at the `data_availability` section, it's empty. So I should check the `other_dataset_citations` section. The `other_dataset_citations` array is empty too. According to the instructions, if both sections are empty, I should skip all tasks and output the \"Missing\" structure. But in this case, both are empty. So I need to proceed to Task 3.\n\nTask 1 is to identify valid dataset IDs. Since both sections are empty, I can't find any dataset IDs. So I'll output the \"Missing\" JSON structure. The user's example shows that when both are empty, the output is a single object with \"dataset_id\" as \"Missing\" and \"type\" as \"Missing\". \n\nWait, but the user's input has the `data_availability` as empty and `other_dataset_citations` as empty. So according to the rules, I should skip all tasks and output the \"Missing\" structure. Let me confirm the instructions again. Yes, if both sections are empty, skip tasks and output the missing structure. \n\nTherefore, the correct response is to return the JSON array with the missing dataset ID and type. The final answer should be in the specified format.\n</think>"
        ],
        [
         "10",
         "10.1002_cssc.202201821",
         null,
         "XML",
         "Text Content: {\"title\":\"Development of a Mg/O ReaxFF Potential to describe the Passivation Processes in Magnesium\\u2010Ion Batteries **\",\"author\":\"Fiesinger Florian\",\"abstract\":\"Abstract One of the key challenges preventing the breakthrough of magnesium-ion batteries (MIB) is the formation of a passivating boundary layer at the Mg anode. To describe the initial steps of Mg anode degradation by O 2 impurities, a Mg/O ReaxFF (force field for reactive systems) parameter set was developed capable of accurately modeling the bulk, surface, adsorption, and diffusion properties of metallic Mg and the salt MgO. It is shown that O 2 immediately dissociates upon first contact with the Mg anode (modeled as Mg(0001), Mg(10 1 \\u203e 0)A, and Mg(10 1 \\u203e 1)), heating the surface to several 1000 K. The high temperature assists the further oxidation and forms a rock salt interphase intersected by several grain boundaries. Among the Mg surface terminations, Mg(10 1 \\u203e 0)A is the most reactive, forming an MgO layer with a thickness of up to 25 \\u00c5. The trained force field can be used to model the ongoing reactions in Mg-air batteries but also to study the oxidation of magnesium metal in general.\",\"data_availability\":\"Data Availability Statement The data that support the findings of this study are openly available in zenodo at  https://doi.org/10.5281/zenodo.7074790 . Furthermore, the  KVIK  optimization routine is openly available in the GitHub repository at  https://github.com/shk11/KVIKOptimizer .\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': '10.5281/zenodo.7074790', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with a paper section and wants me to extract dataset IDs. First, I need to check the data_availability and other_dataset_citations sections.\n\nLooking at the data_availability, there's a DOI: 10.5281/zenodo.7074790. That looks like a dataset ID. The DOI format starts with 10., so that's a valid DOI. Also, the GitHub repository link is https://github.com/shk11/KVIKOptimizer. But the user's instructions say to only extract DOI-based dataset IDs. The GitHub link isn't a DOI, so maybe that's a secondary dataset. But the user's task is to extract dataset IDs, so even if it's a GitHub link, it's a dataset. However, the problem says to focus only on dataset IDs. Wait, the data_availability section here is the data availability statement. The DOI is part of that. So the DOI is valid. So dataset_id would be 10.5281/zenodo.7074790.\n\nNext, the other_dataset_citations is an empty list. So no other dataset citations. So only one dataset ID found. Now, I need to classify it as Primary or Secondary. The abstract mentions \"Mg/O ReaxFF\" and the data about the Mg anode degradation. The DOI is for the parameter set developed, which was used to model the reactions. Since the authors are Fiesinger Florian, and the data is related to the research, this is likely Primary. The other citation is empty, so no secondary. So the final JSON should have that dataset_id and type Primary.\n</think>"
        ],
        [
         "11",
         "10.1002_ece3.3985",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"High levels of polyandry, but limited evidence for multiple paternity, in wild populations of the western rock lobster ( Panulirus cygnus )\",\"author\":\"Loo Jacqueline\",\"abstract\":\"Abstract Polyandry, where multiple mating by females results in the temporal and spatial overlap of ejaculates from two or more males, is taxonomically widespread and occurs in varying frequencies within and among species. In decapods (crabs, lobsters, crayfish, and prawns), rates of polyandry are likely to be variable, but the extent to which patterns of multiple paternity reflect multiple mating, and thus are shaped by postmating processes that bias fertilization toward one or a subset of mated males, is unclear. Here, we use microsatellite markers to examine the frequency of multiple mating (the presence of spermatophores from two or more males) and patterns of paternity in wild populations of western rock lobster ( Panulirus cygnus ). Our data confirm that >45% of females had attached spermatophores arising from at least two males (i.e., confirming polyandry), but we found very limited evidence for multiple paternity; among 24 clutches sampled in this study, only two arose from fertilizations by two or more males. Single inferred paternal genotypes accounted for all remaining progeny genotypes in each clutch, including several instances when the mother had been shown to mate with two or more males. These findings highlight the need for further work to understand whether polyandry is adaptive and to uncover the mechanisms underlying postmating paternity biases in this system.\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON object with a paper section and wants me to extract dataset IDs. First, I need to check the data_availability and other_dataset_citations sections.\n\nLooking at the data_availability, it's an empty string. So I should skip Task 1. Then, Task 2 comes next. But since there are no dataset IDs in the other sections, I need to check if there are any. The other_dataset_citations is an empty list, so I also skip Task 1. \n\nWait, the user's input has data_availability as empty and other_dataset_citations as empty. According to the instructions, if both are empty, skip all tasks and output the \"Missing\" structure. So the final output should be a single JSON object with dataset_id and type both missing. \n\nI need to make sure I'm not missing any other sections. The title and author are there, but they don't relate to dataset IDs. The abstract mentions data availability but it's empty. So yes, the answer is the \"Missing\" structure.\n</think>"
        ],
        [
         "12",
         "10.1002_ece3.4466",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.r6nq870', 'type': 'Primary'}]",
         "XML",
         "Text Content: {\"title\":\"Incubation temperature and parental identity determine sex in the Australian agamid lizard  Ctenophorus pictus\",\"author\":\"Hansson Alexander\",\"abstract\":\"Abstract Sex determination in Australian agamid lizards shows a complex framework of different mechanisms, varying even among closely related taxa. It is clear that discrete classification of these species as either having genetic ( GSD ) or environmental sex determination ( ESD ) does not agree with empirical data. Although many species in this group show temperature-dependent sex determination ( TSD ), recent evidence suggests additional genetic or epigenetic effects. A proposed model explaining the adaptive significance and evolution of TSD in short-lived agamids predicts that selection will favor temperature-biased sex ratios in species with intense male-male competition. Here, we use experimental incubation at (near) constant temperatures to test whether the sex of Australian painted dragons ( Ctenophorus pictus ) is influenced by temperature, building on previous research yet to have reached an agreement regarding the role of temperature in this species. In this study, incubation temperature and parental identity affected hatchling sex suggesting that environment and genetics may work in concert to determine sex in this species. Although our results are consistent with TSD , our data cannot rule out a temperature-by-sex effect on egg or hatchling mortality. However, our findings together with the observed differences of sex determination systems in closely related species within this genus may provide novel opportunities to address fundamental questions in the evolution of sex determination systems.\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with a paper section and wants me to extract dataset IDs. First, I need to check the data_availability and other_dataset_citations sections.\n\nLooking at the input, the data_availability is an empty string, so I should check the other_dataset_citations. The other_dataset_citations array is empty, so no dataset IDs are found here. According to Task 1, if both sections are empty, I skip to Task 3. \n\nSince Task 1 didn't find any valid dataset IDs, I need to return the \"Missing\" JSON structure. The structure should have a single object with dataset_id as \"Missing\" and type as \"Missing\". \n\nI need to make sure I'm not adding any dataset IDs, just the missing one. Also, check if there are any other possible IDs, but in this case, there are none. So the final output is as specified.\n</think>"
        ],
        [
         "13",
         "10.1002_ece3.5260",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.2f62927', 'type': 'Primary'}]",
         "XML",
         "Text Content: {\"title\":\"Ultraconserved element (UCE) probe set design: Base genome and initial design parameters critical for optimization\",\"author\":\"Gustafson Grey T.\",\"abstract\":\"Abstract Targeted capture and enrichment approaches have proven effective for phylogenetic study. Ultraconserved elements (UCEs) in particular have exhibited great utility for phylogenomic analyses, with the software package phyluce being among the most utilized pipelines for UCE phylogenomics, including probe design. Despite the success of UCEs, it is becoming increasing apparent that diverse lineages require probe sets tailored to focal taxa in order to improve locus recovery. However, factors affecting probe design and methods for optimizing probe sets to focal taxa remain underexplored. Here, we use newly available beetle (Coleoptera) genomic resources to investigate factors affecting UCE probe set design using phyluce . In particular, we explore the effects of stringency during initial design steps, as well as base genome choice on resulting probe sets and locus recovery. We found that both base genome choice and initial bait design stringency parameters greatly alter the number of resultant probes included in final probe sets and strongly affect the number of loci detected and recovered during in silico testing of these probe sets. In addition, we identify attributes of base genomes that correlated with high performance in probe design. Ultimately, we provide a recommended workflow for using phyluce to design an optimized UCE probe set that will work across a targeted lineage, and use our findings to develop a new, open-source UCE probe set for beetles of the suborder Adephaga.\",\"data_availability\":\"DATA AVAILABILITY DNA sequences: GenBank MK838494 to MK838511;  phyluce  BED/BAM files for probe design experiments and Adephaga_2.9Kv1 final probe set fasta file: Dryad  https://doi.org/10.5061/dryad.2f62927 ; Custom scripts: Alexander 2018a: Comparing_monolithic_UCE_fastas available from;  https://github.com/laninsky/comparing_monolithic_UCE_fastas ; Alexander 2018b: Phase_target available from  https://github.com/laninsky/reference_aligning_to_established_loci/tree/master/phase_target ; Alexander 2018c: distance_calcs available from:  https://github.com/laninsky/distance_calcs ; Alexander 2018d: Phase_everyone available from  https://github.com/laninsky/reference_aligning_to_established_loci/tree/master/phase_everyone .\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': '10.5061/dryad.2f62927', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON string representing a paper section and wants me to extract dataset IDs and classify them as Primary or Secondary. \n\nFirst, I need to check the data_availability and other_dataset_citations sections. The data_availability has entries like DNA sequences and files. The key here is to look for DOI or Accession IDs. \n\nLooking at the data_availability, there's a mention of \"phyluce BED/BAM files for probe design experiments and Adephaga_2.9Kv1 final probe set fasta file: Dryad\". The Dryad DOI is 10.5061/dryad.2f62927. That looks like a DOI. Then there's a link to a GitHub repository for scripts, which might be a URL, not a DOI. \n\nNext, the other_dataset_citations section is empty. So, only the Dryad DOI is valid. Now, I need to check if this is a dataset. The Dryad DOI is a dataset identifier, so it's a DOI. \n\nNow, applying the classification logic. The abstract mentions using phyluce for UCE probe design, which is a software. The data_availability includes a Dryad DOI, which is a dataset. The citation context doesn't mention people, so it's Secondary. \n\nWait, but the classification depends on whether the data was originally generated by the authors. The authors are Gustafson Grey T. The Dryad DOI is for a dataset, so if the data was generated by the authors, it's Primary. But the abstract says \"we use newly available beetle genomic resources...\" which suggests that the data was acquired from external sources. \n\nSo, the data_availability has a Dryad DOI, which is a dataset. The citation context doesn't mention people, so it's Secondary. Therefore, the dataset_id is Primary. \n\nBut wait, the classification depends on the origin. If the data was generated by the authors, it's Primary. If it's from an external source, Secondary. The abstract mentions using beetle genomic resources, which might imply that the data was acquired from an external source. \n\nSo, the dataset_id is Primary. The Dryad DOI is a valid dataset ID. The other_dataset_citations is empty, so no more citations. \n\nTherefore, the final answer should include the Dryad DOI as a Primary dataset.\n</think>"
        ],
        [
         "14",
         "10.1002_ece3.5395",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"\\u201cCloser\\u2010to\\u2010home\\u201d strategy benefits juvenile survival in a long\\u2010distance migratory bird\",\"author\":\"Cheng Yachang\",\"abstract\":\"Abstract Human-induced changes in the climate and environment that occur at an unprecedented speed are challenging the existence of migratory species. Faced with these new challenges, species with diverse and flexible migratory behaviors may suffer less from population decline, as they may be better at responding to these changes by altering their migratory behavior. At the individual level, variations in migratory behavior may lead to differences in fitness and subsequently influence the population's demographic dynamics. Using lifetime GPS bio-logging data from 169 white storks ( Ciconia ciconia ), we explore whether the recently shortened migration distance of storks affects their survival during different stages of their juvenile life. We also explore how other variations in migratory decisions (i.e., time, destination), movement activity (measured using overall body dynamic acceleration), and early life conditions influence juvenile survival. We observed that their first autumn migration was the riskiest period for juvenile white storks. Individuals that migrated shorter distances and fledged earlier experienced lower mortality risks. In addition, higher movement activity and overwintering 'closer-to-home' (with 84.21% of the tracked individuals stayed Europe or North Africa) were associated with higher survival. Our study shows how avian migrants can change life history decisions over only a few decades, and thus it helps us to understand and predict how migrants respond to the rapidly changing world.\",\"data_availability\":\"DATA ACCESSIBILITY The data used for this study are available through the Movebank Data Repository ( https://www.movebank.org ): with  https://doi.org/10.5441/001/1.v1cs4nn0 ,  https://doi.org/10.5441/001/1.c42j3js7 ,  https://doi.org/10.5441/001/1.4192t2j4 ,  https://doi.org/10.5441/001/1.ck04mn78 ,  https://doi.org/10.5441/001/1.71r7pp6q  (Fiedler, Flack, Sch\\u00e4fle, et al.,  2019 ; Fiedler, Flack, Schmid, Reinhard, & Wikelski,  2019 ; Fiedler, Hilsendegen, et al.,  2019 ; Fiedler, Leppelsack, et al.,  2019 ; Fiedler, Niederer, Sch\\u00f6nenberger, Flack, & Wikelski,  2019 ).\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': '10.12345/12345', 'type': 'Primary'}, {'dataset_id': '10.5441/001/1.v1cs4nn0', 'type': 'Primary'}, {'dataset_id': '10.5441/001/1.c42j3js7', 'type': 'Primary'}, {'dataset_id': '10.5441/001/1.4192t2j4', 'type': 'Primary'}, {'dataset_id': '10.5441/001/1.ck04mn78', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON string representing a paper section, and I need to extract dataset IDs and classify them as Primary or Secondary. Let me start by reading through the input carefully.\n\nFirst, the title is \"Closer to home strategy benefits juvenile survival in a long-distance migratory bird\". The author is Cheng Yachang. The abstract mentions data from 169 white storks using GPS data. The data_availability section lists several DOIs. The other_dataset_citations is empty.\n\nLooking at the data_availability, there are five DOIs listed. Each has a DOI format. The important rule is to check if the DOI is a dataset_id. The DOI starts with \"10.\" and has a 4 or 5-digit number. The examples given in the key definitions include DOIs like 10.12345/12345. The user's task is to extract only valid dataset IDs. So, I need to check each DOI to see if it's a dataset_id. The DOI format is correct, so all these DOIs are valid dataset IDs.\n\nNext, the other_dataset_citations is an empty list, so no citations to consider. Now, I need to classify each dataset_id as Primary or Secondary. The classification depends on the abstract and citation context. The abstract mentions that the data was used for this study, and the citations are empty. The keywords in the abstract include \"survival\", \"migratory\", \"data release\", etc. \n\nFor each dataset_id, I need to check if the specific data product (like the stork data) was generated by the authors of this paper. The abstract doesn't mention any specific data product being generated by the authors, but it does state that the data was used. Since the data_availability section includes the DOIs, and the other_citations are empty, the data is from the study. \n\nNow, applying the classification logic. The abstract doesn't explicitly state if the data was generated by the authors, but the data_availability includes the DOI. The DOI is a valid dataset_id. The classification as Primary or Secondary depends on whether the data was generated by the authors. Since the abstract doesn't mention the authors directly, but the data_availability includes the DOI, which is a dataset_id, I need to check if the DOI refers to a dataset. The DOI format is correct, so all these DOIs are valid dataset IDs. \n\nSince the data_availability includes the DOIs, and the other_citations are empty, the data is from the study. The classification as Primary or Secondary would depend on whether the data was generated by the authors. The abstract doesn't explicitly state that, but the data_availability includes the DOI. \n\nWait, the key point is to check if the specific data product (the stork data) was generated by the authors. The abstract mentions that the data was used for this study, but not the authors. However, the data_availability includes the DOI, which is a dataset_id. The classification as Primary or Secondary would be based on whether the data was generated by the authors. Since the abstract doesn't mention the authors, but the data_availability includes the DOI, which is a dataset_id, I need to check if the DOI refers to a dataset. \n\nIn this case, all the DOIs listed are valid dataset IDs. The classification as Primary or Secondary would depend on whether the data was generated by the authors. Since the abstract doesn't mention the authors, but the data_availability includes the DOI, which is a dataset_id, I need to check if the DOI refers to a dataset. \n\nThe answer here is that all the DOIs are valid dataset IDs. Since the data_availability includes the DOIs, and the other_citations are empty, the data is from the study. The classification as Primary or Secondary would be based on whether the data was generated by the authors. Since the abstract doesn't mention the authors, but the data_availability includes the DOI, which is a dataset_id, I need to check if the DOI refers to a dataset. \n\nIn this case, all the DOIs are valid dataset IDs. Therefore, the classification for each dataset_id would be Primary, as they are directly related to the study's findings. However, the abstract doesn't explicitly state that the data was generated by the authors, but the data_availability includes the DOI, which is a dataset_id. \n\nWait, the classification logic says that if the data is from the authors, it's Primary. If from external sources, Secondary. Since the data_availability includes the DOI, which is a dataset_id, and the other_citations are empty, the data is from the study. Therefore, the classification is Primary. \n\nSo, the final answer would include all the valid dataset IDs as Primary. The other_dataset_citations are empty, so no need to process them. The data_availability includes the DOIs, which are valid dataset IDs. Therefore, the JSON array would have all the dataset_ids classified as Primary.\n</think>"
        ],
        [
         "15",
         "10.1002_ece3.6144",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.zw3r22854', 'type': 'Primary'}]",
         "XML",
         "Text Content: {\"title\":\"Efficacy of metabarcoding for identification of fish eggs evaluated with mock communities\",\"author\":\"Duke Elena M.\",\"abstract\":\"Abstract There is urgent need for effective and efficient monitoring of marine fish populations. Monitoring eggs and larval fish may be more informative than that traditional fish surveys since ichthyoplankton surveys reveal the reproductive activities of fish populations, which directly impact their population trajectories. Ichthyoplankton surveys have turned to molecular methods (DNA barcoding & metabarcoding) for identification of eggs and larval fish due to challenges of morphological identification. In this study, we examine the effectiveness of using metabarcoding methods on mock communities of known fish egg DNA. We constructed six mock communities with known ratios of species. In addition, we analyzed two samples from a large field collection of fish eggs and compared metabarcoding results with traditional DNA barcoding results. We examine the ability of our metabarcoding methods to detect species and relative proportion of species identified in each mock community. We found that our metabarcoding methods were able to detect species at very low input proportions; however, levels of successful detection depended on the markers used in amplification, suggesting that the use of multiple markers is desirable. Variability in our quantitative results may result from amplification bias as well as interspecific variation in mitochondrial DNA copy number. Our results demonstrate that there remain significant challenges to using metabarcoding for estimating proportional species composition; however, the results provide important insights into understanding how to interpret metabarcoding data. This study will aid in the continuing development of efficient molecular methods of biological monitoring for fisheries management.\",\"data_availability\":\"DATA AVAILABILITY STATEMENT Data are available at Dryad Digital Repository at:  https://doi.org/10.5061/dryad.zw3r22854 . Two sequences used in our reference library are on GenBank (accession numbers MH714866 and MH718435).\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': '10.5061/dryad.zw3r22854', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON string representing a paper section, and I need to extract dataset IDs and classify them as Primary or Secondary. Let me start by understanding the input.\n\nFirst, looking at the `data_availability` section, there's a DOI: 10.5061/dryad.zw3r22854. The DOI is a Digital Object Identifier, which is a type of dataset ID. According to the key definitions, DOIs are dataset IDs only if they are explicitly used as such. The citation context here mentions Dryad Digital Repository, which is a data repository. So this should be a valid dataset ID.\n\nNext, the `other_dataset_citations` section is empty. So no other citations are provided. Now, I need to check if this DOI is actually a dataset. The DOI is correctly formatted, starts with 10., and the suffix includes letters and numbers. The key validation rule says to look for surrounding terms like \"data release\" or \"dataset\" in the data_availability section. The data_availability text mentions \"data are available at Dryad Digital Repository\" and the DOI is part of that. So this is valid.\n\nNow, the classification. The abstract mentions using metabarcoding for fish egg identification. The DOI is from Dryad, which is a repository. The data_availability section also mentions two sequences from GenBank. But the task is to extract dataset IDs. The DOI here is the dataset ID. Since the DOI is used as a dataset ID, and the citation context refers to the repository, this should be Primary. The other dataset citations are empty, so no Secondary entries.\n\nWait, but the user's task says to classify only valid dataset IDs. The DOI is valid, so it's Primary. The other dataset citations are empty, so no Secondary. Therefore, the output should include this dataset ID as Primary.\n</think>"
        ],
        [
         "16",
         "10.1002_ece3.6303",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.37pvmcvgb', 'type': 'Primary'}]",
         "XML",
         "Text Content: {\"title\":\"Can polyploidy confer invasive plants with a wider climatic tolerance? A test using  Solidago canadensis\",\"author\":\"Wan Jizhong\",\"abstract\":\"Abstract Polyploidy can cause variation in plant functional traits and thereby generate individuals that can adapt to fluctuating environments and exploit new environments. However, few empirical studies have tested for an association between ploidy level and climatic tolerance of invasive cytotypes relative to conspecific native-range cytotypes. Here, we used an invasive plant Solidago canadensis to test whether invasive populations had a higher proportion of polyploids, greater height and stem-base diameter, and occupied a wider range of climatic conditions than conspecific native-range populations. We also tested whether the invasive populations had overcome genetic founder effects. We sampled a total of 80 populations in parts of the invaded range in China and native range in North America for in situ measurements of plant height and stem-base diameter in the field and for population genetic and cytotype analyses. To examine climatic correlates, we augmented our field-sampled data with occurrence records obtained from Global Biodiversity Information Facility. All, except one, of the populations that we sampled in China occurred in a humid subtropical climate. In contrast, the North American populations occurred in humid continental, humid subtropical, and semi-arid climatic zones. All populations of S. canadensis in China were purely hexaploid, while the North American populations were diploid, tetraploid, and hexaploid. The invasive hexaploids were significantly taller and had a larger stem-base diameter than native hexaploids. Native hexaploids were significantly taller and had larger stem-base diameter than native diploids. Climatic correlate assessment found that invasive and native populations occupied different climatic envelopes, with invasive populations occurring in warmer and less seasonal climates than native populations. However, there was no significant correlation between ploidy level and climatic envelope of S. canadensis . Molecular phylogeo\",\"data_availability\":\"DATA AVAILABILITY STATEMENT DNA sequences have been deposited in the National Center for Biotechnology Information (NCBI) database (accession numbers: MN107084\\u2013MN107151, MN335287\\u2013MN335290). The data that support the findings of this study have been deposited in Dryad with doi: https://doi.org/10.5061/dryad.37pvmcvgb .\",\"other_dataset_citations\":[]}\n",
         "[]",
         ""
        ],
        [
         "17",
         "10.1002_ece3.6784",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"Ecomorphology of the pectoral girdle in anurans (Amphibia, Anura): Shape diversity and biomechanical considerations\",\"author\":\"Engelkes Karolin\",\"abstract\":\"Abstract Frogs and toads (Lissamphibia: Anura) show a diversity of locomotor modes that allow them to inhabit a wide range of habitats. The different locomotor modes are likely to be linked to anatomical specializations of the skeleton within the typical frog Bauplan. While such anatomical adaptations of the hind limbs and the pelvic girdle are comparably well understood, the pectoral girdle received much less attention in the past. We tested for locomotor-mode-related shape differences in the pectoral girdle bones of 64 anuran species by means of micro-computed-tomography-based geometric morphometrics. The pectoral girdles of selected species were analyzed with regard to the effects of shape differences on muscle moment arms across the shoulder joint and stress dissipation within the coracoid. Phylogenetic relationships, size, and locomotor behavior have an effect on the shape of the pectoral girdle in anurans, but there are differences in the relative impact of these factors between the bones of this skeletal unit. Remarkable shape diversity has been observed within locomotor groups indicating many-to-one mapping of form onto function. Significant shape differences have mainly been related to the overall pectoral girdle geometry and the shape of the coracoid. Most prominent shape differences have been found between burrowing and nonburrowing species with headfirst and backward burrowing species significantly differing from one another and from the other locomotor groups. The pectoral girdle shapes of burrowing species have generally larger moment arms for (simulated) humerus retractor muscles across the shoulder joint, which might be an adaptation to the burrowing behavior. The mechanisms of how the moment arms were enlarged differed between species and were associated with differences in the reaction of the coracoid to simulated loading by physiologically relevant forces.\",\"data_availability\":\"DATA AVAILABILITY STATEMENT CT volumes can be downloaded from  https://www.fdr.uni\\u2010hamburg.de/search?page=1&size=20&q=keywords:%22pectoral%20girdle%20morphometrics%20project%22 ; DOI numbers are provided in Appendix S1: Table A1.\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.1007/s00435-017-0386-3\"],\"citation_context\":\"Engelkes , K. , Friedrich , F. , Hammel , J. U. , & Haas , A. ( 2018 ). A simple setup for episcopic microtomy and a digital image processing workflow to acquire high-quality volume data and 3D surface models of small vertebrates . Zoomorphology , 137 ( 1 ), 213 - 228 . 10.1007/s00435-017-0386-3\"}]}\n",
         "[{'dataset_id': '10.1007/s00435-017-0386-3', 'type': 'Primary'}, {'dataset_id': '10.1007/s00435-0386-3', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with a paper section, and I need to extract dataset IDs and classify them as Primary or Secondary. Let me start by reading through the input.\n\nFirst, the title is about the pectoral girdle in anurans. The author is Karolin Engelkes. The abstract mentions using micro-computed tomography (CT) morphometrics to analyze shape differences in the pectoral girdle bones. The data availability section links to a website and provides a DOI in Appendix S1. The other dataset citations have a dataset ID and a citation context.\n\nNow, I need to check if any dataset IDs are present. The data_availability has a DOI in Appendix S1. The other_dataset_citations list has a dataset_id with a DOI. Let me verify the DOI format. The DOI is 10.1007/s00435-017-0386-3. The format starts with 10., which matches the required format. The citation context mentions the authors, so it's a valid dataset. \n\nNext, I need to classify these as Primary or Secondary. For the DOI, the classification depends on the context. The abstract mentions the study's purpose, which is related to the pectoral girdle's shape and biomechanics. The citation context includes the authors, which is a direct reference to the authors of the paper. Since the first person listed is the author, this indicates that the dataset was generated by the authors of this paper. Therefore, it's Primary.\n\nThe other dataset citation also has a DOI. The same logic applies here. The authors are the ones who created the dataset, so it's Primary. \n\nI need to make sure there are no other dataset IDs. The data_availability section mentions the DOI, and the other citations also have a dataset_id. Both are valid and correctly formatted. \n\nNow, I should structure the output. Since both dataset IDs are valid, I need to return them as Primary. The JSON structure should have each dataset_id with the type. Let me double-check the classification logic to ensure I'm not making a mistake. The key points are that the authors are the ones generating the data, and the citation context confirms that. \n\nYes, both dataset IDs are Primary. The final output should be an array with both entries.\n</think>"
        ],
        [
         "18",
         "10.1002_ece3.961",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"The effects of insects, nutrients, and plant invasion on community structure and function above-and belowground\",\"author\":\"Wright Phoebe\",\"abstract\":\"Soil nutrient availability, invasive plants, and insect presence can directly alter ecosystem structure and function, but less is known about how these factors may interact. In this 6-year study in an old-field ecosystem, we manipulated insect abundance (reduced and control), the propagule pressure of an invasive nitrogen-fixing plant (propagules added and control), and soil nutrient availability (nitrogen added, nitrogen reduced and control) in a fully crossed, completely randomized plot design. We found that nutrient amendment and, occasionally, insect abundance interacted with the propagule pressure of an invasive plant to alter above-and belowground structure and function at our site. Not surprisingly, nutrient amendment had a direct effect on aboveground biomass and soil nutrient mineralization. The introduction of invasive nitrogen-fixing plant propagules interacted with nutrient amendment and insect presence to alter soil bacterial abundance and the activity of the microbial community. While the larger-scale, longer-term bulk measurements such as biomass production and nutrient mineralization responded to the direct effects of our treatments, the shorter-term and dynamic microbial communities tended to respond to interactions among our treatments. Our results indicate that soil nutrients, invasive plants, and insect herbivores determine both above-and belowground responses, but whether such effects are independent versus interdependent varies with scale.\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with a paper section and wants me to extract dataset IDs. First, I need to check the data_availability and other_dataset_citations sections.\n\nLooking at the input, the data_availability is an empty string, so I should check the other_dataset_citations. The other_dataset_citations array is empty, so no dataset IDs are found here. According to Task 1, if both sections are empty, I skip to Task 3. \n\nSince Task 1 didn't find any valid dataset IDs, I need to return the \"Missing\" JSON structure. The structure should have a single object with dataset_id as \"Missing\" and type as \"Missing\". \n\nI need to make sure I'm not adding any dataset IDs and that the classification is based on the provided information. Since there are no dataset IDs in either section, the output is correct.\n</think>"
        ],
        [
         "19",
         "10.1002_ece3.9627",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.b8gtht7h3', 'type': 'Primary'}]",
         "XML",
         "Text Content: {\"title\":\"Inferring predator\\u2013prey interactions from camera traps: A\\u00a0Bayesian co\\u2010abundance modeling approach\",\"author\":\"Amir Zachary\",\"abstract\":\"Abstract Predator-prey dynamics are a fundamental part of ecology, but directly studying interactions has proven difficult. The proliferation of camera trapping has enabled the collection of large datasets on wildlife, but researchers face hurdles inferring interactions from observational data. Recent advances in hierarchical co-abundance models infer species interactions while accounting for two species' detection probabilities, shared responses to environmental covariates, and propagate uncertainty throughout the entire modeling process. However, current approaches remain unsuitable for interacting species whose natural densities differ by an order of magnitude and have contrasting detection probabilities, such as predator-prey interactions, which introduce zero inflation and overdispersion in count histories. Here, we developed a Bayesian hierarchical N-mixture co-abundance model that is suitable for inferring predator-prey interactions. We accounted for excessive zeros in count histories using an informed zero-inflated Poisson distribution in the abundance formula and accounted for overdispersion in count histories by including a random effect per sampling unit and sampling occasion in the detection probability formula. We demonstrate that models with these modifications outperform alternative approaches, improve model goodness-of-fit, and overcome parameter convergence failures. We highlight its utility using 20 camera trapping datasets from 10 tropical forest landscapes in Southeast Asia and estimate four predator-prey relationships between tigers, clouded leopards, and muntjac and sambar deer. Tigers had a negative effect on muntjac abundance, providing support for top-down regulation, while clouded leopards had a positive effect on muntjac and sambar deer, likely driven by shared responses to unmodelled covariates like hunting. This Bayesian co-abundance modeling approach to quantify predator-prey relationships is widely applicable across species, ecosystems\",\"data_availability\":\"DATA AVAILABILITY STATEMENT All R code and data used to implement our Bayesian co\\u2010abundance models can be accessed at this Figshare repository:  https://doi.org/10.5061/dryad.b8gtht7h3 .\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': '10.5061/dryad.b8gtht7h3', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON string with a paper section and wants me to extract dataset IDs and classify them as Primary or Secondary. \n\nFirst, I need to check the data_availability and other_dataset_citations sections. The data_availability has a DOI: 10.5061/dryad.b8gtht7h3. The other_dataset_citations is an empty list. \n\nSo, according to Task 1, I should check the data_availability first. The DOI is valid and refers to a dataset. The DOI format is 10.xxxx, which matches the definition of a dataset_id. The citation context here is the repository link, which is a valid dataset. \n\nNow, Task 2: Classify the dataset type. The DOI is used in the data_availability section. The abstract mentions using Bayesian models, which are co-abundance models. The key indicators for Primary are original generation, and the DOI is for a new dataset. The other_dataset_citations is empty, so no secondary citations. \n\nThe fallback rule says if ambiguous, default to Primary. Since the DOI is clearly a dataset, and the data_availability is valid, the classification should be Primary. \n\nSo, the final JSON should include the dataset_id from the DOI and type Primary.\n</think>"
        ],
        [
         "20",
         "10.1002_ecs2.1280",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.p3fg9', 'type': 'Primary'}]",
         "PDF",
         "Text Content: {\"title\":\"\",\"author\":\"Haley A. Catton,,\\u2020 Robert G. Lalonde,1\",\"abstract\":\"Abstract. Understanding the impact of herbivory on plant populations is a fundamental goal of ecology. Damage to individual plants can be visually striking and affect the fates of individuals, but these impacts do not necessarily translate into population-\\u00adlevel differences in vital rates (survival, growth, or fecundity) or population growth rates. In biological control of weeds, quantitative assessments of population-\\u00adlevel impacts of released agents on both target invasive plants and native, nontarget plants are needed to inform evaluations of the benefits and risks of releasing agents into new regions. Here we present a 3-\\u00adyr exper-imental demographic field study using the European root-\\u00adfeeding biocontrol weevil, Mogulones crucifer, first released in Canada in 1997 to control the invasive weed Cynoglossum officinale (Boraginaceae). Mogu-lones crucifer is an effective 'search and destroy' agent in Canada, but sporadically feeds, oviposits, and develops on native nontarget Boraginaceae. We investigated the population-\\u00adlevel impacts of this biocon-trol insect on its target weed and a native nontarget plant, Hackelia micrantha (Boraginaceae), by releasing large numbers of weevils into naturally occurring patches of H. micrantha growing isolated from or inter-spersed with C. officinale. We followed the fates of individual plants on release and nonrelease (control) sites for two transition years, developed matrix models to project population growth rates (\\u03bb) for each plant species, and examined the contributions from differences in vital rates to changes in \\u03bb using life table response experiments (LTRE). In contrast to studies of the insect-plant interaction in its native range, as a biocontrol agent, M. crucifer increased mortality of C. officinale rosettes in the year immediately following release, depressing the weed's \\u03bb to below the population replacement level. However, \\u03bb for H. micrantha was never depressed below the replacement level, and any differences between release and nonrelease sites in the nontarget could not be explained by significant contributions from vital rates in the LTRE. This study is the first to simultaneously and experimentally examine target and nontarget population-\\u00adlevel im-pacts of a weed biocontrol insect in the field, and supports the theoretical prediction that plant life history characteristics and uneven herbivore host preferences can interact to produce differences in population-\\u00ad level impacts between target and nontarget plant species.\",\"data_availability\":\"Data associated with this paper have been deposited in Dryad: http://dx.doi.org/10.5061/dryad.p3fg9\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.1002/ecs2.1280\"],\"citation_context\":\"21508925, 2016, 5, Downloaded from https://esajournals.onlinelibrary.wiley.com/doi/10.1002/ecs2.1280 by California Digital Library University Of California, Wiley Online Library on [31/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"}]}\n",
         "[{'dataset_id': '10.5061/dryad.p3fg9', 'type': 'Primary'}, {'dataset_id': '10.1002/ecs2.1280', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON string representing a paper section and wants me to extract dataset IDs and classify them as Primary or Secondary. \n\nFirst, I need to check the data_availability section. The data_availability here is \"Data associated with this paper have been deposited in Dryad: http://dx.doi.org/10.5061/dryad.p3fg9\". Dryad is a repository for datasets, so this is a valid dataset. The DOI here is 10.5061/dryad.p3fg9. But wait, the important DOI validation rule says only extract DOIs that are explicitly used as dataset IDs. Since this is a Dryad DOI, it's a dataset ID. So that's a valid dataset.\n\nNext, the other_dataset_citations section has a citation with dataset_ids [\"10.1002/ecs2.1280\"] and a citation context. The DOI here is 10.1002/ecs2.1280. Again, this is a valid dataset ID. So both of these are valid dataset IDs.\n\nNow, I need to classify them. For the first dataset, the citation context mentions \"21508925, 2016, 5, Downloaded from...\" which seems to be a citation, not a dataset. So the dataset_id is 10.5061/dryad.p3fg9. The citation context here is about a journal article, not a dataset. So this is a valid dataset ID but not a citation for the dataset. \n\nThe second dataset has the DOI 10.1002/ecs2.1280. The citation context is about a journal article, so again, not a dataset. So both are valid dataset IDs but not citations for the dataset. \n\nNow, looking at the abstract, there's a mention of \"data release\" and \"dataset availability\". The data_availability section is about the dataset being deposited in Dryad, which is a data repository. The other_dataset_citations have their own citations, but the dataset IDs themselves are valid. \n\nSo, both dataset IDs are valid. Now, according to the classification logic, I need to check if they are Primary or Secondary. The abstract mentions that the data is for a field study, which is a primary study. The data_availability is about the dataset being deposited, which is a primary data release. The other_dataset_citations are citations for the dataset, not the dataset itself. \n\nTherefore, both dataset IDs are Primary. The output should be two entries in the JSON array, each with the dataset_id and type \"Primary\".\n</think>"
        ],
        [
         "21",
         "10.1002_ecs2.4619",
         "[{'dataset_id': 'https://doi.org/10.25349/d9qw5x', 'type': 'Primary'}]",
         "XML",
         "Text Content: {\"title\":\"Interactions between temperature and predation impact insect emergence in alpine lakes\",\"author\":\"Caroline H Owens\",\"abstract\":\"While insect populations are simultaneously threatened by many local-and global-scale stressors, the interacting effects of these factors remain poorly understood. These interactions between stressors, whether additive or antagonistic, may have profound effects on our assumptions about and predictions for any given system. Here we address this gap by exploring the interactive effects of introduced predators and elevated temperatures on emerging aquatic insects across alpine lakes. Using a crossed factorial field survey, we examine whether warmer temperatures either directly affect the magnitude or diversity of insect emergence, or mediate the impacts of predation via interactions with predator presence. Based on data from more than 11,000 insects collected across a two-year period, we find a 71% reduction in insect emergence associated with an additional 3.6 C of water temperature in oligotrophic alpine lakes. While our work confirms prior findings that predator presence drives strong reductions in insect emergence, we find that the effects of predation are significantly weaker in warmer lakes (2% reduction in warmest lakes studied vs. 75% reduction in coldest). Combining our results with regional studies of predator communities to identify mechanisms suggests that the interaction effects between temperature and predator presence are likely due to changes in predator behavior across temperatures. Critically, to fully understand the multiple stressors that impact insect populations and predict their future effects, we must consider the interactions between stressors across spatial scales.\",\"data_availability\":\"\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.25349/D9QW5X\"],\"citation_context\":\"Data for: Interactions between Temperature and Predation Impact Insect Emergence in Alpine Lakes|Dryad. Dataset|10.25349/D9QW5X\"}]}\n",
         "[{'dataset_id': '10.25349/D9QW5X', 'type': 'Secondary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON string representing a paper section, and I need to extract dataset IDs and classify them as Primary or Secondary. Let me start by reading through the input carefully.\n\nFirst, the title is \"Interactions between temperature and predation impact insect emergence in alpine lakes\". The author is Caroline H Owens. The abstract mentions that the paper explores the interactive effects of predators and temperature on insect emergence. The data_availability is empty, so I need to check the other_dataset_citations.\n\nLooking at the other_dataset_citations, there's a dataset with ID \"10.25349/D9QW5X\" and a citation context. The citation context says \"Data for: Interactions between Temperature and Predation Impact Insect Emergence in Alpine Lakes|Dryad. Dataset|10.25349/D9QW5X\". \n\nNow, I need to check if this dataset_id is a DOI or an Accession ID. The DOI format starts with \"10.\" and has a suffix. The given ID is \"10.25349/D9QW5X\". The suffix here is \"D9QW5X\", which looks like an Accession ID. The key point is to apply the important DOI validation rule. The citation context mentions \"Dryad\" and \"Dataset\", which are common for datasets. \n\nSince the citation context clearly refers to a dataset, and the data_availability is empty, this seems like a valid dataset. Now, I need to check if this dataset is Primary or Secondary. The abstract mentions that the effects of predation are weaker in warmer lakes, which suggests that the data might be from an existing dataset. However, the citation context here is about the interaction between temperature and predation, which is the focus of the paper. \n\nThe author is Caroline H Owens, and the dataset is part of the paper's findings. The citation context doesn't mention the author of the paper, so it's more likely Secondary. The data_availability is empty, so no other data sources are mentioned. \n\nTherefore, the dataset_id \"10.25349/D9QW5X\" is a Secondary dataset because it's from an existing source (Dryad) and the abstract indicates that the effects are due to interactions between temperature and predation, which is the focus of the paper. \n\nI should format the answer accordingly. Since there's one valid dataset, the output should be a JSON array with that dataset. No other dataset IDs are present, so the result is just that one entry.\n</think>"
        ],
        [
         "22",
         "10.1002_ejic.201900904",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"\",\"author\":\"\",\"abstract\":\"Chemical properties and biological activity of Au(III) compounds obtained from dichlorido[2-(dimethylaminomethyl)phenyl-C 1 ,N]gold(III), [Au(damp-C 1 ,N)Cl 2 ], and halogenated, potentially tridentate thiosemicarbazones have been studied. The results of this work show that the complexation of the halogenated thiosemicarbazones with Au(III) enhances their stability against hydrolysis and retains or enhances their anti-parasitic activity. Fluorination in the periphery of the ligands has expectedly no influence on the structural chemistry of the obtained Au(III) complexes, but modulates their biological behaviour. Best results with a remarkably high selectivity index for the trypomastigote form of Trypanosoma cruzi were obtained with the complex containing the ligand, which presents a 3,5-fluorine substitution in meta-position of an aromatic ring, [Au(dampH)(L-3,5-F)]Cl.\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with some paper sections, and I need to extract dataset IDs. First, I'll check the data_availability and other_dataset_citations sections. The data_availability is empty, so I'll look at the other_dataset_citations. There are no entries there, so maybe there are no dataset IDs. But wait, the abstract mentions \"data release\" and \"dataset\", so maybe there's a dataset in the abstract. Let me check the abstract again.\n\nThe abstract says: \"Chemical properties and biological activity of Au(III) compounds obtained from dichlorido[2-(dimethylaminomethyl)phenyl-C 1 ,N]gold(III), [Au(damp-C 1 ,N)Cl 2 ], and halogenated, potentially tridentate thiosemicarbazones have been studied. The results of this work show that the complexation of the halogenated thiosemicarbazones with Au(III) enhances their stability against hydrolysis and retains or enhances their anti-parasitic activity. Fluorination in the periphery of the ligands has expectedly no influence on the structural chemistry of the obtained Au(III) complexes, but modulates their biological behaviour. Best results with a remarkably high selectivity index for the trypomastigote form of Trypanosoma cruzi were obtained with the complex containing the ligand, which presents a 3,5-fluorine substitution in meta-position of an aromatic ring, [Au(dampH)(L-3,5-F)]Cl.\"\n\nHmm, the abstract mentions \"data release\" and \"dataset\", but the data_availability is empty. The other_dataset_citations are empty. So according to Task 1, no dataset IDs were found. Then, Task 3 would output a \"Missing\" JSON structure. But wait, maybe the abstract contains a dataset ID? The abstract doesn't mention any specific dataset IDs. The data_availability is empty, so no. Therefore, the answer should be a missing dataset.\n</think>"
        ],
        [
         "23",
         "10.1002_ejoc.202000139",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "PDF",
         "Text Content: {\"title\":\"\",\"author\":\"Ann Christin Reiers\\u00f8lmoen[a\",\"abstract\":\"Abstract: Studies on gold(III) coordination of a series of pre-pared polydentate pyridine and quinoline based ligands are re-ported. Characterization (1H, 13C, 15N NMR, and XRD) of the novel gold(III) complexes, prepared in 31-98 % yield, revealed different coordination ability of the pyridine and quinoline nitrogen atoms. Testing of catalytic activity in cyclopropanation\",\"data_availability\":\"10990690, 2020, 19, Downloaded from https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ejoc.202000139 by California Digital Library University Of California, Wiley Online Library on [09/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\",\"other_dataset_citations\":[]}\n",
         "[]",
         ""
        ],
        [
         "24",
         "10.1002_ejoc.202000916",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "XML",
         "Text Content: {\"title\":\"Ammonium Salt\\u2010Catalyzed Ring\\u2010Opening of Aryl\\u2010Aziridines with \\u03b2\\u2010Keto Esters\",\"author\":\"Haider Victoria\",\"abstract\":\"We herein report an ammonium salt-catalyzed protocol for the regioselective ring opening of aryl-aziridines with \\u03b2-keto esters. The reaction gives access to a variety of highly functionalized target molecules with two consecutive stereogenic centers and can be rendered enantioselective (up to e.r . = 91:9) by using bifunctional chiral ammonium salt catalysts.\",\"data_availability\":\"\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with some paper sections, and I need to extract dataset IDs. The task is to identify and classify them as Primary or Secondary based on the given rules.\n\nFirst, looking at the input data: the `data_availability` section is empty, and the `other_dataset_citations` is also empty. According to the instructions, if both are empty, I should skip all tasks and output a \"Missing\" JSON structure. But in this case, both are empty, so I need to check if there are any dataset IDs. However, the data_availability is empty, so there's no data available. The other_dataset_citations is also empty. So according to Task 1, no valid dataset IDs are found. Therefore, I should return the \"Missing\" structure.\n\nWait, but maybe I'm misunderstanding. The user's input has the `data_availability` as an empty string, and `other_dataset_citations` as an empty list. So according to Task 1, no valid dataset IDs are found. Therefore, the correct output is the \"Missing\" JSON array. I need to make sure that I follow the instructions exactly. The user's example input has both sections empty, so the output should be the \"Missing\" structure. Yes, that's right. So the final answer is the JSON array with the missing dataset_id and type.\n</think>"
        ],
        [
         "25",
         "10.1002_esp.5058",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.jh9w0vt9t', 'type': 'Primary'}]",
         "PDF",
         "Text Content: {\"title\":\"\",\"author\":\"Jonathan Mark Nelson2\",\"abstract\":\"R E S E A R C H A R T I C L E Past, present and future of a meandering river in the Bolivian Amazon basin Kattia Rubi Arnez Ferrel1 | Jonathan Mark Nelson2 | Yasuyuki Shimizu1 | Tomoko Kyuka1 1Graduate School of Engineering, Hokkaido University, Sapporo, Japan 2U.S. Geological Survey, Golden, Colorado, USA Correspondence Kattia Rubi Arnez Ferrel, Graduate School of Engineering, Hokkaido University, Sapporo 060-0808, Japan. Email: rubikraf@gmail.com Funding information Nitobe School Project Abstract Field observations on small rivers of the Amazon basin are less common due to their remote location and difficult accessibility. Here we show, through remote sensing analysis and field works, the planform evolution and riverbed topography of a small river located in the upper foreland Amazon basin, the Ichilo River. By tracking plan-form changes over 30 years, we identified the factors that control meander migration rates in the Ichilo River: cutoffs, climate and human interventions. The data suggest that neck cutoffs are the main controls in the Ichilo River, with an annual density of 0.022 cutoffs/km. In addition, climate controls have been identified in the form of high-precipitation events that may have promoted cutoffs, an increase in meander migration rate and channel widening. The width distribution of the Ichilo River is well represented by general extreme value and inverse Gaussian distributions. The spatio-temporal variability of meandering migration rates in the Ichilo River is analysed in two locations where neck cutoffs are expected. Analysing the distance across the neck in these two points, we predict the occurrence of a new cutoff. The combined methodology of bathymetric surveys and structure from motion photogrammetry shows us the Ichilo riverbed topography and banks at high resolution, where two scour holes were identified. Finally, we discuss the impact of planform changes of the Ichilo River on communities that are established along its riverbanks. K E Y W O R D S bathymetric surveys, Bolivian Amazon basin, Ichilo River, meanders, remote sensing, UAV 1 | INTRODUCTION The Amazon basin exhibits a constantly changing landscap\",\"data_availability\":\"DATA AVAILABILITY STATEMENT\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.1002/esp.5058\"],\"citation_context\":\"10969837, 2021, 4, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/esp.5058, Wiley Online Library on [31/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\"},{\"dataset_ids\":[\"10.1002/esp.4637\"],\"citation_context\":\", M.R., Chandler, J.H., Eltner, A., Fraser, C., Miller, P.E., Mills, J.P. et al. (2019) Guidelines on the use of structure-from-motion photo-grammetry in geomorphic research. Earth Surface Processes and Land-forms, 44(10), 2081-2084. https://doi.org/10.1002/esp.4637 Dataset Kinzel, P., Nelson, J.M. & Kattia, R.A.F. (2019) Bathymetric survey of the Ichilo and Sajta Rivers, near Puerto Villarroel, Bolivia, May 23-24, 2019. U.S. Geological Survey data release. https://doi.org/10.5066/P9FW6E8K Lanzoni, S. & Semin\"},{\"dataset_ids\":[\"10.5066/P9FW6E8K\"],\"citation_context\":\"84. https://doi.org/10.1002/esp.4637 Dataset Kinzel, P., Nelson, J.M. & Kattia, R.A.F. (2019) Bathymetric survey of the Ichilo and Sajta Rivers, near Puerto Villarroel, Bolivia, May 23-24, 2019. U.S. Geological Survey data release. https://doi.org/10.5066/P9FW6E8K Lanzoni, S. & Seminara, G. (2006) On the nature of meander instability. Journal of Geophysical Research - Earth Surface, 111(F4), 1-14. https://doi.org/10.1029/2005JF000416 Li, J., Grenfell, M.C., Wei, H., Tooth, S. & Ngiem, S. (2020) Chute cutoff-d\"},{\"dataset_ids\":[\"10.1029/2005JF000416\"],\"citation_context\":\", Bolivia, May 23-24, 2019. U.S. Geological Survey data release. https://doi.org/10.5066/P9FW6E8K Lanzoni, S. & Seminara, G. (2006) On the nature of meander instability. Journal of Geophysical Research - Earth Surface, 111(F4), 1-14. https://doi.org/10.1029/2005JF000416 Li, J., Grenfell, M.C., Wei, H., Tooth, S. & Ngiem, S. (2020) Chute cutoff-driven abandonment and sedimentation of meander bends along a fine-grained, non-vegetated, ephemeral river on the Bolivian Alti-plano. Geomorphology, 350, 106917. https://doi\"}]}\n",
         "[{'dataset_id': '10.1002/esp.5058', 'type': 'Primary'}, {'dataset_id': '10.1002/esp.4637', 'type': 'Primary'}, {'dataset_id': '10.5066/P9FW6E8K', 'type': 'Primary'}, {'dataset_id': '10.1029/2005JF000416', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON string representing a paper section with title, author, abstract, data_availability, and other_dataset_citations. My job is to extract valid dataset IDs and classify them as Primary or Secondary.\n\nFirst, I need to check the data_availability section. The abstract mentions \"data availability\" and \"dataset\" in the citation contexts. The data_availability is a string, so I should look for any DOI or Accession IDs here. The example in the data_availability has \"10.1002/esp.5058\" and \"10.5066/P9FW6E8K\". These look like DOIs. I need to apply the important DOI validation rule: only extract DOIs that are explicitly used as dataset IDs. The citation contexts here are \"10969837, 2021, 4, Downloaded from...\" and \"Dataset Kinzel, P., Nelson, J.M. & Kattia, R.A.F. (2019) Bathymetric survey...\". Both of these seem to be DOIs. So, I'll extract those.\n\nNext, check the other_dataset_citations. Each entry has a dataset_id and a citation context. The first entry has \"10.1002/esp.5058\" and the citation context mentions \"10969837, 2021, 4, Downloaded from...\" which matches the DOI. The second entry has \"10.1002/esp.4637\" and the citation context is a list of authors. The third entry has \"10.5066/P9FW6E8K\" and the citation context is a single line. The fourth entry has \"10.1029/2005JF000416\" and the citation context is another line. All these are DOIs, so they should be extracted.\n\nNow, I need to classify each dataset_id as Primary or Secondary. For each DOI, I check the abstract and citation contexts. The abstract mentions \"data availability\" and \"dataset\" in the citations. The first citation context refers to a study by Kinzel et al. and the data is from the U.S. Geological Survey. The second citation context is a list of authors, but the first author is the author of the paper, so it's Primary. The third and fourth entries also refer to the same dataset, so they are Primary. \n\nWait, the third entry's citation context is a single line with \"Dataset Kinzel...\" and the data is from the U.S. Geological Survey. The fourth entry's citation context is a list of authors, but the first is the author of the paper, so it's Primary. \n\nSo all the dataset IDs are Primary. The other_dataset_citations also have the same dataset, so they are Primary. \n\nNow, the data_availability section is empty, so I skip Task 1. Then, Task 2 and 3 proceed. The final output should be a JSON array with the dataset IDs classified as Primary. Since all are Primary, the result is a list of those dataset_ids.\n</think>"
        ],
        [
         "26",
         "10.1002_esp.5090",
         "[{'dataset_id': 'https://doi.org/10.5066/p9353101', 'type': 'Secondary'}]",
         "XML",
         "Text Content: {\"title\":\"\",\"author\":\"\",\"abstract\":\"Abstract The 20 May 2016 M W 6.1 Petermann earthquake in central Australia generated a 21 km surface rupture with 0.1 to 1 m vertical displacements across a low-relief landscape. No paleo-scarps or potentially analogous topographic features are evident in pre-earthquake Worldview-1 and Worldview-2 satellite data. Two excavations across the surface rupture expose near-surface fault geometry and mixed aeolian-sheetwash sediment faulted only in the 2016 earthquake. A 10.6 \\u00b1 0.4 ka optically stimulated luminescence (OSL) age of sheetwash sediment provides a minimum estimate for the period of quiescence prior to 2016 rupture. Seven cosmogenic beryllium-10 ( 10 Be) bedrock erosion rates are derived for samples < 5 km distance from the surface rupture on the hanging-wall and foot-wall, and three from samples 19 to 50 km from the surface rupture. No distinction is found between fault proximal rates (1.3 \\u00b1 0.1 to 2.6 \\u00b1 0.2 m Myr \\u22121 ) and distal samples (1.4 \\u00b1 0.1 to 2.3 \\u00b1 0.2 m Myr \\u22121 ). The thickness of rock fragments (2-5 cm) coseismically displaced in the Petermann earthquake perturbs the steady-state bedrock erosion rate by only 1 to 3%, less than the erosion rate uncertainty estimated for each sample (7-12%). Using 10 Be erosion rates and scarp height measurements we estimate approximately 0.5 to 1 Myr of differential erosion is required to return to pre-earthquake topography. By inference any pre-2016 fault-related topography likely required a similar time for removal. We conclude that the Petermann earthquake was the first on this fault in the last ca. 0.5-1 Myr. Extrapolating single nuclide erosion rates across this timescale introduces large uncertainties, and we cannot resolve whether 2016 represents the first ever surface rupture on this fault, or a > 1 Myr interseismic period. Either option reinforces the importance of including distributed earthquake sources in fault displacement and seismic hazard analyses.\",\"data_availability\":\"\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.1080/08120090802546977\"],\"citation_context\":\"Constraints on the current rate of deformation and surface uplift of the Australian continent from a new seismic database and low-T thermochronological data|Australian Journal of Earth Sciences|https://doi.org/10.1080/08120090802546977\"},{\"dataset_ids\":[\"10.1177/0959683613515732\"],\"citation_context\":\"Late-Holocene climatic variability indicated by three natural archives in arid southern Australia|Holocene|https://doi.org/10.1177/0959683613515732\"}]}\n",
         "[{'dataset_id': '10.1080/08120090802546977', 'type': 'Secondary'}, {'dataset_id': '10.1177/0959683613515732', 'type': 'Secondary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with sections for a paper, and I need to extract dataset IDs and classify them as Primary or Secondary. Let me start by looking at the input.\n\nFirst, the title is empty, so I don't have that. The author is also empty, which might be a problem, but maybe it's just an empty string. The abstract mentions a 20 May 2016 earthquake in Australia, which is a geological event. The data_availability section is empty, and the other_dataset_citations have two entries. \n\nLooking at the other_dataset_citations, each has a dataset_id. The first one is \"10.1080/08120090802546977\" and the citation context says it's from a new seismic database and low-T thermochronological data. The second is \"10.1177/0959683613515732\" with a citation about climatic variability in arid southern Australia.\n\nNow, I need to check if these dataset IDs are valid. The first one looks like a DOI. The important rule says only extract DOIs that are explicitly used as dataset IDs. The citation context mentions a new database, so it's a dataset. The second one is a DOI as well, and the citation context refers to a climatic study, which is a dataset. \n\nNext, I need to classify them as Primary or Secondary. The abstract mentions the earthquake and the data used. The data_availability is empty, so I can't check that. The other_dataset_citations have both entries. \n\nFor the first dataset_id, the citation context refers to a new seismic database and low-T data. Since the authors of the paper are not mentioned in the citation context, but the first person in the citation is \"Aerts H, Wee L, Velazquez E*, et al.\" which might be people, not the authors. However, the abstract doesn't mention the authors, so maybe the dataset is from an external source. But the key is to check if the data was generated by the authors of this paper. The abstract doesn't state that, so maybe it's Secondary. \n\nThe second dataset_id's citation context is about climatic variability, which is a dataset. The authors of the paper are not mentioned, so again, maybe Secondary. \n\nBut wait, the classification logic says if the citation context refers to people, check if the first person is the author. If not, it's Secondary. The first citation's citation context includes \"Aerts H, Wee L, Velazquez E*, et al.\" which might be people, but the author is not specified. The second citation's citation context is about a study, so maybe the dataset is from an external source. \n\nSo both dataset IDs are from external sources. Therefore, they should be classified as Secondary. \n\nBut wait, the first dataset_id's citation context mentions a new database, which could imply that the data was generated by the authors of the paper. However, the abstract doesn't mention the authors, so maybe it's Secondary. \n\nI think both dataset IDs are Secondary because they refer to external datasets. The first one is from a new database, and the second from a climatic study. The classification logic says if the citation context refers to people, check if the first person is the author. Since the first person in both citations is not the author, it's Secondary. \n\nTherefore, both dataset IDs are Secondary. The output should be two entries in the JSON array with type \"Secondary\".\n</think>"
        ],
        [
         "27",
         "10.1002_mp.14424",
         "[{'dataset_id': 'https://doi.org/10.7937/k9/tcia.2015.pf0m9rei', 'type': 'Secondary'}, {'dataset_id': 'https://doi.org/10.7937/tcia.2020.6c7y-gq39', 'type': 'Primary'}]",
         "XML",
         "Text Content: {\"title\":\"PleThora: Pleural effusion and thoracic cavity segmentations in diseased lungs for benchmarking chest CT processing pipelines\",\"author\":\"Kiser Kendall J.\",\"abstract\":\"This manuscript describes a dataset of thoracic cavity segmentations and discrete pleural effusion segmentations we have annotated on 402 computed tomography (CT) scans acquired from patients with non-small cell lung cancer. The segmentation of these anatomic regions precedes fundamental tasks in image analysis pipelines such as lung structure segmentation, lesion detection, and radiomics feature extraction. Bilateral thoracic cavity volumes and pleural effusion volumes were manually segmented on CT scans acquired from The Cancer Imaging Archive 'NSCLC Radiomics' data collection. Four hundred and two thoracic segmentations were first generated automatically by a U-Net based algorithm trained on chest CTs without cancer, manually corrected by a medical student to include the complete thoracic cavity (normal, pathologic, and atelectatic lung parenchyma, lung hilum, pleural effusion, fibrosis, nodules, tumor, and other anatomic anomalies), and revised by a radiation oncologist or a radiologist. Seventy-eight pleural effusions were manually segmented by a medical student and revised by a radiologist or radiation oncologist. Interobserver agreement between the radiation oncologist and radiologist corrections was acceptable. All expert-vetted segmentations are publicly available in NIfTI format through The Cancer Imaging Archive at https://doi.org/10.7937/tcia.2020.6c7y-gq39 . Tabular data detailing clinical and technical metadata linked to segmentation cases are also available. Thoracic cavity segmentations will be valuable for developing image analysis pipelines on pathologic lungs - where current automated algorithms struggle most. In conjunction with gross tumor volume segmentations already available from 'NSCLC Radiomics,' pleural effusion segmentations may be valuable for investigating radiomics profile differences between effusion and primary tumor or training algorithms to discriminate between them.\",\"data_availability\":\"\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.7937/K9/TCIA.2015.PF0M9REI\"],\"citation_context\":\"Aerts HJWL , Wee L , Rios Velazquez E , et al. Data from NSCLC-Radiomics [Dataset] . In: The Cancer Imaging Archive ; 2019 10.7937/K9/TCIA.2015.PF0M9REI\"},{\"dataset_ids\":[\"10.7937/tcia.2020.6c7y-gq39\"],\"citation_context\":\"Kiser KJ , Ahmed S , Stieb SM , et al. Data from the thoracic volume and pleural effusion segmentations in diseased lungs for benchmarking chest CT processing pipelines [Dataset] . In: The Cancer Imaging Archive ; 2020 10.7937/tcia.2020.6c7y-gq39\"}]}\n",
         "[{'dataset_id': '10.7937/K9/TCIA.2015.PF0M9REI', 'type': 'Primary'}, {'dataset_id': '10.7937/tcia.2020.6c7y-gq39', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with a paper section and wants me to extract dataset IDs and classify them as Primary or Secondary. \n\nFirst, I need to check the data_availability section. The input says it's empty, so I should look at the other_dataset_citations. There are two entries. \n\nLooking at the first citation: dataset_ids are [\"10.7937/K9/TCIA.2015.PF0M9REI\"] and the citation context mentions Aerts HJWL, Wee L, Rios Velazquez E, et al. The first person here is the author of the paper, Kiser Kendall J. So this seems like a Primary dataset. The DOI is valid and refers to a dataset, so that's a Primary.\n\nThe second citation has a dataset_id of [\"10.7937/tcia.2020.6c7y-gq39\"] and the context is Kiser KJ, Ahmed S, Stieb SM, et al. Again, the first person is the author, so this is also Primary. \n\nWait, but the data_availability is empty, so I need to check the other sections. Both citations are valid and refer to datasets. So both should be Primary. \n\nI need to make sure there are no other dataset IDs. The other_dataset_citations have those two. So the output should have both dataset IDs classified as Primary. \n\nNo missing dataset IDs here. So the final JSON array should include both entries.\n</think>"
        ],
        [
         "28",
         "10.1002_nafm.10870",
         "[{'dataset_id': 'https://doi.org/10.5066/p9gtumay', 'type': 'Primary'}]",
         "PDF",
         "Text Content: {\"title\":\"\",\"author\":\"Revised\",\"abstract\":\"Abstract Objective: Silver Chub Macrhybopsis storeriana, a predominately riverine species throughout its native range, exists within Lake Erie as the only known lake popu-lation. Its population declined in the 1950s and never fully recovered. Canada has listed Silver Chub in the Great Lakes-\\u00adSt. Lawrence River as endangered and has initiated a recovery plan that recognized the identification of spawning areas as a critical component to inform Silver Chub's recovery potential. Methods: We investigated potential spawning locations of Silver Chub using cap-ture records, otolith microchemistry, and daily age analysis. Lapillus otolith Sr:Ca ra-tios from 27 age-\\u00ad0 Silver Chub were used to identify potential spawning areas. Daily ages estimated from lapilli were used to calculate hatch dates, which then were com-pared with capture data of adults and river flows to further inform potential spawn-ing areas. Result: The Detroit River (and its nearshore area) was all but ruled out as a potential spawning location. The Maumee, Portage, and Sandusky rivers or their nearshore areas were all possible spawning locations. Projected hatch dates spanned the end of May through the end of June and occurred across a wide range of flows, although some peaks in hatch dates corresponded to flow peaks, indicating recruitment is po-tentially enhanced by high flows. Conclusion: Silver Chub spawning period and hypothesized spawning rivers or lacustuaries overlap those of invasive Grass Carp Ctenopharyngodon idella, creat-ing a need to jointly consider Grass Carp control efforts with conservation of Silver Chub when assessing management alternatives. Further research on spawning guild and the use of rivers themselves or nearshore areas influenced by rivers as spawning areas are required to maximize potential for conservation and recovery of Silver Chub.\",\"data_availability\":\"DATA AVAILABILITY STATEMENT U.S. Geological Survey data are publicly available at the websites in the references section. Western Basin intera-gency August trawl survey data are available from the For-age Task Group of the Lake Erie Committee (http://www. glfc.org/lake-erie-committee.php) by request. Data col-lected by the U.S. Fish and Wildlife Service are available upon request from Anjanette Bowen (anjanette_bowen@ fws.gov). Silver Chub otolith microchemistry data are available upon request from John Farver or Jeff Miner (jfarver@bgsu.edu or jminer@bgsu.edu, respectively).\",\"other_dataset_citations\":[{\"dataset_ids\":[\"10.5066/P9GTUMAY.\"],\"citation_context\":\"associated species. Doctoral dissertation. The Ohio State University, Columbus. Ko\\u010dovsk\\u00fd, P. M., and J. Farver. 2019. Water chemistry of Great Lakes tributaries, 2017-\\u00ad2018. U.S. Geological Survey data release. Available: https://doi.org/10.5066/P9GTUMAY. Ko\\u010dovsk\\u00fd, P. M., N. R. King, C. M. Merkes, and T. M. Schreier. 2018. Developmental stages of Grass Carp (Ctenopharyngodon idella) eggs in the Sandusky River (version 3.0, July 2020). U.S. Geological Survey data release. Available: https://doi. org/1\"},{\"dataset_ids\":[\"10.5066/P94BH3W0.\"],\"citation_context\":\". Ko\\u010dovsk\\u00fd, P. M., N. R. King, C. M. Merkes, and T. M. Schreier. 2018. Developmental stages of Grass Carp (Ctenopharyngodon idella) eggs in the Sandusky River (version 3.0, July 2020). U.S. Geological Survey data release. Available: https://doi. org/10.5066/P94BH3W0. Ko\\u010dovsk\\u00fd, P. M., N. R. King, E. Weimer, C. Mayer, and S. S. Qian. 2021. Validation of the model-\\u00adpredicted spawning area of Grass Carp Ctenopharyngodon idella in the Sandusky River. Journal of Great Lakes Research 47:29-36. Koel, T. M., and R. E. Sp\"},{\"dataset_ids\":[\"10.5066/P90A4UGJ.\"],\"citation_context\":\"ples and techniques. American Fisheries Society, Bethesda, Maryland. Long, J. M., and P. M. Kocovsky. 2022. Daily ages of young-\\u00adof-\\u00adyear Silver Chub from western Lake Erie, 2017-\\u00ad2018: U.S. Geological Survey data release. Available: https://doi.org/10.5066/P90A4UGJ. Long, J. M., and M. J. Porta. 2019. Age and growth of stocked Shoal Bass in a tailwater: environmental variation and accuracy of daily age estimates. PLoS (Public Library of Science) ONE 14(10):e0224018. Mandrak, N. E., and E. J. Crossman. 1992. Pos\"},{\"dataset_ids\":[\"10.5066/F75M63X0.\"],\"citation_context\":\"can Fisheries Society, Bethesda, Maryland. USGS (U.S. Geological Survey). 2019a. Great Lakes Science Center Great Lakes research vessel operations 1958-\\u00ad2018. (version 3.0, April 2019). U.S. Geological Survey data release. Available: https://doi.org/10.5066/F75M63X0. USGS (U.S. Geological Survey). 2019b. Newly hatched invasive Grass Carp found in Maumee River, OH. U.S. Geological Survey, Office of Communications and Publishing, Reston, Virginia. Available: www.usgs.gov/news/newly-\\u00adhatched-\\u00adinvasive-\\u00adgrass -\\u00adcarp\"}]}\n",
         "[{'dataset_id': '10.5066/P9GTUMAY.', 'type': 'Primary'}, {'dataset_id': '10.5066/P94BH3W0.', 'type': 'Secondary'}, {'dataset_id': '10.5066/P90A4UGJ.', 'type': 'Primary'}, {'dataset_id': '10.5066/F75M63X0.', 'type': 'Secondary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON string with a paper's sections, and I need to extract dataset IDs and classify them as Primary or Secondary. Let me start by understanding the structure.\n\nFirst, the input JSON has \"title\", \"author\", \"abstract\", \"data_availability\", and \"other_dataset_citations\". The data_availability has a string with DOI links, and other_dataset_citations have lists of dataset IDs. The task is to check each dataset_id in these sections to determine if it's Primary or Secondary.\n\nLooking at the abstract, the main point is about Silver Chub's population decline and recovery. The data_availability mentions U.S. Geological Survey data, which includes various datasets. The other_dataset_citations have entries like \"10.5066/P9GTUMAY.\" and \"10.5066/P94BH3W0.\".\n\nNow, I need to check each dataset_id. For each, I should look in the abstract and citation contexts. The key is to see if the dataset is directly related to the research topic. The abstract mentions Silver Chub's spawning areas, so looking at the citations, the first one is about a doctoral dissertation by Koel and Farver, which is a dataset. The citation context here is \"associated species. Doctoral dissertation. The Ohio State University...\" which points to a dataset. So that's a Primary dataset because it's a new data product created by the authors.\n\nNext, the second citation is about Grass Carp eggs, which is a secondary dataset because it's related to another species. The third citation is about daily ages of Silver Chub, which is a Primary dataset since it's part of the research. The fourth citation is about a Fisheries Society publication, which is also a Primary dataset. The last one is about a USGS data release, which is a Secondary dataset because it's from a different source.\n\nI need to ensure that each dataset_id is only processed once. Also, check if the data_availability is empty or the other_citations are empty. In this case, both are not, so proceed. Now, classify each dataset. The first one is Primary, the second Secondary, third Primary, fourth Secondary. So the final JSON array should include all these entries.\n</think>"
        ],
        [
         "29",
         "10.1007_jhep07(2018)134",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "PDF",
         "Text Content: {\"title\":\"\",\"author\":\"TeV\",\"abstract\":\"Abstract: The production cross-sections of \\u03a5(1S), \\u03a5(2S) and \\u03a5(3S) mesons in proton-proton collisions at \\u221as = 13 TeV are measured with a data sample corresponding to an integrated luminosity of 277 \\u00b1 11 pb\\u22121 recorded by the LHCb experiment in 2015. The \\u03a5 mesons are reconstructed in the decay mode \\u03a5 \\u2192\\u00b5+\\u00b5\\u2212. The differential production cross-sections times the dimuon branching fractions are measured as a function of the \\u03a5 transverse momentum, pT, and rapidity, y, over the range 0 < pT < 30 GeV/c and 2.0 < y < 4.5. The ratios of the cross-sections with respect to the LHCb measurement at \\u221as = 8 TeV are also determined. The measurements are compared with theoretical predictions based on NRQCD.\",\"data_availability\":\"Acknowledgments\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with some paper sections, and I need to extract dataset IDs. First, I'll check the data_availability section. The data_availability here is \"Acknowledgments\", which doesn't look like a dataset. So, I'll move to the other_dataset_citations section. There are no entries there, so Task 1 didn't find any valid dataset IDs. Since Task 1 resulted in no valid IDs, I need to return the \"Missing\" JSON structure as per Scenario A. The dataset_id is missing, and the type is also missing. I should make sure to format it correctly as per the instructions.\n</think>"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 30
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_info</th>\n",
       "      <th>text_type</th>\n",
       "      <th>llm_input</th>\n",
       "      <th>llm_response</th>\n",
       "      <th>llm_thinking_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.17882/4938...</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Assessing the variabil...</td>\n",
       "      <td>[{'dataset_id': '10.17882/49388', 'type': 'Pri...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Effective and Reversib...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Trifluoromethyl Sulfox...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1002_anie.202007717</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Metal\\u2013Ligand Coop...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1002_chem.201902131</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"A Synthetic Route Towa...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1002_chem.201903120</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Structure\\u2013Solubil...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.1002_chem.202000235</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"An Isolable Bis(Silano...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.1002_chem.202001412</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Iron(0)\\u2010Mediated ...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.1002_chem.202001668</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"The First Use of a ReX...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.1002_chem.202003167</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Metal\\u2010only Lewis ...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.1002_cssc.202201821</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Development of a Mg/O ...</td>\n",
       "      <td>[{'dataset_id': '10.5281/zenodo.7074790', 'typ...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.1002_ece3.3985</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"High levels of polyand...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.1002_ece3.4466</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Incubation temperature...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.1002_ece3.5260</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Ultraconserved element...</td>\n",
       "      <td>[{'dataset_id': '10.5061/dryad.2f62927', 'type...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.1002_ece3.5395</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"\\u201cCloser\\u2010to\\u...</td>\n",
       "      <td>[{'dataset_id': '10.12345/12345', 'type': 'Pri...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.1002_ece3.6144</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Efficacy of metabarcod...</td>\n",
       "      <td>[{'dataset_id': '10.5061/dryad.zw3r22854', 'ty...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.1002_ece3.6303</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Can polyploidy confer ...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.1002_ece3.6784</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Ecomorphology of the p...</td>\n",
       "      <td>[{'dataset_id': '10.1007/s00435-017-0386-3', '...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.1002_ece3.961</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"The effects of insects...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.1002_ece3.9627</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Inferring predator\\u20...</td>\n",
       "      <td>[{'dataset_id': '10.5061/dryad.b8gtht7h3', 'ty...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.1002_ecs2.1280</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "      <td>PDF</td>\n",
       "      <td>Text Content: {\"title\":\"\",\"author\":\"Haley A. C...</td>\n",
       "      <td>[{'dataset_id': '10.5061/dryad.p3fg9', 'type':...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.1002_ecs2.4619</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.25349/d9qw...</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Interactions between t...</td>\n",
       "      <td>[{'dataset_id': '10.25349/D9QW5X', 'type': 'Se...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.1002_ejic.201900904</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"\",\"author\":\"\",\"abstrac...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.1002_ejoc.202000139</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>PDF</td>\n",
       "      <td>Text Content: {\"title\":\"\",\"author\":\"Ann Christ...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.1002_ejoc.202000916</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Ammonium Salt\\u2010Cat...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.1002_esp.5058</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "      <td>PDF</td>\n",
       "      <td>Text Content: {\"title\":\"\",\"author\":\"Jonathan M...</td>\n",
       "      <td>[{'dataset_id': '10.1002/esp.5058', 'type': 'P...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.1002_esp.5090</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5066/p9353...</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"\",\"author\":\"\",\"abstrac...</td>\n",
       "      <td>[{'dataset_id': '10.1080/08120090802546977', '...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10.1002_mp.14424</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.7937/k9/tc...</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"PleThora: Pleural effu...</td>\n",
       "      <td>[{'dataset_id': '10.7937/K9/TCIA.2015.PF0M9REI...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.1002_nafm.10870</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5066/p9gtu...</td>\n",
       "      <td>PDF</td>\n",
       "      <td>Text Content: {\"title\":\"\",\"author\":\"Revised\",\"...</td>\n",
       "      <td>[{'dataset_id': '10.5066/P9GTUMAY.', 'type': '...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.1007_jhep07(2018)134</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>PDF</td>\n",
       "      <td>Text Content: {\"title\":\"\",\"author\":\"TeV\",\"abst...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 article_id  \\\n",
       "0      10.1002_2017jc013030   \n",
       "1    10.1002_anie.201916483   \n",
       "2    10.1002_anie.202005531   \n",
       "3    10.1002_anie.202007717   \n",
       "4    10.1002_chem.201902131   \n",
       "5    10.1002_chem.201903120   \n",
       "6    10.1002_chem.202000235   \n",
       "7    10.1002_chem.202001412   \n",
       "8    10.1002_chem.202001668   \n",
       "9    10.1002_chem.202003167   \n",
       "10   10.1002_cssc.202201821   \n",
       "11        10.1002_ece3.3985   \n",
       "12        10.1002_ece3.4466   \n",
       "13        10.1002_ece3.5260   \n",
       "14        10.1002_ece3.5395   \n",
       "15        10.1002_ece3.6144   \n",
       "16        10.1002_ece3.6303   \n",
       "17        10.1002_ece3.6784   \n",
       "18         10.1002_ece3.961   \n",
       "19        10.1002_ece3.9627   \n",
       "20        10.1002_ecs2.1280   \n",
       "21        10.1002_ecs2.4619   \n",
       "22   10.1002_ejic.201900904   \n",
       "23   10.1002_ejoc.202000139   \n",
       "24   10.1002_ejoc.202000916   \n",
       "25         10.1002_esp.5058   \n",
       "26         10.1002_esp.5090   \n",
       "27         10.1002_mp.14424   \n",
       "28       10.1002_nafm.10870   \n",
       "29  10.1007_jhep07(2018)134   \n",
       "\n",
       "                                         dataset_info text_type  \\\n",
       "0   [{'dataset_id': 'https://doi.org/10.17882/4938...       XML   \n",
       "1      [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "2      [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "3      [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "4      [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "5      [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "6      [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "7      [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "8      [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "9      [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "10                                                NaN       XML   \n",
       "11     [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "12  [{'dataset_id': 'https://doi.org/10.5061/dryad...       XML   \n",
       "13  [{'dataset_id': 'https://doi.org/10.5061/dryad...       XML   \n",
       "14     [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "15  [{'dataset_id': 'https://doi.org/10.5061/dryad...       XML   \n",
       "16  [{'dataset_id': 'https://doi.org/10.5061/dryad...       XML   \n",
       "17     [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "18     [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "19  [{'dataset_id': 'https://doi.org/10.5061/dryad...       XML   \n",
       "20  [{'dataset_id': 'https://doi.org/10.5061/dryad...       PDF   \n",
       "21  [{'dataset_id': 'https://doi.org/10.25349/d9qw...       XML   \n",
       "22     [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "23     [{'dataset_id': 'Missing', 'type': 'Missing'}]       PDF   \n",
       "24     [{'dataset_id': 'Missing', 'type': 'Missing'}]       XML   \n",
       "25  [{'dataset_id': 'https://doi.org/10.5061/dryad...       PDF   \n",
       "26  [{'dataset_id': 'https://doi.org/10.5066/p9353...       XML   \n",
       "27  [{'dataset_id': 'https://doi.org/10.7937/k9/tc...       XML   \n",
       "28  [{'dataset_id': 'https://doi.org/10.5066/p9gtu...       PDF   \n",
       "29     [{'dataset_id': 'Missing', 'type': 'Missing'}]       PDF   \n",
       "\n",
       "                                            llm_input  \\\n",
       "0   Text Content: {\"title\":\"Assessing the variabil...   \n",
       "1   Text Content: {\"title\":\"Effective and Reversib...   \n",
       "2   Text Content: {\"title\":\"Trifluoromethyl Sulfox...   \n",
       "3   Text Content: {\"title\":\"Metal\\u2013Ligand Coop...   \n",
       "4   Text Content: {\"title\":\"A Synthetic Route Towa...   \n",
       "5   Text Content: {\"title\":\"Structure\\u2013Solubil...   \n",
       "6   Text Content: {\"title\":\"An Isolable Bis(Silano...   \n",
       "7   Text Content: {\"title\":\"Iron(0)\\u2010Mediated ...   \n",
       "8   Text Content: {\"title\":\"The First Use of a ReX...   \n",
       "9   Text Content: {\"title\":\"Metal\\u2010only Lewis ...   \n",
       "10  Text Content: {\"title\":\"Development of a Mg/O ...   \n",
       "11  Text Content: {\"title\":\"High levels of polyand...   \n",
       "12  Text Content: {\"title\":\"Incubation temperature...   \n",
       "13  Text Content: {\"title\":\"Ultraconserved element...   \n",
       "14  Text Content: {\"title\":\"\\u201cCloser\\u2010to\\u...   \n",
       "15  Text Content: {\"title\":\"Efficacy of metabarcod...   \n",
       "16  Text Content: {\"title\":\"Can polyploidy confer ...   \n",
       "17  Text Content: {\"title\":\"Ecomorphology of the p...   \n",
       "18  Text Content: {\"title\":\"The effects of insects...   \n",
       "19  Text Content: {\"title\":\"Inferring predator\\u20...   \n",
       "20  Text Content: {\"title\":\"\",\"author\":\"Haley A. C...   \n",
       "21  Text Content: {\"title\":\"Interactions between t...   \n",
       "22  Text Content: {\"title\":\"\",\"author\":\"\",\"abstrac...   \n",
       "23  Text Content: {\"title\":\"\",\"author\":\"Ann Christ...   \n",
       "24  Text Content: {\"title\":\"Ammonium Salt\\u2010Cat...   \n",
       "25  Text Content: {\"title\":\"\",\"author\":\"Jonathan M...   \n",
       "26  Text Content: {\"title\":\"\",\"author\":\"\",\"abstrac...   \n",
       "27  Text Content: {\"title\":\"PleThora: Pleural effu...   \n",
       "28  Text Content: {\"title\":\"\",\"author\":\"Revised\",\"...   \n",
       "29  Text Content: {\"title\":\"\",\"author\":\"TeV\",\"abst...   \n",
       "\n",
       "                                         llm_response  \\\n",
       "0   [{'dataset_id': '10.17882/49388', 'type': 'Pri...   \n",
       "1      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "2      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "3      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "4      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "5      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "6      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "7      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "8      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "9      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "10  [{'dataset_id': '10.5281/zenodo.7074790', 'typ...   \n",
       "11     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "12     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "13  [{'dataset_id': '10.5061/dryad.2f62927', 'type...   \n",
       "14  [{'dataset_id': '10.12345/12345', 'type': 'Pri...   \n",
       "15  [{'dataset_id': '10.5061/dryad.zw3r22854', 'ty...   \n",
       "16                                                 []   \n",
       "17  [{'dataset_id': '10.1007/s00435-017-0386-3', '...   \n",
       "18     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "19  [{'dataset_id': '10.5061/dryad.b8gtht7h3', 'ty...   \n",
       "20  [{'dataset_id': '10.5061/dryad.p3fg9', 'type':...   \n",
       "21  [{'dataset_id': '10.25349/D9QW5X', 'type': 'Se...   \n",
       "22     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "23                                                 []   \n",
       "24     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "25  [{'dataset_id': '10.1002/esp.5058', 'type': 'P...   \n",
       "26  [{'dataset_id': '10.1080/08120090802546977', '...   \n",
       "27  [{'dataset_id': '10.7937/K9/TCIA.2015.PF0M9REI...   \n",
       "28  [{'dataset_id': '10.5066/P9GTUMAY.', 'type': '...   \n",
       "29     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "\n",
       "                                 llm_thinking_content  \n",
       "0   <think>\\nOkay, let's tackle this problem. The ...  \n",
       "1   <think>\\nOkay, let's tackle this problem. The ...  \n",
       "2   <think>\\nOkay, let's tackle this problem. The ...  \n",
       "3   <think>\\nOkay, let's tackle this problem. The ...  \n",
       "4   <think>\\nOkay, let's tackle this problem. The ...  \n",
       "5   <think>\\nOkay, let's tackle this problem. The ...  \n",
       "6   <think>\\nOkay, let's tackle this problem. The ...  \n",
       "7   <think>\\nOkay, let's tackle this problem. The ...  \n",
       "8   <think>\\nOkay, let's tackle this problem. The ...  \n",
       "9   <think>\\nOkay, let's tackle this problem. The ...  \n",
       "10  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "11  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "12  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "13  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "14  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "15  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "16                                                     \n",
       "17  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "18  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "19  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "20  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "21  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "22  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "23                                                     \n",
       "24  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "25  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "26  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "27  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "28  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "29  <think>\\nOkay, let's tackle this problem. The ...  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_articles_df = evaluate_articles(file_paths_df, inference_model)\n",
    "# Save processed_articles_df to CSV\n",
    "processed_articles_df.to_csv(\"sample_evaluated_articles.csv\", index=False)\n",
    "print(f\"Processed articles DataFrame shape: {processed_articles_df.shape}\")\n",
    "processed_articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d9cf05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset_id(dataset_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Formats the dataset_id by removing any leading/trailing whitespace and ensuring it is a string.\n",
    "    \n",
    "    Args:\n",
    "        dataset_id (str): The dataset identifier to format.\n",
    "        \n",
    "    Returns:\n",
    "        str: The formatted dataset identifier.\n",
    "    \"\"\"\n",
    "    if dataset_id and dataset_id.startswith(\"10.\") and len(dataset_id) > 10:\n",
    "        # If the dataset_id starts with \"10.\" and is longer than 10 characters, it's likely a DOI\n",
    "        dataset_id = \"https://doi.org/\" + dataset_id.lower().strip()\n",
    "    return dataset_id\n",
    "\n",
    "# Create a DataFrame to hold the evaluation results by expaning the 'llm_response' column\n",
    "def expand_evaluation_results(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expands the evaluation results DataFrame by extracting dataset_id and type from the 'llm_response' column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing evaluation results.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with expanded dataset_id and type columns.\n",
    "    \"\"\"\n",
    "    expanded_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        article_id = row['article_id']\n",
    "        article_doi = article_id.replace('_', '/')\n",
    "        datasets = row['llm_response']\n",
    "        missing_dataset = {\n",
    "            'article_id': article_id,\n",
    "            'dataset_id': 'Missing',\n",
    "            'type': 'Missing',\n",
    "        }\n",
    "\n",
    "        if datasets:\n",
    "            for dataset in datasets:\n",
    "                dataset_id = dataset.get('dataset_id', 'Missing')\n",
    "                # Skip if the dataset_id is the same as the article DOI\n",
    "                if dataset_id == article_doi:\n",
    "                    # If the dataset_id is the same as the article DOI add it as Missing\n",
    "                    expanded_rows.append(missing_dataset)\n",
    "                else:\n",
    "                    expanded_rows.append({\n",
    "                        'article_id': article_id,\n",
    "                        'dataset_id': dataset.get('dataset_id', 'Missing'),\n",
    "                        'type': dataset.get('type', 'Missing'),\n",
    "                    })\n",
    "        else:\n",
    "            # If no datasets were found, add a row with 'Missing' values\n",
    "            expanded_rows.append(missing_dataset)\n",
    "    \n",
    "    # Create a DataFrame from the expanded rows\n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "    expanded_df['dataset_id'] = expanded_df['dataset_id'].apply(format_dataset_id)  # Format dataset_id\n",
    "    expanded_df['type'] = expanded_df['type'].str.strip().str.capitalize()  # Ensure type is capitalized and stripped of whitespace\n",
    "    expanded_df = expanded_df.sort_values(by=[\"article_id\", \"dataset_id\", \"type\"], ascending=True).drop_duplicates(subset=['article_id', 'dataset_id'], keep=\"first\").reset_index(drop=True)\n",
    "    \n",
    "    return expanded_df\n",
    "\n",
    "def prepare_for_submission(expanded_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepares the expanded DataFrame for submission by ensuring the correct columns and formatting.\n",
    "    \n",
    "    Args:\n",
    "        expanded_df (pd.DataFrame): The DataFrame containing expanded dataset information.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame ready for submission with 'article_id', 'dataset_id', and 'type' columns.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame has the correct columns\n",
    "    submission_df = expanded_df[['article_id', 'dataset_id', 'type']].copy()\n",
    "    # Rename columns to match the expected format\n",
    "    submission_df.columns = ['article_id', 'dataset_id', 'type']\n",
    "\n",
    "    # Remove rows where type is 'Missing' and reset index\n",
    "    submission_df = submission_df[submission_df['type'] != 'Missing'].reset_index(drop=True)\n",
    "    submission_df['row_id'] = range(len(submission_df))\n",
    "\n",
    "    # Reorder columns to match the submission format\n",
    "    submission_df = submission_df[['row_id', 'article_id', 'dataset_id', 'type']]\n",
    "    \n",
    "    return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5e9bc75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Eval DataFrame shape: (46, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f2da659c-5b21-4a89-88a9-3e0626304515",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "https://doi.org/10.17882/47142",
         "Primary"
        ],
        [
         "1",
         "10.1002_2017jc013030",
         "https://doi.org/10.17882/49388",
         "Primary"
        ],
        [
         "2",
         "10.1002_2017jc013030",
         "https://doi.org/10.5194/essd-2017-58",
         "Primary"
        ],
        [
         "3",
         "10.1002_anie.201916483",
         "Missing",
         "Missing"
        ],
        [
         "4",
         "10.1002_anie.202005531",
         "Missing",
         "Missing"
        ],
        [
         "5",
         "10.1002_anie.202007717",
         "Missing",
         "Missing"
        ],
        [
         "6",
         "10.1002_chem.201902131",
         "Missing",
         "Missing"
        ],
        [
         "7",
         "10.1002_chem.201903120",
         "Missing",
         "Missing"
        ],
        [
         "8",
         "10.1002_chem.202000235",
         "Missing",
         "Missing"
        ],
        [
         "9",
         "10.1002_chem.202001412",
         "Missing",
         "Missing"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/47142</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/49388</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.5194/essd-2017-58</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1002_anie.202007717</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.1002_chem.201902131</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.1002_chem.201903120</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.1002_chem.202000235</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.1002_chem.202001412</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                            dataset_id     type\n",
       "0    10.1002_2017jc013030        https://doi.org/10.17882/47142  Primary\n",
       "1    10.1002_2017jc013030        https://doi.org/10.17882/49388  Primary\n",
       "2    10.1002_2017jc013030  https://doi.org/10.5194/essd-2017-58  Primary\n",
       "3  10.1002_anie.201916483                               Missing  Missing\n",
       "4  10.1002_anie.202005531                               Missing  Missing\n",
       "5  10.1002_anie.202007717                               Missing  Missing\n",
       "6  10.1002_chem.201902131                               Missing  Missing\n",
       "7  10.1002_chem.201903120                               Missing  Missing\n",
       "8  10.1002_chem.202000235                               Missing  Missing\n",
       "9  10.1002_chem.202001412                               Missing  Missing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "expanded_df = expand_evaluation_results(processed_articles_df)\n",
    "expanded_df.to_csv(os.path.join(BASE_OUTPUT_DIR, 'expanded_eval_results.csv'), index=False)\n",
    "print(f\"Expanded Eval DataFrame shape: {expanded_df.shape}\")\n",
    "display(expanded_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "58819a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "67c48212-b5d5-4502-a910-f21bcc049006",
       "rows": [
        [
         "Primary",
         "22"
        ],
        [
         "Secondary",
         "5"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "type\n",
       "Primary      22\n",
       "Secondary     5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_submission_df = prepare_for_submission(expanded_df)\n",
    "sample_submission_df.to_csv(\"test_sample_submission.csv\", index=False)\n",
    "\n",
    "sample_submission_df[\"type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1df31a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 7\n",
      "FP: 20\n",
      "FN: 7\n",
      "F1 Score: 0.341\n"
     ]
    }
   ],
   "source": [
    "def f1_score(tp, fp, fn):\n",
    "    return 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) != 0 else 0.0\n",
    "    \n",
    "    \n",
    "# if not os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "pred_df = sample_submission_df.copy()\n",
    "label_df = pd.read_csv(\"./kaggle/input/make-data-count-finding-data-references/sample_submission.csv\")\n",
    "label_df = label_df[label_df['type'] != 'Missing'].reset_index(drop=True)\n",
    "\n",
    "hits_df = label_df.merge(pred_df, on=[\"article_id\", \"dataset_id\", \"type\"])\n",
    "\n",
    "tp = hits_df.shape[0]\n",
    "fp = pred_df.shape[0] - tp\n",
    "fn = label_df.shape[0] - tp\n",
    "\n",
    "print(\"TP:\", tp)\n",
    "print(\"FP:\", fp)\n",
    "print(\"FN:\", fn)\n",
    "print(\"F1 Score:\", round(f1_score(tp, fp, fn), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
