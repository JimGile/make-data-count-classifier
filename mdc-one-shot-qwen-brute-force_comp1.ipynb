{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0a8ecd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d2db1bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- 0. Environment Setup & Offline Preparation ---\n",
    "\n",
    "# Standard Imports\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import lxml.etree as etree\n",
    "from lxml.etree import _Element as Element # Type hinting for lxml.etree.Element\n",
    "import collections # For deque in parenthesis removal\n",
    "import fitz # PyMuPDF for PDF processing\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from transformers.training_args import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "from datasets import Dataset # Hugging Face datasets library\n",
    "import kagglehub\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = \"cuda\" if torch and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b667ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this line once to download the model\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "48e263db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for file paths and model configurations\n",
    "BASE_INPUT_DIR = './kaggle/input/make-data-count-finding-data-references'\n",
    "ARTICLE_TRAIN_DIR = os.path.join(BASE_INPUT_DIR, 'train')\n",
    "ARTICLE_TEST_DIR = os.path.join(BASE_INPUT_DIR, 'test')\n",
    "\n",
    "# Define directories for articles in train and test sets\n",
    "LABELED_TRAINING_DATA_CSV_PATH = os.path.join(BASE_INPUT_DIR, 'train_labels.csv')\n",
    "\n",
    "# Define the base model path\n",
    "QWEN_BASE_MODEL_PATH = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "\n",
    "# Output directory for the fine-tuned model and results\n",
    "BASE_OUTPUT_DIR = \"./kaggle/working\"\n",
    "FINE_TUNED_MODEL_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, \"qwen_finetuned_dataset_classifier\")\n",
    "FINAL_RESULTS_CSV_PATH = os.path.join(BASE_OUTPUT_DIR, \"article_dataset_classification.csv\")\n",
    "\n",
    "# Load a spaCy model (e.g., 'en_core_web_sm')\n",
    "# python -m spacy download en_core_web_sm \n",
    "NLP_SPACY = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0734f1",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f5e14",
   "metadata": {},
   "source": [
    "### Common dataset identifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2e50e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Information Extraction (IE) - Dataset Identification ---\n",
    "NON_STD_UNICODE_DASHES = re.compile(r'[\\u2010\\u2011\\u2012\\u2013\\u2014]')\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input text by removing non-standard unicode dashes and extra whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Replace all non-standard unicode dashes with '-'\n",
    "    text = text.replace('\\u200b', '').replace('-\\n', '-').replace('_\\n', '_').replace('/\\n', '/')\n",
    "    text = NON_STD_UNICODE_DASHES.sub('-', text)\n",
    "    # Remove extra whitespace\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Regex patterns for common dataset identifiers\n",
    "# DOI_PATTERN = r'10\\.\\d{4,5}/[-._;()/:A-Za-z0-9\\u002D\\u2010\\u2011\\u2012\\u2013\\u2014\\u2015]+'\tDOI_PATTERN\n",
    "# DOI_PATTERN = r'10\\.\\s?\\d{4,5}\\/[-._()<>;\\/:A-Za-z0-9]+\\s?(?:(?![A-Z]+)(?!\\d{1,3}\\.))+[-._()<>;\\/:A-Za-z0-9]+'\n",
    "#DOI_PATTERN = r'\\bhttps://doi.org/10\\.\\d{4,5}\\/[-._\\/:A-Za-z0-9]+'\n",
    "DOI_PATTERN = r'\\b10\\.\\d{4,5}\\/[-._\\/:A-Za-z0-9]+'\n",
    "EPI_PATTERN = r'\\bEPI[-_A-Z0-9]{2,}'\n",
    "SAM_PATTERN = r'\\bSAMN[0-9]{2,}'          # SAMN07159041\n",
    "IPR_PATTERN = r'\\bIPR[0-9]{2,}'\n",
    "CHE_PATTERN = r'\\bCHEMBL[0-9]{2,}'\n",
    "PRJ_PATTERN = r'\\bPRJ[A-Z0-9]{2,}'\n",
    "E_G_PATTERN = r'\\bE-[A-Z]{4}-[0-9]{2,}'   # E-GEOD-19722 or E-PROT-100\n",
    "ENS_PATTERN = r'\\bENS[A-Z]{4}[0-9]{2,}'\n",
    "CVC_PATTERN = r'\\bCVCL_[A-Z0-9]{2,}'\n",
    "EMP_PATTERN = r'\\bEMPIAR-[0-9]{2,}'\n",
    "PXD_PATTERN = r'\\bPXD[0-9]{2,}'\n",
    "HPA_PATTERN = r'\\bHPA[0-9]{2,}'\n",
    "SRR_PATTERN = r'\\bSRR[0-9]{2,}'\n",
    "GSE_PATTERN = r'\\b(GSE|GSM|GDS|GPL)\\d{4,6}\\b' # Example for GEO accession numbers (e.g., GSE12345, GSM12345)\n",
    "GNB_PATTERN = r'\\b[A-Z]{1,2}\\d{5,6}\\b' # GenBank accession numbers (e.g., AB123456, AF000001)\n",
    "CAB_PATTERN = r'\\bCAB[0-9]{2,}'\n",
    "\n",
    "# Combine all patterns into a list\n",
    "DATASET_ID_PATTERNS = [\n",
    "    DOI_PATTERN,\n",
    "    EPI_PATTERN,\n",
    "    SAM_PATTERN,\n",
    "    IPR_PATTERN,\n",
    "    CHE_PATTERN,\n",
    "    PRJ_PATTERN,\n",
    "    E_G_PATTERN,\n",
    "    ENS_PATTERN,\n",
    "    CVC_PATTERN,\n",
    "    EMP_PATTERN,\n",
    "    PXD_PATTERN,\n",
    "    HPA_PATTERN,\n",
    "    SRR_PATTERN,\n",
    "    GSE_PATTERN,\n",
    "    GNB_PATTERN,\n",
    "    CAB_PATTERN,\n",
    "]\n",
    "\n",
    "# Compile all patterns for efficiency\n",
    "COMPILED_DATASET_ID_REGEXES = [re.compile(p) for p in DATASET_ID_PATTERNS]\n",
    "\n",
    "# Data related keywords to look for in the text\n",
    "# These keywords help to ensure that the text is relevant to datasets\n",
    "DATA_RELATED_KEYWORDS = ['data release', 'data associated', 'data availability', 'data access', 'download', 'program data', 'the data', 'dataset', 'database', 'repository', 'data source', 'data access', 'archive', 'arch.', 'digital']\n",
    "\n",
    "def is_text_data_related(text: str) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in DATA_RELATED_KEYWORDS)\n",
    "\n",
    "def text_has_dataset_id(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given text contains any dataset identifier.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to check for dataset identifiers.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if any dataset identifier is found, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    occurrences_with_context: list[str] = []\n",
    "    for regex in COMPILED_DATASET_ID_REGEXES:\n",
    "        if regex.search(text):\n",
    "            text_lower = text.lower()\n",
    "            # Check for specific keywords in the text\n",
    "            if any(keyword in text_lower for keyword in DATA_RELATED_KEYWORDS):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def extract_dataset_ids(text: str, context_chars: int = 250) -> str:\n",
    "    \"\"\"\n",
    "    Extract dataset identifiers with context from the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to search for dataset identifiers.\n",
    "        context_chars (int): Number of characters to include before and after the match for context.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: A list of extracted dataset identifiers with context.\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    is_small_context = len(text) < context_chars * 2\n",
    "    dataset_ids: list[str] = []\n",
    "    occurrences_with_context: list[str] = []\n",
    "    if is_text_data_related(text):\n",
    "        for regex in COMPILED_DATASET_ID_REGEXES:\n",
    "            matches = regex.finditer(text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                dataset_id = text[match.start() : match.end()]\n",
    "                if is_small_context:\n",
    "                    dataset_ids.append(dataset_id)\n",
    "                else:\n",
    "                    citation_context = text[max(0, match.start() - context_chars): match.end() + context_chars ]\n",
    "                    citation_context = citation_context.replace('\\n', '').replace('[', '').replace(']', '')\n",
    "                    citation_context = re.sub(r'\\s+', ' ', citation_context).strip()\n",
    "                    if is_text_data_related(citation_context):\n",
    "                        occurrences_with_context.append(\"{\" + f'\"dataset_ids\": {[dataset_id]}, citation_context: \"{citation_context}\"' + \"}\")\n",
    "        if dataset_ids:\n",
    "            occurrences_with_context.append(\"{\" + f'\"dataset_ids\": {dataset_ids}, citation_context: \"{text}\"' + \"}\")\n",
    "    \n",
    "    # If no occurrences found, return an empty string\n",
    "    # Otherwise, join the occurrences with a specific separator\n",
    "    if not occurrences_with_context:\n",
    "        return \"\"\n",
    "    return \",\".join(occurrences_with_context)\n",
    "\n",
    "# Use NLP to get sentences from the given text\n",
    "\n",
    "def get_sentences_from_text(text: str, nlp=NLP_SPACY) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Replace all non-standard unicode dashes with '-'\n",
    "    text = clean_text(text)\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    doc_spacy = nlp(text)\n",
    "    return \"\\n\".join([sent.text for sent in doc_spacy.sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e6435",
   "metadata": {},
   "source": [
    "### XML Element Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0b7a53c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_element_text(element: Element | None) -> str:\n",
    "    if element is not None:\n",
    "        # Use itertext() to get all text content from the <p> tag and its descendants\n",
    "        # and join them into a single string.\n",
    "        all_text = \" \".join(element.itertext(tag=None)).replace('\\u200b', '').strip()\n",
    "        return all_text[:2000]\n",
    "    else:\n",
    "        return \"\"\n",
    "    \n",
    "def extract_next_sibling_text(elements: list[Element] | None, sibling_xpath: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from the next sibling of the given XML element.\n",
    "    \n",
    "    Args:\n",
    "        element (Element | None): The XML element whose next sibling's text is to be extracted.\n",
    "        sibling_xpath (str): The XPath expression to find the next sibling element. (eg. \"following-sibling::passage[1]\")\n",
    "        \n",
    "    Returns:\n",
    "        str: A string containing the text from the next sibling element, or an empty string if no sibling exists.\n",
    "\n",
    "    \"\"\"\n",
    "    # Check if the provided elements list is None or empty\n",
    "    if not elements:\n",
    "        return \"\"\n",
    "    \n",
    "    # Assuming there's only one such element, take the first one found\n",
    "    # and find the element immediately following based on the given sibling_xpath.\n",
    "    first_element = elements[0]\n",
    "    sibling_elements = first_element.xpath(sibling_xpath)\n",
    "\n",
    "    if not sibling_elements:\n",
    "        # print(\"DEBUG: No following <passage> element found.\") # Uncomment for debugging\n",
    "        return \"\"\n",
    "    \n",
    "    next_sibling = sibling_elements[0]\n",
    "    if next_sibling is None:\n",
    "        return \"\"\n",
    "    \n",
    "    return extract_element_text(next_sibling)\n",
    "\n",
    "def extract_elements_text(elements: list[Element] | None, sep: str = \" \") -> str:\n",
    "    elements_text = []\n",
    "    if elements is None:\n",
    "        return \"\"\n",
    "    \n",
    "    for element in elements:\n",
    "        text = extract_element_text(element)\n",
    "        if text:\n",
    "            elements_text.append(text)\n",
    "\n",
    "    return sep.join(elements_text).strip()\n",
    "\n",
    "def extract_elements_text_from_xpath_list(root: Element | None, xpath_list: list[str], ns: dict[str, str] | None = None) -> str:\n",
    "    elements_text = \"\"\n",
    "    if root is None or not xpath_list:\n",
    "        return \"\"\n",
    "    \n",
    "    for xpath in xpath_list:\n",
    "        element = root.find(xpath, namespaces=ns)\n",
    "        elements_text += extract_element_text(element)\n",
    "    return elements_text\n",
    "\n",
    "def extract_text_from_elements_within_element(element: Element | None, child_xpaths: list[str] = [], ns: dict[str, str] | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from elements within a given XML element that match the specified tag names.\n",
    "    \n",
    "    Args:\n",
    "        element (Element | None): The XML element to search within.\n",
    "        tag_names (list[str]): A list of tag names to search for.\n",
    "        \n",
    "    Returns:\n",
    "        str: A string containing the extracted text from the matching elements.\n",
    "    \"\"\"\n",
    "    if element is None:\n",
    "        return \"\"\n",
    "    \n",
    "    if not child_xpaths:\n",
    "        # If no child tag names are provided, return the text of the element itself\n",
    "        return extract_element_text(element)\n",
    "    \n",
    "    extracted_text = []\n",
    "    for xpath in child_xpaths:\n",
    "        for child in element.findall(xpath, namespaces=ns):\n",
    "            text = extract_element_text(child)\n",
    "            if text:\n",
    "                extracted_text.append(text)\n",
    "    \n",
    "    return \"|\".join(extracted_text)\n",
    "\n",
    "def extract_data_related_elements_text(elements: list[Element] | None, child_xpaths: list[str] = [], ns: dict[str, str] | None = None) -> list[str]:\n",
    "    elements_text = []\n",
    "    if elements is None:\n",
    "        return elements_text\n",
    "    \n",
    "    for element in elements:\n",
    "        text = extract_dataset_ids(extract_text_from_elements_within_element(element, child_xpaths, ns))\n",
    "        if text:\n",
    "            elements_text.append(text)\n",
    "\n",
    "    return elements_text\n",
    "\n",
    "def extract_data_related_elements_text_from_xpath_list(root: Element | None, xpath_list: list[str], ns: dict[str, str] | None = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts text from elements in the XML tree that match the provided XPath expressions.\n",
    "    \n",
    "    Args:\n",
    "        root (Element | None): The root element of the XML tree.\n",
    "        xpath_list (list[str]): A list of XPath expressions to search for elements.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: A list of extracted text from the matching elements.\n",
    "    \"\"\"\n",
    "    elements_text = []\n",
    "    if root is None or not xpath_list:\n",
    "        return elements_text\n",
    "    \n",
    "    for xpath in xpath_list:\n",
    "        primary_xpath, *child_xpath_text = xpath.split('||')\n",
    "        child_xpaths = child_xpath_text[0].split(',') if child_xpath_text else []\n",
    "        elements = root.findall(primary_xpath, namespaces=ns)\n",
    "        if elements:\n",
    "            elements_text.extend(extract_data_related_elements_text(elements, child_xpaths, ns))\n",
    "    return elements_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab6146",
   "metadata": {},
   "source": [
    "### PDF File Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "02f6ceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_author_names(full_text: str, nlp=NLP_SPACY) -> str:\n",
    "    \"\"\"\n",
    "    Extracts potential author names from the beginning of a research article's text\n",
    "    using spaCy's Named Entity Recognition. It attempts to isolate the author section\n",
    "    and applies heuristics to filter out non-author entities.\n",
    "\n",
    "    Args:\n",
    "        full_text (str): The complete text content of the research article,\n",
    "                         typically extracted from a PDF.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of unique strings, each representing a potential author name,\n",
    "                   sorted alphabetically. Returns an empty list if no authors are found.\n",
    "    \"\"\"\n",
    "    if not full_text or not full_text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    full_text = full_text.replace('1\\n,', ',').replace('1,', ',').replace('\\u2019', \"'\")\n",
    "\n",
    "    # 1. Isolate the potential author section\n",
    "    # Authors are typically at the very beginning, before the abstract or introduction.\n",
    "    # We'll search for common section headers to define the end of the author block.\n",
    "    # Using regex for case-insensitive search and handling various newline/spacing.\n",
    "    header_patterns = [\n",
    "        r\"\\n\\s*Abstract\\s*\\n\",\n",
    "        r\"\\n\\s*Introduction\\s*\\n\",\n",
    "        r\"\\n\\s*Summary\\s*\\n\",\n",
    "        r\"\\n\\s*Keywords\\s*\\n\",\n",
    "        r\"\\n\\s*Graphical Abstract\\s*\\n\",\n",
    "        r\"\\n\\s*1\\.\\s*Introduction\\s*\\n\", # Common for numbered sections\n",
    "        r\"\\n\\s*DOI:\\s*\\n\" # Sometimes DOI appears before abstract\n",
    "    ]\n",
    "\n",
    "    author_section_end_index = len(full_text)\n",
    "    for pattern in header_patterns:\n",
    "        match = re.search(pattern, full_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            # Take text up to the start of the found header\n",
    "            author_section_end_index = min(author_section_end_index, match.start())\n",
    "            break\n",
    "    \n",
    "    # As a fallback or if no header is found early, limit the search to the first\n",
    "    # 2500 characters. This prevents processing the entire document for authors.\n",
    "    author_section_text = full_text[:min(author_section_end_index, 2500)]\n",
    "\n",
    "    if not author_section_text.strip():\n",
    "        return \"\"\n",
    "\n",
    "    # 2. Process the isolated author section with spaCy\n",
    "    doc = nlp(author_section_text)\n",
    "\n",
    "    # 3. Extract PERSON entities and apply initial filtering\n",
    "    potential_authors: list[str] = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            name = ent.text.strip()\n",
    "            # Basic filtering to reduce false positives:\n",
    "            # - Exclude very short strings (e.g., single letters, common conjunctions)\n",
    "            # - Exclude common stop words (e.g., \"The\", \"And\")\n",
    "            # - Exclude all-uppercase strings that might be acronyms (e.g., \"WHO\", \"NASA\")\n",
    "            # - Ensure it contains at least one space (e.g., \"John Doe\") or is a capitalized\n",
    "            #   single word that's longer than 2 characters (e.g., \"Smith\").\n",
    "            if (len(name) > 1 and\n",
    "                name.lower() not in nlp.Defaults.stop_words and\n",
    "                not name.isupper() and\n",
    "                (' ' in name or (name[0].isupper() and len(name) > 2))):\n",
    "                \n",
    "                potential_authors.append(name)\n",
    "\n",
    "    # 4. Apply more advanced heuristics to filter out non-author names\n",
    "    # This step is crucial for accuracy and often requires tuning.\n",
    "    filtered_authors = []\n",
    "    for author in potential_authors:\n",
    "        # Heuristic 1: Filter out names that contain common affiliation keywords.\n",
    "        # This is a simple check; more robust solutions might use spaCy's dependency\n",
    "        # parsing to check if a PERSON entity is part of an ORG entity.\n",
    "        affiliation_keywords = [\"univ\", \"observ\", \"institute\", \"department\", \"center\", \"lab\",\n",
    "                                \"hospital\", \"college\", \"school\", \"inc.\", \"ltd.\", \"company\",\n",
    "                                \"corp.\", \"group\", \"foundation\", \"research\"]\n",
    "        if any(keyword in author.lower() for keyword in affiliation_keywords):\n",
    "            continue # Skip if it looks like an affiliation\n",
    "\n",
    "        # Heuristic 2: Filter out names that contain email patterns or ORCID patterns.\n",
    "        if '@' in author or re.search(r'\\b\\d{4}-\\d{4}-\\d{4}-\\d{3}[\\dX]\\b', author):\n",
    "            continue # Skip if it contains an email or ORCID\n",
    "\n",
    "        # Heuristic 3: Filter out names that are likely just initials or very short.\n",
    "        # This is partially covered by initial filtering, but can be refined.\n",
    "        # E.g., \"J. D.\" might be an author, but \"J.\" alone is unlikely.\n",
    "        if len(author.split()) == 1 and len(author) <= 2 and author.isupper():\n",
    "            continue # Skip single-letter or two-letter uppercase (e.g., \"JD\")\n",
    "\n",
    "        filtered_authors.append(author)\n",
    "\n",
    "    # Convert to list and sort for consistent output\n",
    "    return filtered_authors[0] if filtered_authors else \"\"\n",
    "\n",
    "def extract_pdf_doc_text(pdf_doc: fitz.Document)  -> dict[str, str | list[str]]:\n",
    "    \"\"\"\n",
    "    Extracts all text from a PDF document using PyMuPDF.\n",
    "    \n",
    "    Args:\n",
    "        pdf_doc (fitz.Document): The PDF document to extract text from.\n",
    "        \n",
    "    Returns:\n",
    "        str: A JSON string of the article_dict containing specific elements extracted from the PDF.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the article dictionary with empty strings\n",
    "    article_dict = {\n",
    "        'title': '',\n",
    "        'author': '',\n",
    "        'abstract': '',\n",
    "        'data_availability': '',\n",
    "        'other_dataset_citations': []\n",
    "    }\n",
    "\n",
    "    # Initialize variables for text extraction\n",
    "    p1 = None  # Placeholder for the first page text\n",
    "    other_dataset_citations = set()  # Use a set to avoid duplicates\n",
    "    for page in pdf_doc:\n",
    "        # Extract text from the page\n",
    "        textpage = page.get_textpage()\n",
    "        if page.number == 0:\n",
    "            p1_txt = textpage.extractTEXT()\n",
    "            p1 = get_sentences_from_text(p1_txt)\n",
    "            p1 = p1[:int(len(p1)/2)]\n",
    "            article_dict['author'] = extract_author_names(p1_txt, nlp=NLP_SPACY)\n",
    "\n",
    "        # Extract text from all blocks that have an abstract or dataset id's\n",
    "        blocks = textpage.extractBLOCKS()\n",
    "        for block in blocks:\n",
    "            block_text = get_sentences_from_text(block[4])\n",
    "            block_text_lower = block_text.lower()\n",
    "            if page.number == 0 and len(block_text) > 100 and \"abstract\" in block_text_lower:\n",
    "                # Add the abstract block text to the article dictionary\n",
    "                article_dict['abstract'] = block_text\n",
    "            elif \"data availability\" in block_text_lower or \"data accessibility\" in block_text_lower or \"acknowledgments\" in block_text_lower:\n",
    "                # Add the data availability block text to the article dictionary\n",
    "                article_dict['data_availability'] = block_text\n",
    "            else:\n",
    "                context_chars = min(250, len(block_text))  # Use a minimum\n",
    "                dataset_ids_found = extract_dataset_ids(block_text, context_chars)  # Extract dataset IDs from the block text\n",
    "                if dataset_ids_found:\n",
    "                    if len(article_dict['data_availability']) > 0 and len(article_dict['data_availability']) < 25:\n",
    "                        # If data availability text is only a few characters, append the next block text to it\n",
    "                        # This is a heuristic to ensure that we capture relevant dataset IDs\n",
    "                        article_dict['data_availability'] = block_text\n",
    "                    else:\n",
    "                        # Append the dataset IDs found in the block to the other_dataset_citations\n",
    "                        other_dataset_citations.add(dataset_ids_found)\n",
    "\n",
    "    article_dict['other_dataset_citations'] = list(other_dataset_citations) if other_dataset_citations else []\n",
    "    \n",
    "    # If an abstract was not found, use the first page text as the abstract\n",
    "    if not article_dict['abstract'] and p1:\n",
    "        article_dict['abstract'] = p1\n",
    "\n",
    "    # Return the article dictionary as a JSON string\n",
    "    return article_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2690ab4",
   "metadata": {},
   "source": [
    "### XML File Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8d341bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_xml_text_jats(root: Element) -> dict[str, str | list[str]]:\n",
    "    # Find the title, abstract, and data availablity info for Journal Archiving and Interchange DTD (JATS)\n",
    "    # The \".//\" ensures it searches anywhere in the document, not just direct children of root.\n",
    "    ns = None  # No namespaces for JATS\n",
    "\n",
    "    xpath_title = \".//article-title\"\n",
    "    xpath_authors_1 = \".//contrib-group/contrib[@contrib-type='author']/name\"\n",
    "    xpath_authors_2 = \".//biblstruct/analytic/author[@role='corresp']/persname\"\n",
    "    author = extract_element_text(root.find(xpath_authors_1, namespaces=ns))\n",
    "    if not author:\n",
    "        author = extract_element_text(root.find(xpath_authors_2, namespaces=ns))\n",
    "    xpath_abstract = \".//abstract\"\n",
    "    xpath_data_avails = [\".//notes[@notes-type='data-availability']\", \".//sec[@sec-type='data-availability']\"]\n",
    "    xpath_citations = [\".//element-citation||.article-title,.source,.pub-id\", \".//mixed-citation\"]  # List of XPath expressions for citations\n",
    "\n",
    "    return {\n",
    "        'title': extract_element_text(root.find(xpath_title, ns)),\n",
    "        'author': author,\n",
    "        'abstract': extract_element_text(root.find(xpath_abstract, ns)),\n",
    "        'data_availability': extract_elements_text_from_xpath_list(root, xpath_data_avails, ns=ns),\n",
    "        'other_dataset_citations': extract_data_related_elements_text_from_xpath_list(root, xpath_citations, ns=ns),\n",
    "    }\n",
    "\n",
    "def extract_xml_text_tei(root: Element) -> dict[str, str | list[str]]:\n",
    "    # Find the title, abstract, and data availability info for Text Encoding Initiative (TEI)\n",
    "    # Set the namespace for TEI\n",
    "    ns = {'tei': 'http://www.tei-c.org/ns/1.0'}\n",
    "\n",
    "    xpath_title = \".//tei:title\"\n",
    "    xpath_authors = \".//tei:sourcedesc/tei:biblstruct/tei:analytic/tei:author/tei:persname\"\n",
    "    xpath_abstract = \".//tei:abstract\"\n",
    "    xpath_data_avail = \"\" #\".//tei:biblstruct\"\n",
    "    xpath_citations = [\".//tei:biblstruct||.//tei:title,.//tei:idno,.//tei:notes\"]  # List of XPath expressions for citations\n",
    "        \n",
    "    return {\n",
    "        'title': extract_element_text(root.find(xpath_title, namespaces=ns)),\n",
    "        'author': extract_element_text(root.find(xpath_authors, namespaces=ns)),\n",
    "        'abstract': extract_element_text(root.find(xpath_abstract, namespaces=ns)),\n",
    "        'data_availability': xpath_data_avail,  # No direct extraction for TEI data_availability\n",
    "        'other_dataset_citations': extract_data_related_elements_text_from_xpath_list(root, xpath_citations, ns=ns),\n",
    "    }\n",
    "\n",
    "def extract_xml_text_wiley(root: Element) -> dict[str, str | list[str]]:\n",
    "    # Find the title, abstract, and data availability info for Wiley XML format\n",
    "    # Set the namespace for Wiley\n",
    "    ns = {'ns': 'http://www.wiley.com/namespaces/wiley'}\n",
    "\n",
    "    xpath_title = \".//ns:publicationMeta[@level='part']/ns:titleGroup\"    #<publicationMeta level=\"part\"><titleGroup><title type=\"main\">\n",
    "    xpath_authors = \".//selfCitationGroup/citation[@type='self']/author\"\n",
    "    xpath_abstract = \".//ns:abstract[@type='main']\"  #<abstract type=\"main\"\n",
    "    xpath_data_avail = \".//ns:section[@type='dataAvailability']\"  #<section numbered=\"no\" type=\"dataAvailability\"\n",
    "    xpath_citations = [\".//ns:citation||.//ns:articleTitle,.//ns:journalTitle,.//ns:url\"]  # List of XPath expressions for citations\n",
    "        \n",
    "    return {\n",
    "        'title': extract_elements_text(root.findall(xpath_title, namespaces=ns)),\n",
    "        'author': extract_element_text(root.find(xpath_authors, namespaces=ns)),\n",
    "        'abstract': extract_element_text(root.find(xpath_abstract, namespaces=ns)),\n",
    "        'data_availability': extract_element_text(root.find(xpath_data_avail, namespaces=ns)),\n",
    "        'other_dataset_citations': extract_data_related_elements_text_from_xpath_list(root, xpath_citations, ns=ns),\n",
    "    }\n",
    "\n",
    "def extract_xml_text_biorxiv(root: Element) -> dict[str, str | list[str]]:\n",
    "    # Find the title, abstract, and data availability info for BioRxiv XML format\n",
    "    # Set the namespace for BioRxiv\n",
    "    ns = {'biorxiv': 'http://www.biorxiv.org'}\n",
    "\n",
    "    xpath_title = \".//biorxiv:title\"\n",
    "    xpath_authors = \".//biorxiv:contrib[@contrib-type='author']/biorxiv:name\"\n",
    "    xpath_abstract = \".//biorxiv:abstract\"\n",
    "    xpath_data_avail = \".//biorxiv:sec[@sec-type='data-availability']\"  #<sec sec-type=\"data-availability\"\n",
    "    xpath_citations = [\".//biorxiv:biblio||.//biorxiv:title,.//biorxiv:source,.//biorxiv:pub-id\"]  # List of XPath expressions for citations\n",
    "        \n",
    "    return {\n",
    "        'title': extract_element_text(root.find(xpath_title, namespaces=ns)),\n",
    "        'author': extract_element_text(root.find(xpath_authors, namespaces=ns)),\n",
    "        'abstract': extract_element_text(root.find(xpath_abstract, namespaces=ns)),\n",
    "        'data_availability': extract_element_text(root.find(xpath_data_avail, namespaces=ns)),\n",
    "        'other_dataset_citations': extract_data_related_elements_text_from_xpath_list(root, xpath_citations, ns=ns),\n",
    "    }\n",
    "\n",
    "def extract_xml_text_bioc(root: Element) -> dict[str, str | list[str]]:\n",
    "    # Find the title, abstract, and data availability info for BioC-API XML format\n",
    "    ns = None  # No namespaces for BioC\n",
    "\n",
    "    xpath_title = \"string(.//passage[infon[@key='section_type' and text()='TITLE']]/text)\"\n",
    "    xpath_authors = \"string(.//infon[@key='name_0'] | .//infon[@key='name_1'])\"\n",
    "    xpath_abstract = \"string(.//passage[infon[@key='section_type' and text()='ABSTRACT']]/text)\"\n",
    "    xpath_data_avail = \".//passage[text[text()='DATA ACCESSIBILITY:']]\"\n",
    "    xpath_data_avail_sibling = \"following-sibling::passage[1]\"\n",
    "    xpath_citations = []\n",
    "        \n",
    "    return {\n",
    "        'title': root.xpath(xpath_title, namespaces=ns),\n",
    "        'author': root.xpath(xpath_authors, namespaces=ns).strip().replace('surname:', '').replace(';given-names:', ' '),\n",
    "        'abstract': root.xpath(xpath_abstract, namespaces=ns)[:2000],  # Limit to 2000 characters\n",
    "        'data_availability': extract_next_sibling_text(root.xpath(xpath_data_avail, namespaces=ns), xpath_data_avail_sibling),\n",
    "        'other_dataset_citations': xpath_citations,\n",
    "    }\n",
    "\n",
    "def extract_xml_text_taxonx(root: Element) -> dict[str, str | list[str]]:\n",
    "    # Find the title, abstract, and data availability info for TaxonX format\n",
    "    ns = None  # No namespaces for Taxonomic Treatment Publishing DTD\n",
    "\n",
    "    xpath_title = \"string(.//article-meta/title-group/article-title)\"\n",
    "    xpath_authors = \"\"\n",
    "    xpath_abstract = \"string(.//article-meta/abstract)\"\n",
    "    xpath_data_avail = \"\"\n",
    "    xpath_citations = []\n",
    "        \n",
    "    return {\n",
    "        'title': root.xpath(xpath_title, namespaces=ns),\n",
    "        'author': xpath_authors,\n",
    "        'abstract': root.xpath(xpath_abstract, namespaces=ns)[:2000],  # Limit to 2000 characters\n",
    "        'data_availability': xpath_data_avail,  # No direct extraction for TaxonX data_availability\n",
    "        'other_dataset_citations': xpath_citations,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229f8e8",
   "metadata": {},
   "source": [
    "## File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "15e0697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dictionary mapping XML types to their respective extraction functions\n",
    "XML_TYPE_EXTRACTORS = {\n",
    "    'jats': extract_xml_text_jats,\n",
    "    'tei': extract_xml_text_tei,\n",
    "    'wiley': extract_xml_text_wiley,\n",
    "    'bioc': extract_xml_text_bioc,\n",
    "    'taxonx': extract_xml_text_taxonx,\n",
    "}\n",
    "\n",
    "# --- Data Loading ---\n",
    "def get_file_extension(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the file extension of the given file path.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the file.\n",
    "        \n",
    "    Returns:\n",
    "        str: The file extension, or an empty string if no extension is found.\n",
    "    \"\"\"\n",
    "    _, ext = os.path.splitext(file_path)\n",
    "    return ext.lower() if ext else \"\"\n",
    "\n",
    "def read_first_line_of_xml(file_path: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Reads and returns the first line of an XML file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "\n",
    "    Returns:\n",
    "        str | None: The first line of the file, stripped of leading/trailing whitespace,\n",
    "                    or None if the file cannot be read or is empty.\n",
    "    \"\"\"\n",
    "    if not file_path and not os.path.exists(file_path):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline().replace('<?xml version=\"1.0\" encoding=\"UTF-8\"?>', '').replace('<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>', '').strip()\n",
    "            # If the first line is empty, read the next line\n",
    "            if not first_line:\n",
    "                first_line = f.readline()\n",
    "            return first_line.strip()[:90] if first_line else None\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='iso-8859-1') as f:\n",
    "                first_line = f.readline().replace('<?xml version=\"1.0\" encoding=\"UTF-8\"?>', '').replace('<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>', '').strip()\n",
    "                if not first_line:\n",
    "                    first_line = f.readline()\n",
    "                return first_line.strip()[:90] if first_line else None\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file '{file_path}': {e}\")\n",
    "        return None\n",
    "    \n",
    "def identify_xml_type(first_line: str) -> str:\n",
    "    \"\"\"\n",
    "    Identifies the XML type based on the first line of the XML file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "        \n",
    "    Returns:\n",
    "        str: The identified XML type ('jats', 'tei', 'wiley', 'bioc', or 'unknown').\n",
    "    \"\"\"\n",
    "    if not first_line:\n",
    "        return \"unknown\"\n",
    "    first_line_lower = first_line.lower()\n",
    "    # Check for specific patterns in the first line\n",
    "    if 'journal archiving and interchange dtd' in first_line_lower:\n",
    "        return \"jats\"\n",
    "    elif 'xmlns=\"http://www.tei-c.org/ns/1.0\"' in first_line_lower:\n",
    "        return \"tei\"\n",
    "    elif 'xmlns=\"http://www.wiley.com/namespaces/wiley\"' in first_line_lower:\n",
    "        return \"wiley\"\n",
    "    elif 'bioc.dtd' in first_line_lower or 'bioc-api' in first_line_lower:\n",
    "        return \"bioc\"\n",
    "    elif 'taxonomic treatment publishing dtd' in first_line_lower:\n",
    "        return \"taxonx\"\n",
    "    \n",
    "    return \"unknown\"    \n",
    "\n",
    "def get_xml_type(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Determines the XML type of a file based on its first line.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "        \n",
    "    Returns:\n",
    "        str: The identified XML type ('jats', 'tei', 'wiley', 'bioc', 'taxonx', or 'unknown').\n",
    "    \"\"\"\n",
    "    first_line = \"\"\n",
    "    if \".xml\" == get_file_extension(file_path):\n",
    "        # If the file is an XML file, read the first line and identify the type\n",
    "        first_line = read_first_line_of_xml(file_path)\n",
    "    return identify_xml_type(first_line) if first_line else \"unknown\"\n",
    "\n",
    "def load_file_paths(dataset_type_dir: str) -> pd.DataFrame: \n",
    "    pdf_path = os.path.join(dataset_type_dir, 'PDF')\n",
    "    xml_path = os.path.join(dataset_type_dir, 'XML')\n",
    "    dataset_type = os.path.basename(dataset_type_dir)\n",
    "    pdf_files = [f for f in os.listdir(pdf_path) if f.endswith('.pdf')]\n",
    "    xml_files = [f for f in os.listdir(xml_path) if f.endswith('.xml')]\n",
    "    df_pdf = pd.DataFrame({\n",
    "        'article_id': [f.replace('.pdf', '') for f in pdf_files],\n",
    "        'pdf_file_path': [os.path.join(pdf_path, f) for f in pdf_files]\n",
    "    })\n",
    "    df_xml = pd.DataFrame({\n",
    "        'article_id': [f.replace('.xml', '') for f in xml_files],\n",
    "        'xml_file_path': [os.path.join(xml_path, f) for f in xml_files]\n",
    "    })\n",
    "    merge_df = pd.merge(df_pdf, df_xml, on='article_id', how='outer', suffixes=('_pdf', '_xml'), validate=\"one_to_many\")\n",
    "    merge_df['dataset_type'] = dataset_type\n",
    "    return merge_df\n",
    "\n",
    "def extract_pdf_text(file_path: str, xml_type: str | None = None) -> dict[str, str | list[str]]:\n",
    "    \"\"\"Extracts all text from a PDF file using PyMuPDF.\"\"\"\n",
    "    article_dict = {}\n",
    "    if file_path and os.path.exists(file_path):\n",
    "        try:\n",
    "            with fitz.open(file_path) as doc:\n",
    "                article_dict = extract_pdf_doc_text(doc)  # Extract text from the PDF document\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"PDF file not found: {file_path}\")\n",
    "    \n",
    "    return article_dict\n",
    "\n",
    "def extract_xml_text(file_path: str, xml_type: str) -> dict[str, str | list[str]]:\n",
    "    \"\"\"Reads and extracts text from an XML file based on the specified XML type.\n",
    "    Args:\n",
    "        file_path (str): The path to the XML file.\n",
    "        xml_type (str): The type of XML format (e.g., 'jats', 'tei', 'wiley', 'bioc', 'taxonx').\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted text from the XML file.\n",
    "    \"\"\"\n",
    "    # Initialize the article dictionary\n",
    "    article_dict = {}\n",
    "    if file_path and os.path.exists(file_path):\n",
    "        # Disable external entity resolution for security\n",
    "        parser = etree.XMLParser(resolve_entities=False, no_network=True)\n",
    "        try:\n",
    "            tree = etree.parse(file_path, parser)\n",
    "            root = tree.getroot()\n",
    "            # Use the appropriate extraction function based on the xml_type\n",
    "            extract_function = XML_TYPE_EXTRACTORS.get(xml_type, extract_xml_text_jats)  \n",
    "            article_dict = extract_function(root)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading XML {file_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"XML file not found: {file_path}\")    \n",
    "    return article_dict\n",
    "\n",
    "def process_unsupported_file(file_path: str, xml_type: str | None = None) -> str:\n",
    "    return f\"Unsupported file type for: {file_path}\"\n",
    "\n",
    "# Dictionary mapping file extensions to loading functions\n",
    "FILE_EXTRACTORS = {\n",
    "    '.xml': extract_xml_text,\n",
    "    '.pdf': extract_pdf_text,\n",
    "}\n",
    "\n",
    "def extract_article_text(file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Loads text content from a single article file (PDF or XML).\n",
    "    Returns the text content of the given file.\n",
    "    \"\"\"\n",
    "    text_content = \"\"\n",
    "\n",
    "    # Get the file extension (e.g., '.xml', '.pdf')\n",
    "    file_extension = get_file_extension(file_path)\n",
    "\n",
    "    # Get the XML type if the file is an XML file\n",
    "    xml_type = get_xml_type(file_path)\n",
    "\n",
    "    # Get the appropriate function from the dictionary,\n",
    "    # or fall back to a default 'unsupported' function if not found.\n",
    "    extract_function = FILE_EXTRACTORS.get(file_extension, process_unsupported_file)\n",
    "\n",
    "    # Call the selected function\n",
    "    article_dict = extract_function(file_path, xml_type=xml_type)\n",
    "    text_content = json.dumps(article_dict, separators=(',', ':'))\n",
    "    print(f\"Extracted text from {file_path}. Length: {len(text_content)} characters, xml_type: {xml_type}\")\n",
    "\n",
    "    return text_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5f45bff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\train\\XML\\10.1007_s00382-022-06361-7.xml. Length: 1888 characters, xml_type: tei\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1888"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'{\"title\":\"Winter wet-dry weather patterns driving atmospheric rivers and Santa Ana winds provide evidence for increasing wildfire hazard in California\",\"author\":\"Kristen Guirguis\",\"abstract\":\"Floods caused by atmospheric rivers and wildfires fanned by Santa Ana winds are common occurrences in California with devastating societal impacts. In this work, we show that winter weather variability in California, including the occurrence of extreme and impactful events, is linked to four atmospheric circulation regimes over the North Pacific Ocean previously named and identified as the \\\\\"NP4 modes\\\\\". These modes come in and out of phase with each other during the season, resulting in distinct weather patterns that recur throughout the historical record. Some phase combinations favor atmospheric river landfalls and extreme daily or multi-day precipitation, while other phase combinations favor anomalously hot weather and drying Santa Ana wind conditions over Southern California. This historical perspective of atmospheric circulation and impacts over 70 years reveals that weather patterns are changing in a way that enhances wildfire hazard in California, while the frequency of weather patterns linked to historical floods is not diminishing. These changes highlight the rising hazards of cascading weather extremes in California\\'s present and future.\",\"data_availability\":\"\",\"other_dataset_citations\":[\"{\\\\\"dataset_ids\\\\\": [\\'10.6075/J0154FJJ\\'], citation_context: \\\\\"Data from: Four atmospheric circulation regimes over the North Pacific and their relationship to California precipitation on daily to seasonal timescales|UC San Diego Library Digital Collections|10.6075/J0154FJJ\\\\\"}\",\"{\\\\\"dataset_ids\\\\\": [\\'10.6075/J089161B\\'], citation_context: \\\\\"Historical catalog of winter weather regimes impacting California, 1949-2017|UC San Diego Library Digital Collections|10.6075/J089161B\\\\\"}\"]}'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test extracting text from various PDF and XML files\n",
    "pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1002_2017jc013030.pdf')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1002_2017jc013030.xml')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TEST_DIR, 'PDF', '10.1002_ecs2.1280.pdf')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1017_rdc.2022.19.pdf')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1017_s0007123423000601.pdf')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.3389_fcimb.2024.1292467.pdf')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1002_esp.5058.pdf') # This one is big\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1002_esp.5059.pdf') # This one is big\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1002_ece3.4466.pdf') # dryad\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1002_ece3.4466.xml') # dryad\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1002_mp.14424.xml')\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1007_s00259-022-06053-8.xml')    # jats\n",
    "pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1007_s00382-022-06361-7.xml')    # tei\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1111_1365-2435.13431.xml')       # wiley\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1111_mec.16977.xml')             # bioc\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.3897_zoologia.35.e23481.xml')      # taxonx\n",
    "# pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'XML', '10.1002_ece3.6144.xml')               # jats\n",
    "article_text = extract_article_text(pdf_file_path)\n",
    "display(len(article_text))\n",
    "article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dbdf50",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "29d7f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labeled training data from: ./kaggle/input/make-data-count-finding-data-references\\train_labels.csv\n",
      "Training labels shape: (1028, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "54b0be65-f833-4cfb-84b3-5ec3e1a05329",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "https://doi.org/10.17882/49388",
         "Primary"
        ],
        [
         "1",
         "10.1002_anie.201916483",
         "Missing",
         "Missing"
        ],
        [
         "2",
         "10.1002_anie.202005531",
         "Missing",
         "Missing"
        ],
        [
         "3",
         "10.1002_anie.202007717",
         "Missing",
         "Missing"
        ],
        [
         "4",
         "10.1002_chem.201902131",
         "Missing",
         "Missing"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/49388</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1002_anie.202007717</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1002_chem.201902131</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                      dataset_id     type\n",
       "0    10.1002_2017jc013030  https://doi.org/10.17882/49388  Primary\n",
       "1  10.1002_anie.201916483                         Missing  Missing\n",
       "2  10.1002_anie.202005531                         Missing  Missing\n",
       "3  10.1002_anie.202007717                         Missing  Missing\n",
       "4  10.1002_chem.201902131                         Missing  Missing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the labeled training data CSV file\n",
    "print(f\"Loading labeled training data from: {LABELED_TRAINING_DATA_CSV_PATH}\")\n",
    "train_labels_df = pd.read_csv(LABELED_TRAINING_DATA_CSV_PATH)\n",
    "\n",
    "print(f\"Training labels shape: {train_labels_df.shape}\")\n",
    "display(train_labels_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "560a47e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example grouped training data for article_id '10.1002_2017jc013030': [{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]\n"
     ]
    }
   ],
   "source": [
    "# Group training data by article_id to get all datasets for each article\n",
    "# This creates a dictionary where keys are article_ids and values are lists of dataset dicts\n",
    "grouped_training_data = {}\n",
    "for article_id, group_df in train_labels_df.groupby('article_id'):\n",
    "    grouped_training_data[article_id] = group_df[['dataset_id', 'type']].to_dict('records')\n",
    "\n",
    "# Example usage of grouped_training_data\n",
    "print(f\"Example grouped training data for article_id '10.1002_2017jc013030': {grouped_training_data['10.1002_2017jc013030']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5d2b7050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base file dir for the articles to be processed\n",
    "base_file_dir = ARTICLE_TEST_DIR \\\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN') \\\n",
    "    else ARTICLE_TRAIN_DIR\n",
    "\n",
    "# Just for testing, always set to the ARTICLE_TEST_DIR\n",
    "base_file_dir = ARTICLE_TEST_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "34f61a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files paths shape: (30, 6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_info",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "xml_type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "012b2348-5a66-4573-af41-c2be96c125e2",
       "rows": [
        [
         "17",
         "10.1002_ece3.6784",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6784.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.6784.xml",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "jats"
        ],
        [
         "5",
         "10.1002_chem.201903120",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.201903120.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.201903120.xml",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "jats"
        ],
        [
         "0",
         "10.1002_2017jc013030",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_2017jc013030.xml",
         "test",
         "[{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]",
         "tei"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>dataset_info</th>\n",
       "      <th>xml_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.1002_ece3.6784</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>jats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1002_chem.201903120</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>jats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.17882/4938...</td>\n",
       "      <td>tei</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                article_id                                      pdf_file_path  \\\n",
       "17       10.1002_ece3.6784  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "5   10.1002_chem.201903120  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "0     10.1002_2017jc013030  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                        xml_file_path dataset_type  \\\n",
       "17  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "5   ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "0   ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "\n",
       "                                         dataset_info xml_type  \n",
       "17     [{'dataset_id': 'Missing', 'type': 'Missing'}]     jats  \n",
       "5      [{'dataset_id': 'Missing', 'type': 'Missing'}]     jats  \n",
       "0   [{'dataset_id': 'https://doi.org/10.17882/4938...      tei  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load file paths for training and testing datasets\n",
    "file_paths_df = load_file_paths(base_file_dir)\n",
    "file_paths_df['xml_file_path'] = file_paths_df['xml_file_path'].fillna('')\n",
    "\n",
    "# Merge the file paths with the grouped_training_data\n",
    "file_paths_df['dataset_info'] = file_paths_df['article_id'].map(grouped_training_data)\n",
    "\n",
    "# Get the xml type for each file based on the first line of the XML file\n",
    "file_paths_df['xml_type'] = file_paths_df['xml_file_path'].apply(get_xml_type)\n",
    "\n",
    "print(f\"Files paths shape: {file_paths_df.shape}\")\n",
    "display(file_paths_df.sample(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "e30a9aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf. Length: 6686 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.201916483.pdf. Length: 1336 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202005531.pdf. Length: 1622 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202007717.pdf. Length: 1710 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.201902131.pdf. Length: 648 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.201903120.pdf. Length: 872 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202000235.pdf. Length: 686 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202001412.pdf. Length: 972 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202001668.pdf. Length: 436 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202003167.pdf. Length: 917 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_cssc.202201821.pdf. Length: 3861 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.3985.pdf. Length: 3948 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.4466.pdf. Length: 2585 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.5260.pdf. Length: 2717 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.5395.pdf. Length: 3782 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6144.pdf. Length: 2886 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6303.pdf. Length: 3198 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6784.pdf. Length: 6656 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.961.pdf. Length: 3187 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.9627.pdf. Length: 2723 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.1280.pdf. Length: 3277 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.4619.pdf. Length: 3440 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejic.201900904.pdf. Length: 620 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejoc.202000139.pdf. Length: 1368 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejoc.202000916.pdf. Length: 298 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_esp.5058.pdf. Length: 4457 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_esp.5090.pdf. Length: 5294 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_mp.14424.pdf. Length: 4896 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_nafm.10870.pdf. Length: 4969 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1007_jhep07(2018)134.pdf. Length: 877 characters, xml_type: unknown\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_2017jc013030.xml. Length: 1447 characters, xml_type: tei\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.201916483.xml. Length: 1028 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.202005531.xml. Length: 233 characters, xml_type: bioc\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.202007717.xml. Length: 1406 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.201902131.xml. Length: 1186 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.201903120.xml. Length: 1935 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.202000235.xml. Length: 804 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.202001412.xml. Length: 1160 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.202001668.xml. Length: 833 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.202003167.xml. Length: 1868 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_cssc.202201821.xml. Length: 1560 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.3985.xml. Length: 1644 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.4466.xml. Length: 1780 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.5260.xml. Length: 2471 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.5395.xml. Length: 3731 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.6144.xml. Length: 2169 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.6303.xml. Length: 2579 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.6784.xml. Length: 2767 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.961.xml. Length: 1699 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.9627.xml. Length: 2470 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ecs2.4619.xml. Length: 1997 characters, xml_type: tei\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ejic.201900904.xml. Length: 1013 characters, xml_type: wiley\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ejoc.202000916.xml. Length: 586 characters, xml_type: jats\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_esp.5090.xml. Length: 2703 characters, xml_type: wiley\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_mp.14424.xml. Length: 2769 characters, xml_type: jats\n",
      "File paths DataFrame shape: (30, 8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_info",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e71c5875-298e-4c73-8cf6-783301d489c9",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_2017jc013030.xml",
         "test",
         "[{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]",
         "tei",
         "{\"title\":\"\",\"author\":\"Marie Barbieux\",\"abstract\":\"Abstract Characterizing phytoplankton distribution and dynamics in the world\\u2019s open oceans requires in situ observations over a broad range of space and time scales.\\nIn addition to temperature/salinity measure-ments, Biogeochemical-Argo (BGC-Argo) profiling floats are capable of autonomously observing at high-frequency bio-optical properties such as the chlorophyll fluorescence, a proxy of the chlorophyll a concen-tration (Chla), the particulate backscattering coefficient (bbp), a proxy of the stock of particulate organic car-bon, and the light available for photosynthesis.\\nWe analyzed an unprecedented BGC-Argo database of more than 8,500 multivariable profiles collected in various oceanic conditions, from subpolar waters to subtropical gyres.\\nOur objective is to refine previously established Chla versus bbp relationships and gain insights into the sources of vertical, seasonal, and regional variability in this relationship.\\nDespite some regional, seasonal and vertical variations, a general covariation occurs at a global scale.\\nWe distinguish two main contrasted situations: (1) concomitant changes in Chla and bbp that correspond to actual variations in phytoplankton biomass, e.g., in subpolar regimes; (2) a decoupling between the two variables attributed to photoacclima-tion or changes in the relative abundance of nonalgal particles, e.g., in subtropical regimes.\\nThe variability in the bbp:Chla ratio in the surface layer appears to be essentially influenced by the type of particles and by photoacclimation processes.\\nThe large BGC-Argo database helps identifying the spatial and temporal scales at which this ratio is predominantly driven by one or the other of these two factors.\",\"data_availability\":\"Acknowledgments This paper represents a contribution to the following research projects: remOcean (funded by the European Research Council, grant 246777), NAOS (funded by the Agence Nationale de la Recherche in the frame of the French \\u2018\\u2018Equipement d\\u2019avenir\\u2019\\u2019 program, grant ANR J11R107-F), the SOCLIM (Southern Ocean and climate) project supported by the French research program LEFE-CYBER of INSU-CNRS, the Climate Initiative of the foundation BNP Paribas and the French polar institute (IPEV), AtlantOS (funded by the European Union\\u2019s Horizon 2020 Research and Innovation program, grant 2014- 633211), E-AIMS (funded by the European Commission\\u2019s FP7 project, grant 312642), U.K. Bio-Argo (funded by the British Natural Environment Research Council-NERC, grant NE/L012855/1), REOPTIMIZE (funded by the European Union\\u2019s Horizon 2020 Research and Innovation program, Marie Sk\\u0142odowska-Curie grant 706781), Argo-Italy (funded by the Italian Ministry of Education, University and Research -MIUR), and the French Bio-Argo program (BGC-Argo France; funded by CNES-TOSCA, LEFE Cyber, and GMMC).\\nWe thank the PIs of several BGC-Argo floats missions and projects: Giorgio Dall\\u2019Olmo (Plymouth Marine Laboratory, United Kingdom; E-AIMS and U.K. Bio-Argo); Kjell-Arne Mork (Institute of Marine Research, Norway; E-AIMS); Violeta Slabakova (Bulgarian Academy of Sciences, Bulgaria; E-AIMS); Emil Stanev (University of Oldenburg, Germany; E-AIMS); Claire Lo Monaco (Laboratoire d\\u2019Oc\\ufffdeanographie et du Climat: Exp\\ufffderimentations et\\nApproches Num\\ufffderiques); Pierre-Marie Poulain (National Institute of Oceanography and Experimental Geophysics, Italy; Argo-Italy); Sabrina Speich (Laboratoire de M\\ufffdet\\ufffdeorologie Dynamique, France; LEFE-GMMC); Virginie Thierry (Ifremer, France; LEFE-GMMC); Pascal Conan (Observatoire Oc\\ufffdeanologique de Banyuls sur mer, France; LEFE-GMMC); Laurent Coppola (Laboratoire d\\u2019Oc\\ufffdeanographie de Villefranche, France; LEFE-GMMC); Anne Petrenko (Mediterranean Institute of Oceanography, France; LEFE-GMMC); and Jean-Baptiste Sall\\ufffdee (Laboratoire d\\u2019Oc\\ufffdeanographie et du Climat, France; LEFE-GMMC).\\nCollin Roesler (Bowdoin College, USA) and Yannick Huot (University of Sherbrooke, Canada) are acknowledged for useful comments and fruitful discussion.\\nWe also thank the International Argo Program and the CORIOLIS project that contribute to make the data freely and publicly available.\\nData referring to Organelli et al. (2016a; https://doi.org/10.17882/47142) and Barbieux et al. (2017; https://doi.org/10.17882/49388) are freely available on SEANOE.\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.1002/2017JC013030'], citation_context: \\\"Citation: Barbieux, M., Uitz, J., Bricaud, A., Organelli, E., Poteau, A., Schmechtig, C., . . . Claustre, H. (2018). Assessing the variability in the relationship between the particulate backscattering coefficient and the chlorophyll a concentration from a global Biogeochemical-Argo database. Journal of Geophysical Research: Oceans, 123, 1229-1250. https://doi. org/10.1002/2017JC013030\\\"}\",\"{\\\"dataset_ids\\\": ['10.17882/49388'], citation_context: \\\"nelli, E., Claustre, H., Schmechtig, C., Poteau, A., Boss, E., . . . Xing, X. (2017). A global database of vertical profiles derived from Biogeochemical Argo float measurements for biogeochemical and bio-optical applications. SEANOE. https://doi.org/10.17882/49388 Barton, A. D., Lozier, M. S., & Williams, R. G. (2015). Physical controls of variability in North Atlantic phytoplankton communities. Limnology and Oceanography, 60(1), 181-197. https://doi.org/10.1002/lno.10011 Beckmann, A., & Hense, I. (2007). Ben\\\"}\",\"{\\\"dataset_ids\\\": ['10.1016/j.rse.'], citation_context: \\\", Alvain, S., Vantrepotte, V., & Huertas, I. E. (2014). Identification of dominant phytoplankton functional types in the Mediterra-nean Sea based on a regionalized remote sensing approach. Remote Sensing of Environment, 152, 557-575. https://doi.org/10.1016/j.rse. 2014.06.029 Organelli, E., Barbieux, M., Claustre, H., Schmechtig, C., Poteau, A., Bricaud, A., . . . Dall\\u2019Olmo, G. (2016a). A global bio-optical database derived from Biogeochemical Argo float measurements within the layer of interest for field and\\\"},{\\\"dataset_ids\\\": ['10.17882/47142'], citation_context: \\\"mechtig, C., Poteau, A., Bricaud, A., . . . Dall\\u2019Olmo, G. (2016a). A global bio-optical database derived from Biogeochemical Argo float measurements within the layer of interest for field and remote ocean colour applications. SEANOE. https://doi.org/10.17882/47142 Organelli, E., Barbieux, M., Claustre, H., Schmechtig, C., Poteau, A., Bricaud, A., . . . Xing, X. (2017b). Two databases derived from BGC-Argo float measurements for marine biogeochemical and bio-optical applications. Earth System Science Data, 9,\\\"}\"]}",
         "{\"title\":\"Assessing the variability in the relationship between the particulate backscattering coefficient and the chlorophyll a concentration from a global Biogeochemical-Argo database\",\"author\":\"Marie Barbieux\",\"abstract\":\"The particulate backscattering coefficient vs chlorophyll a concentration relationship varies along the water column, according to seasons and oceanic regions. -The b bp -to-Chla ratio is a valuable biogeochemical proxy for assessing the nature of the particulate assemblage and revealing photoacclimation processes. -The BGC-Argo float network yields an unprecedented amount of quality data for studying biogeochemical processes at a global scale and along the vertical dimension.\",\"data_availability\":\"\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.17882/49388'], citation_context: \\\"A global database of vertical profiles derived from Biogeochemical Argo float measurements for biogeochemical and bio-optical applications|SEANOE|10.17882/49388\\\"}\",\"{\\\"dataset_ids\\\": ['10.17882/47142'], citation_context: \\\"A global bio-optical database derived from Biogeochemical Argo float measurements within the layer of interest for field and remote ocean colour applications|SEANOE|10.17882/47142\\\"}\",\"{\\\"dataset_ids\\\": ['10.5194/essd-2017-58'], citation_context: \\\"Two databases derived from BGC-Argo float measurements for marine biogeochemical and biooptical applications|Earth Syst. Sci. Data|10.5194/essd-2017-58\\\"}\"]}"
        ],
        [
         "1",
         "10.1002_anie.201916483",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.201916483.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.201916483.xml",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "jats",
         "{\"title\":\"\",\"author\":\"Daniel Werner\",\"abstract\":\"Abstract: The homoleptic pyrazolate complexes [CeIII 4-(Me2pz)12] and [CeIV(Me2pz)4]2 quantitatively insert CO2 to give [CeIII 4(Me2pz\\u00b7CO2)12] and [CeIV(Me2pz\\u00b7CO2)4], respec-tively (Me2pz = 3,5-dimethylpyrazolato).\\nThis process is rever-sible for both complexes, as observed by in situ IR and NMR spectroscopy in solution and by TGA in the solid state.\\nBy adjusting the molar ratio, one molecule of CO2 per [CeIV-(Me2pz)4] complex could be inserted to give trimetallic [Ce3-(Me2pz)9(Me2pz\\u00b7CO2)3(thf)].\\nBoth the cerous and ceric in-sertion products catalyze the formation of cyclic carbonates from epoxides and CO2 under mild conditions.\\nIn the absence of epoxide, the ceric catalyst is prone to reduction by the co-catalyst tetra-n-butylammonium bromide (TBAB).\",\"data_availability\":\"\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.1002/anie.201916483'], citation_context: \\\"15213773, 2020, 14, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/anie.201916483 by California Digital Library University Of California, Wiley Online Library on [09/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\\\"}\"]}",
         "{\"title\":\"Effective and Reversible Carbon Dioxide Insertion into Cerium Pyrazolates\",\"author\":\"Bayer Uwe\",\"abstract\":\"Abstract The homoleptic pyrazolate complexes [Ce III \\n 4 (Me 2 pz) 12 ] and [Ce IV (Me 2 pz) 4 ] 2  quantitatively insert CO 2  to give [Ce III \\n 4 (Me 2 pz\\u22c5CO 2 ) 12 ] and [Ce IV (Me 2 pz\\u22c5CO 2 ) 4 ], respectively (Me 2 pz=3,5\\u2010dimethylpyrazolato). This process is reversible for both complexes, as observed by in\\u2005situ IR and NMR spectroscopy in solution and by TGA in the solid state. By adjusting the molar ratio, one molecule of CO 2  per [Ce IV (Me 2 pz) 4 ] complex could be inserted to give trimetallic [Ce 3 (Me 2 pz) 9 (Me 2 pz\\u22c5CO 2 ) 3 (thf)]. Both the cerous and ceric insertion products catalyze the formation of cyclic carbonates from epoxides and CO 2  under mild conditions. In the absence of epoxide, the ceric catalyst is prone to reduction by the co\\u2010catalyst tetra\\u2010 n \\u2010butylammonium bromide (TBAB).\",\"data_availability\":\"\",\"other_dataset_citations\":[]}"
        ],
        [
         "2",
         "10.1002_anie.202005531",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202005531.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.202005531.xml",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "bioc",
         "{\"title\":\"\",\"author\":\"Dong Wang\",\"abstract\":\"Abstract: Trifluoromethyl sulfoxides are a new class of trifluoromethylthiolating reagent.\\nThe sulfoxides engage in metal-free C\\ufffdH trifluoromethylthiolation with a range of (hetero)arenes.\\nThe method is also applicable to the function-alization of important compound classes, such as ligand derivatives and polyaromatics, and in the late-stage trifluor-omethylthiolation of medicines and agrochemicals.\\nThe iso-lation and characterization of a sulfonium salt intermediate supports an interrupted Pummerer reaction mechanism.\\nIncorporating fluorine into organic compounds is a useful tool in drug design and development.\\nThe fluoro group is well known to improve the pharmacokinetic properties of a mol-ecule and fluorine-18 is an important radioisotope in molec-ular imaging.[1,2] Trifluoromethylthio (SCF3) groups are commonly found in drug molecules and veterinary medi-cines.[3,4]\\nBy combining a fluorinated moiety with a hetero-atom, many have turned to the SCF3 group to impart useful properties, such as high lipophilicity, to a compound of interest.[5]\",\"data_availability\":\"\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.1002/anie.202005531'], citation_context: \\\"15213773, 2020, 37, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/anie.202005531 by California Digital Library University Of California, Wiley Online Library on [09/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\\\"}\"]}",
         "{\"title\":\"Trifluoromethyl Sulfoxides: Reagents for Metal\\u00e2\\u0080\\u0090Free C\\u00e2\\u0088\\u0092H Trifluoromethylthiolation\",\"author\":\"Wang Dong;prefix:Dr.\",\"abstract\":\"Abstract\",\"data_availability\":\"\",\"other_dataset_citations\":[]}"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>dataset_info</th>\n",
       "      <th>xml_type</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>xml_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.17882/4938...</td>\n",
       "      <td>tei</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Marie Barbieux\",\"abstrac...</td>\n",
       "      <td>{\"title\":\"Assessing the variability in the rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>jats</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Daniel Werner\",\"abstract...</td>\n",
       "      <td>{\"title\":\"Effective and Reversible Carbon Diox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>bioc</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Dong Wang\",\"abstract\":\"A...</td>\n",
       "      <td>{\"title\":\"Trifluoromethyl Sulfoxides: Reagents...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                                      pdf_file_path  \\\n",
       "0    10.1002_2017jc013030  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "1  10.1002_anie.201916483  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "2  10.1002_anie.202005531  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                       xml_file_path dataset_type  \\\n",
       "0  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "1  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "2  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "\n",
       "                                        dataset_info xml_type  \\\n",
       "0  [{'dataset_id': 'https://doi.org/10.17882/4938...      tei   \n",
       "1     [{'dataset_id': 'Missing', 'type': 'Missing'}]     jats   \n",
       "2     [{'dataset_id': 'Missing', 'type': 'Missing'}]     bioc   \n",
       "\n",
       "                                            pdf_text  \\\n",
       "0  {\"title\":\"\",\"author\":\"Marie Barbieux\",\"abstrac...   \n",
       "1  {\"title\":\"\",\"author\":\"Daniel Werner\",\"abstract...   \n",
       "2  {\"title\":\"\",\"author\":\"Dong Wang\",\"abstract\":\"A...   \n",
       "\n",
       "                                            xml_text  \n",
       "0  {\"title\":\"Assessing the variability in the rel...  \n",
       "1  {\"title\":\"Effective and Reversible Carbon Diox...  \n",
       "2  {\"title\":\"Trifluoromethyl Sulfoxides: Reagents...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the PDF text content for each article\n",
    "file_paths_df['pdf_text'] = file_paths_df['pdf_file_path'].apply(lambda x: extract_article_text(x) if x else \"\")\n",
    "# Load the XML text content for each article\n",
    "file_paths_df['xml_text'] = file_paths_df['xml_file_path'].apply(lambda x: extract_article_text(x) if x else \"\")\n",
    "# Fill NaN values in the 'xml_type' and 'xml_text' columns with empty strings\n",
    "file_paths_df['xml_file_path'] = file_paths_df['xml_file_path'].fillna('')\n",
    "file_paths_df['xml_type'] = file_paths_df['xml_type'].fillna('')\n",
    "file_paths_df['xml_text'] = file_paths_df['xml_text'].fillna('')\n",
    "# Display the first few rows of the training file paths DataFrame\n",
    "print(f\"File paths DataFrame shape: {file_paths_df.shape}\")\n",
    "display(file_paths_df.head(3))\n",
    "# Save the file paths DataFrame to CSV files\n",
    "file_paths_df.to_csv(os.path.join(BASE_OUTPUT_DIR, 'file_paths.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5033e1",
   "metadata": {},
   "source": [
    "## Define the Qwen evaluation model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2640c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QwenModelEval Class ---\n",
    "# kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "#max_new_tokens=32768\n",
    "class QwenModelEval:\n",
    "    def __init__(self, model_name, sys_prompt, enable_thinking=True, max_new_tokens=1024, max_input_length=8200):\n",
    "        print(f\"Loading Qwen model and tokenizer from: {model_name}\")\n",
    "        self.model_name = model_name\n",
    "        self.sys_prompt = sys_prompt\n",
    "        self.enable_thinking = enable_thinking  # Enable or disable thinking mode\n",
    "        self.max_new_tokens = max_new_tokens  # Set the maximum number of new tokens to generate\n",
    "        self.max_input_length = max_input_length  # Set the maximum input length for the model\n",
    "        # Load the tokenizer and model\n",
    "        # Using trust_remote_code=True to allow custom model code execution\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\", trust_remote_code=True)\n",
    "        self.model.eval() # Set the model to evaluation mode here.\n",
    "\n",
    "    def generate_response(self, user_input):  \n",
    "        inputs = self._get_inputs(user_input)\n",
    "        # Disable gradient calculation during inference\n",
    "        # Generate the response using the model\n",
    "        with torch.no_grad(): \n",
    "            generated_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.convert_tokens_to_ids(\"<|im_end|>\"),\n",
    "                # do_sample=False, # Use greedy decoding (fastest)\n",
    "                # num_beams=1,     # Do not use beam search (fastest)\n",
    "                # temperature=0.0, # Make output deterministic (if do_sample=False, this has no effect)                \n",
    "                temperature=0.6 if self.enable_thinking else 0.7,\n",
    "                top_p=0.95 if self.enable_thinking else 0.8,\n",
    "                top_k=20,\n",
    "                min_p=0\n",
    "            )\n",
    "        # Parse the response and thinking content\n",
    "        return self._parse_response(inputs, generated_ids)\n",
    "\n",
    "    def _get_inputs(self, user_input):\n",
    "        \"\"\"Prepare the input for the model based on user input.\"\"\"\n",
    "        # Trim the user input to a maximum length for better performance\n",
    "        user_input = user_input[:self.max_input_length]  # Limit input length to 4096 characters\n",
    "        print(f\"Preparing input with length: {len(user_input)}\")\n",
    "        # Create the messages for the chat template\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=self.enable_thinking\n",
    "        )\n",
    "        return self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "    \n",
    "    def _parse_response(self, inputs, generated_ids):\n",
    "        # Extract the output IDs from the generated IDs\n",
    "        output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist()\n",
    "        try:\n",
    "            index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        thinking_content = self.tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "        raw_response = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "        response = self._parse_json(raw_response)\n",
    "        print(f\"Parsed response: {response}\")\n",
    "        return response, thinking_content\n",
    "    \n",
    "    def _parse_json(self, raw_response: str) -> list[dict[str,str]]:\n",
    "        # Remove code block markers and leading/trailing whitespace\n",
    "        cleaned = raw_response.strip()\n",
    "        if cleaned.startswith(\"```json\"):\n",
    "            cleaned = cleaned[len(\"```json\"):].strip()\n",
    "        if cleaned.endswith(\"```\"):\n",
    "            cleaned = cleaned[:-3].strip()\n",
    "\n",
    "        # Now parse as JSON\n",
    "        try:\n",
    "            return json.loads(cleaned)\n",
    "        except json.JSONDecodeError as e:\n",
    "            return []        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2dee95",
   "metadata": {},
   "source": [
    "## Define the System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "74e5a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the one-shot reasoning and task prompt\n",
    "# This prompt is designed to guide the model through a structured reasoning process\n",
    "SYS_PROMPT = \"\"\"\n",
    "You are an advanced AI research assistant specialized in identifying and classifying datasets used within academic research papers.\n",
    "Your primary goal is to accurately extract and categorize dataset identifiers (dataset_ids) from provided paper sections.\n",
    "\n",
    "---\n",
    "\n",
    "### Input Data Structure\n",
    "\n",
    "You will receive a JSON string representing key sections of an academic paper, structured as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"title\": \"Title of the paper\",\n",
    "    \"author\": \"The primary author of the paper, e.g., 'Author A'\",\n",
    "    \"abstract\": \"Abstract of the paper\",\n",
    "    \"data_availability\": \"Data availability information\",\n",
    "    \"other_dataset_citations\": [\n",
    "        {\"dataset_ids\": [\"10.12345/12345\"], \"citation_context\": \"Dataset citation context 1\"},\n",
    "        {\"dataset_ids\": [\"10.1234/xxxx.1x1x-xx11\", \"EPI_ISL_12345678\"], \"citation_context\": \"Dataset citation context 2\"},\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Guidance on Input Sections:**\n",
    "*   **`title`**: Provides general context for the paper's topic.\n",
    "*   **`author`**: The primary author of the paper. This is crucial for determining if the dataset's *raw data* was *originally generated by this author*.\n",
    "*   **`abstract`**: **CRITICAL** for understanding the research scope and, most importantly, for determining if a dataset's *raw data* was *originally generated by the authors of *this* paper*.\n",
    "*   **`data_availability`**: This section provides information on datasets. Its content must be evaluated to determine if the data was *originally generated by the authors of this paper* (Primary) or *acquired from an existing source* (Secondary).\n",
    "*   **`other_dataset_citations`**: A list of potential dataset citations. The `citation_context` is vital to confirm if a `dataset_id` truly refers to a dataset and to aid in classifying its origin (Primary or Secondary).\n",
    "\n",
    "---\n",
    "\n",
    "### Core Objective & Critical Exclusion\n",
    "\n",
    "Your overarching objective is to identify and classify **only valid, data-related `dataset_id`s**.\n",
    "\n",
    "**CRITICAL EXCLUSION**: You **MUST NOT** extract any `dataset_id`s that refer to other academic papers, articles, or the paper itself. Focus strictly on identifiers for *datasets*.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Definitions\n",
    "\n",
    "*   **`dataset_id`**: A unique, persistent identifier for a dataset. There are two main types:\n",
    "\n",
    "    1.  **Digital Object Identifier (DOI)**:\n",
    "        *   **Format**: `[prefix]/[suffix]`. The prefix always starts with \"10.\" and is followed by a 4 or 5 digit number. The suffix can contain letters, numbers, and special characters.\n",
    "        *   May or may not start with \"https://doi.org/\" or \"doi:\".\n",
    "        *   **IMPORTANT DOI VALIDATION RULE**:\n",
    "            *   Only identify DOIs that are explicitly used as `dataset_id`s.\n",
    "            *   **DO NOT extract DOIs for academic papers/articles.**\n",
    "            *   **If a DOI is presented as a reference to a publication (e.g., \"as described in [DOI]\", \"cited in [DOI]\", \"see [DOI] for details on the method\"), it is NOT a dataset_id.**\n",
    "            *   A DOI is a `dataset_id` ONLY if the surrounding `citation_context` or `data_availability` section clearly indicates it refers to a dataset, data repository, data archive, or similar data-specific entity.\n",
    "\n",
    "    2.  **Accession ID**:\n",
    "        *   Typically alphanumeric strings that uniquely identify a dataset within a specific data repository.\n",
    "        *   Can be found in both `data_availability` and `other_dataset_citations` sections.\n",
    "        *   *Examples*: `\"EPI_ISL_12345678\"` (EPI dataset), `\"IPR000264\"` (InterPro dataset), `\"SAMN07159041\"` (NCBI Sequence Read Archive dataset), `\"CHEMBL1782574\"` (ChEMBL dataset)\n",
    "\n",
    "*   **Dataset Type Classification**: **This is the MOST CRITICAL distinction. Focus ONLY on the *ORIGIN* of the *RAW DATA*.**\n",
    "\n",
    "    *   **Primary**: Raw or processed data that was **ORIGINALLY GENERATED, COLLECTED, or CREATED by the AUTHORS OF *THIS SPECIFIC PAPER*** as part of the novel research presented in this paper. This data is central to the new findings and was *not* acquired from an existing public source.\n",
    "        *   *Keywords often associated with Primary*: \"generated in this study\", \"our data\", \"newly sequenced\", \"collected for this project\", \"created by us\", \"original data produced here\", \"developed by us\".\n",
    "        *   *Example*: If the paper describes *new* patient scans or *new* experimental results that *these authors* produced, and then provides a DOI for *those new scans/results*, that's Primary.\n",
    "\n",
    "    *   **Secondary**: Raw or processed data that was **ACQUIRED, DERIVED, REUSED, or RE-ANALYZED from EXISTING, PUBLICLY AVAILABLE RECORDS or PREVIOUSLY PUBLISHED DATASETS** that were *not originally generated by the authors of this specific paper*. Even if the authors perform significant new processing, annotation, or analysis *on* this existing data, the *original raw data source* is considered Secondary if it was not their creation.\n",
    "        *   *Keywords often associated with Secondary*: \"publicly available data\", \"previously published\", \"existing\", \"external\", \"re-analyzed data\", \"obtained from\", \"data from\", \"acquired from\", \"derived from\", \"based on data from\", \"sourced from\".\n",
    "        *   **CRITICAL EXAMPLE FOR SECONDARY**: In the provided abstract, if it states \"CT scans **acquired from The Cancer Imaging Archive 'NSCLC Radiomics' data collection**\", then the `dataset_id` for \"NSCLC Radiomics\" (e.g., `10.7937/K9/TCIA.2015.PF0M9REI`) is **Secondary**. This is because the raw CT scans were *not* originally generated by the authors of *this* paper; they *used* this existing, external data.\n",
    "\n",
    "---\n",
    "\n",
    "### Classification Logic Flow (for each identified `dataset_id`):\n",
    "\n",
    "To classify a `dataset_id` as Primary or Secondary, follow these steps strictly:\n",
    "\n",
    "1.  **Examine the `abstract` and `citation_context` (if available) for the `dataset_id` in question.**\n",
    "2.  **Look for explicit phrases indicating the *origin* of the *raw data*:**\n",
    "    *   Does the text state the data was \"acquired from\", \"obtained from\", \"derived from\", \"from [a named external archive/database]\", \"publicly available\", or \"previously published\"?\n",
    "        *   **IF YES**: Classify as **Secondary**. (e.g., `10.7937/K9/TCIA.2015.PF0M9REI` is Secondary because the abstract says \"CT scans acquired from The Cancer Imaging Archive 'NSCLC Radiomics' data collection\").\n",
    "    *   Does the text state the data was \"generated by us\", \"created by the authors\", \"our data\", \"newly sequenced\", or describe a process of *original data collection/creation* by the authors of *this paper*?\n",
    "        *   **IF YES**: Classify as **Primary**. (e.g., `10.7937/tcia.2020.6c7y-gq39` is Primary because the abstract says \"This manuscript describes a dataset of thoracic cavity segmentations and discrete pleural effusion segmentations **we have annotated**\" and describes the generation process by the authors).\n",
    "3.  **If, after applying the above rules, the origin of the *raw data* remains truly ambiguous, then default to \"Primary\".**\n",
    "\n",
    "---\n",
    "\n",
    "### Tasks: Step-by-Step Instructions\n",
    "\n",
    "Follow these three tasks in order:\n",
    "\n",
    "**Task 1: Identify Valid Dataset IDs**\n",
    "\n",
    "1.  **Search Priority**: Begin by searching the `data_availability` section. Then, search the `other_dataset_citations` section.\n",
    "2.  **Validation**: For each potential `dataset_id` (DOI or Accession ID), confirm it is truly data-related.\n",
    "    *   **For DOIs**: Strictly apply the **IMPORTANT DOI VALIDATION RULE** defined above. If it refers to a publication, **DO NOT** extract it.\n",
    "    *   **For all IDs**: Look for surrounding terms like \"data release\", \"data availability\", \"dataset\", \"database\", \"repository\", \"data source\", \"data access\", or \"data archive\" within the `data_availability` section or the `citation_context`.\n",
    "3.  **Deduplication**: If the same `dataset_id` is found multiple times, **only process the first instance encountered**.\n",
    "4.  **Conditional Proceeding**:\n",
    "    *   If **no valid `dataset_id`s are found** after searching both sections, **skip directly to Task 3** and output the \"Missing\" JSON structure.\n",
    "    *   If one or more valid `dataset_id`s are found, proceed to Task 2.\n",
    "\n",
    "**Task 2: Classify Dataset Types**\n",
    "\n",
    "1.  For each valid `dataset_id` identified in Task 1, classify its type as either \"Primary\" or \"Secondary\".\n",
    "2.  **STRICTLY APPLY THE \"CLASSIFICATION LOGIC FLOW\" ABOVE for each `dataset_id`.** Use the `author` section, the `abstract` section, and the `citation_context` to determine if the *raw data* associated with the `dataset_id` was *originally generated by the authors of *this* paper* (Primary) or *acquired/reused from an existing source* (Secondary).\n",
    "3.  Apply the \"Key Definitions\" for Primary and Secondary types, paying close attention to the associated keywords and the provided examples.\n",
    "4.  Remember the \"Fallback Rule\": Only default to \"Primary\" if, after careful consideration, the classification remains truly ambiguous regarding the *original generation* of the raw data.\n",
    "\n",
    "**Task 3: Format and Return Results**\n",
    "\n",
    "Return your final results as a JSON array of objects.\n",
    "\n",
    "1.  **Scenario A: No Valid Datasets Found**\n",
    "    If Task 1 resulted in no valid `dataset_id`s, return a single JSON object with the following structure:\n",
    "    ```json\n",
    "    [\n",
    "        {\n",
    "            \"dataset_id\": \"Missing\",\n",
    "            \"type\": \"Missing\"\n",
    "        }\n",
    "    ]\n",
    "    ```\n",
    "2.  **Scenario B: One or More Valid Datasets Found**\n",
    "    If Task 1 identified one or more valid `dataset_id`s, return every valid dataset found in a JSON array of objects, where each object has the following structure:\n",
    "    ```json\n",
    "    [\n",
    "        {\n",
    "            \"dataset_id\": \"example_id_1\",\n",
    "            \"type\": \"Primary\"\n",
    "        },\n",
    "        {\n",
    "            \"dataset_id\": \"example_id_2\",\n",
    "            \"type\": \"Secondary\"\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    ```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe217a6",
   "metadata": {},
   "source": [
    "## Evaluate the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f8a8a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen model and tokenizer from: C:\\Users\\jim\\.cache\\kagglehub\\models\\qwen-lm\\qwen-3\\transformers\\0.6b\\1\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the QwenModelEval class with the model path and system prompt\n",
    "inference_model = QwenModelEval(QWEN_BASE_MODEL_PATH, sys_prompt=SYS_PROMPT, enable_thinking=True, max_new_tokens=1576)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "749ca3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_articles(file_paths_df: pd.DataFrame, model) -> pd.DataFrame:\n",
    "    results = []\n",
    "    for i, row in file_paths_df.iterrows():\n",
    "        article_id = row['article_id']\n",
    "        dataset_info = row['dataset_info']\n",
    "        pdf_text = row['pdf_text']\n",
    "        xml_text = row['xml_text']\n",
    "        text_type = \"XML\" if xml_text else \"PDF\"\n",
    "        text_content = xml_text if xml_text else pdf_text\n",
    "        response = \"\"\n",
    "        thinking_content = \"\"\n",
    "\n",
    "        # Prepare the user input for the model\n",
    "        user_input = f\"Text Content: {text_content}\\n\"\n",
    "        \n",
    "        if text_content:\n",
    "            print(f\"Processing article {i}/{len(file_paths_df)}: {article_id}, type: {text_type}\")\n",
    "            # Generate response from the model\n",
    "            response, thinking_content = model.generate_response(user_input)\n",
    "\n",
    "        results.append({\n",
    "            'article_id': article_id,\n",
    "            'dataset_info': dataset_info,\n",
    "            'text_type': text_type,\n",
    "            'llm_input': user_input,\n",
    "            'llm_response': response,\n",
    "            'llm_thinking_content': thinking_content\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(by=[\"article_id\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "dc937f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File paths DataFrame shape: (30, 8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_info",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "19c3af25-569c-4cde-9568-3dde1abebaa3",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_2017jc013030.xml",
         "test",
         "[{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]",
         "tei",
         "{\"title\":\"\",\"author\":\"Marie Barbieux\",\"abstract\":\"Abstract Characterizing phytoplankton distribution and dynamics in the world\\u2019s open oceans requires in situ observations over a broad range of space and time scales.\\nIn addition to temperature/salinity measure-ments, Biogeochemical-Argo (BGC-Argo) profiling floats are capable of autonomously observing at high-frequency bio-optical properties such as the chlorophyll fluorescence, a proxy of the chlorophyll a concen-tration (Chla), the particulate backscattering coefficient (bbp), a proxy of the stock of particulate organic car-bon, and the light available for photosynthesis.\\nWe analyzed an unprecedented BGC-Argo database of more than 8,500 multivariable profiles collected in various oceanic conditions, from subpolar waters to subtropical gyres.\\nOur objective is to refine previously established Chla versus bbp relationships and gain insights into the sources of vertical, seasonal, and regional variability in this relationship.\\nDespite some regional, seasonal and vertical variations, a general covariation occurs at a global scale.\\nWe distinguish two main contrasted situations: (1) concomitant changes in Chla and bbp that correspond to actual variations in phytoplankton biomass, e.g., in subpolar regimes; (2) a decoupling between the two variables attributed to photoacclima-tion or changes in the relative abundance of nonalgal particles, e.g., in subtropical regimes.\\nThe variability in the bbp:Chla ratio in the surface layer appears to be essentially influenced by the type of particles and by photoacclimation processes.\\nThe large BGC-Argo database helps identifying the spatial and temporal scales at which this ratio is predominantly driven by one or the other of these two factors.\",\"data_availability\":\"Acknowledgments This paper represents a contribution to the following research projects: remOcean (funded by the European Research Council, grant 246777), NAOS (funded by the Agence Nationale de la Recherche in the frame of the French \\u2018\\u2018Equipement d\\u2019avenir\\u2019\\u2019 program, grant ANR J11R107-F), the SOCLIM (Southern Ocean and climate) project supported by the French research program LEFE-CYBER of INSU-CNRS, the Climate Initiative of the foundation BNP Paribas and the French polar institute (IPEV), AtlantOS (funded by the European Union\\u2019s Horizon 2020 Research and Innovation program, grant 2014- 633211), E-AIMS (funded by the European Commission\\u2019s FP7 project, grant 312642), U.K. Bio-Argo (funded by the British Natural Environment Research Council-NERC, grant NE/L012855/1), REOPTIMIZE (funded by the European Union\\u2019s Horizon 2020 Research and Innovation program, Marie Sk\\u0142odowska-Curie grant 706781), Argo-Italy (funded by the Italian Ministry of Education, University and Research -MIUR), and the French Bio-Argo program (BGC-Argo France; funded by CNES-TOSCA, LEFE Cyber, and GMMC).\\nWe thank the PIs of several BGC-Argo floats missions and projects: Giorgio Dall\\u2019Olmo (Plymouth Marine Laboratory, United Kingdom; E-AIMS and U.K. Bio-Argo); Kjell-Arne Mork (Institute of Marine Research, Norway; E-AIMS); Violeta Slabakova (Bulgarian Academy of Sciences, Bulgaria; E-AIMS); Emil Stanev (University of Oldenburg, Germany; E-AIMS); Claire Lo Monaco (Laboratoire d\\u2019Oc\\ufffdeanographie et du Climat: Exp\\ufffderimentations et\\nApproches Num\\ufffderiques); Pierre-Marie Poulain (National Institute of Oceanography and Experimental Geophysics, Italy; Argo-Italy); Sabrina Speich (Laboratoire de M\\ufffdet\\ufffdeorologie Dynamique, France; LEFE-GMMC); Virginie Thierry (Ifremer, France; LEFE-GMMC); Pascal Conan (Observatoire Oc\\ufffdeanologique de Banyuls sur mer, France; LEFE-GMMC); Laurent Coppola (Laboratoire d\\u2019Oc\\ufffdeanographie de Villefranche, France; LEFE-GMMC); Anne Petrenko (Mediterranean Institute of Oceanography, France; LEFE-GMMC); and Jean-Baptiste Sall\\ufffdee (Laboratoire d\\u2019Oc\\ufffdeanographie et du Climat, France; LEFE-GMMC).\\nCollin Roesler (Bowdoin College, USA) and Yannick Huot (University of Sherbrooke, Canada) are acknowledged for useful comments and fruitful discussion.\\nWe also thank the International Argo Program and the CORIOLIS project that contribute to make the data freely and publicly available.\\nData referring to Organelli et al. (2016a; https://doi.org/10.17882/47142) and Barbieux et al. (2017; https://doi.org/10.17882/49388) are freely available on SEANOE.\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.1002/2017JC013030'], citation_context: \\\"Citation: Barbieux, M., Uitz, J., Bricaud, A., Organelli, E., Poteau, A., Schmechtig, C., . . . Claustre, H. (2018). Assessing the variability in the relationship between the particulate backscattering coefficient and the chlorophyll a concentration from a global Biogeochemical-Argo database. Journal of Geophysical Research: Oceans, 123, 1229-1250. https://doi. org/10.1002/2017JC013030\\\"}\",\"{\\\"dataset_ids\\\": ['10.17882/49388'], citation_context: \\\"nelli, E., Claustre, H., Schmechtig, C., Poteau, A., Boss, E., . . . Xing, X. (2017). A global database of vertical profiles derived from Biogeochemical Argo float measurements for biogeochemical and bio-optical applications. SEANOE. https://doi.org/10.17882/49388 Barton, A. D., Lozier, M. S., & Williams, R. G. (2015). Physical controls of variability in North Atlantic phytoplankton communities. Limnology and Oceanography, 60(1), 181-197. https://doi.org/10.1002/lno.10011 Beckmann, A., & Hense, I. (2007). Ben\\\"}\",\"{\\\"dataset_ids\\\": ['10.1016/j.rse.'], citation_context: \\\", Alvain, S., Vantrepotte, V., & Huertas, I. E. (2014). Identification of dominant phytoplankton functional types in the Mediterra-nean Sea based on a regionalized remote sensing approach. Remote Sensing of Environment, 152, 557-575. https://doi.org/10.1016/j.rse. 2014.06.029 Organelli, E., Barbieux, M., Claustre, H., Schmechtig, C., Poteau, A., Bricaud, A., . . . Dall\\u2019Olmo, G. (2016a). A global bio-optical database derived from Biogeochemical Argo float measurements within the layer of interest for field and\\\"},{\\\"dataset_ids\\\": ['10.17882/47142'], citation_context: \\\"mechtig, C., Poteau, A., Bricaud, A., . . . Dall\\u2019Olmo, G. (2016a). A global bio-optical database derived from Biogeochemical Argo float measurements within the layer of interest for field and remote ocean colour applications. SEANOE. https://doi.org/10.17882/47142 Organelli, E., Barbieux, M., Claustre, H., Schmechtig, C., Poteau, A., Bricaud, A., . . . Xing, X. (2017b). Two databases derived from BGC-Argo float measurements for marine biogeochemical and bio-optical applications. Earth System Science Data, 9,\\\"}\"]}",
         "{\"title\":\"Assessing the variability in the relationship between the particulate backscattering coefficient and the chlorophyll a concentration from a global Biogeochemical-Argo database\",\"author\":\"Marie Barbieux\",\"abstract\":\"The particulate backscattering coefficient vs chlorophyll a concentration relationship varies along the water column, according to seasons and oceanic regions. -The b bp -to-Chla ratio is a valuable biogeochemical proxy for assessing the nature of the particulate assemblage and revealing photoacclimation processes. -The BGC-Argo float network yields an unprecedented amount of quality data for studying biogeochemical processes at a global scale and along the vertical dimension.\",\"data_availability\":\"\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.17882/49388'], citation_context: \\\"A global database of vertical profiles derived from Biogeochemical Argo float measurements for biogeochemical and bio-optical applications|SEANOE|10.17882/49388\\\"}\",\"{\\\"dataset_ids\\\": ['10.17882/47142'], citation_context: \\\"A global bio-optical database derived from Biogeochemical Argo float measurements within the layer of interest for field and remote ocean colour applications|SEANOE|10.17882/47142\\\"}\",\"{\\\"dataset_ids\\\": ['10.5194/essd-2017-58'], citation_context: \\\"Two databases derived from BGC-Argo float measurements for marine biogeochemical and biooptical applications|Earth Syst. Sci. Data|10.5194/essd-2017-58\\\"}\"]}"
        ],
        [
         "1",
         "10.1002_anie.201916483",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.201916483.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.201916483.xml",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "jats",
         "{\"title\":\"\",\"author\":\"Daniel Werner\",\"abstract\":\"Abstract: The homoleptic pyrazolate complexes [CeIII 4-(Me2pz)12] and [CeIV(Me2pz)4]2 quantitatively insert CO2 to give [CeIII 4(Me2pz\\u00b7CO2)12] and [CeIV(Me2pz\\u00b7CO2)4], respec-tively (Me2pz = 3,5-dimethylpyrazolato).\\nThis process is rever-sible for both complexes, as observed by in situ IR and NMR spectroscopy in solution and by TGA in the solid state.\\nBy adjusting the molar ratio, one molecule of CO2 per [CeIV-(Me2pz)4] complex could be inserted to give trimetallic [Ce3-(Me2pz)9(Me2pz\\u00b7CO2)3(thf)].\\nBoth the cerous and ceric in-sertion products catalyze the formation of cyclic carbonates from epoxides and CO2 under mild conditions.\\nIn the absence of epoxide, the ceric catalyst is prone to reduction by the co-catalyst tetra-n-butylammonium bromide (TBAB).\",\"data_availability\":\"\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.1002/anie.201916483'], citation_context: \\\"15213773, 2020, 14, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/anie.201916483 by California Digital Library University Of California, Wiley Online Library on [09/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\\\"}\"]}",
         "{\"title\":\"Effective and Reversible Carbon Dioxide Insertion into Cerium Pyrazolates\",\"author\":\"Bayer Uwe\",\"abstract\":\"Abstract The homoleptic pyrazolate complexes [Ce III \\n 4 (Me 2 pz) 12 ] and [Ce IV (Me 2 pz) 4 ] 2  quantitatively insert CO 2  to give [Ce III \\n 4 (Me 2 pz\\u22c5CO 2 ) 12 ] and [Ce IV (Me 2 pz\\u22c5CO 2 ) 4 ], respectively (Me 2 pz=3,5\\u2010dimethylpyrazolato). This process is reversible for both complexes, as observed by in\\u2005situ IR and NMR spectroscopy in solution and by TGA in the solid state. By adjusting the molar ratio, one molecule of CO 2  per [Ce IV (Me 2 pz) 4 ] complex could be inserted to give trimetallic [Ce 3 (Me 2 pz) 9 (Me 2 pz\\u22c5CO 2 ) 3 (thf)]. Both the cerous and ceric insertion products catalyze the formation of cyclic carbonates from epoxides and CO 2  under mild conditions. In the absence of epoxide, the ceric catalyst is prone to reduction by the co\\u2010catalyst tetra\\u2010 n \\u2010butylammonium bromide (TBAB).\",\"data_availability\":\"\",\"other_dataset_citations\":[]}"
        ],
        [
         "2",
         "10.1002_anie.202005531",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202005531.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.202005531.xml",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "bioc",
         "{\"title\":\"\",\"author\":\"Dong Wang\",\"abstract\":\"Abstract: Trifluoromethyl sulfoxides are a new class of trifluoromethylthiolating reagent.\\nThe sulfoxides engage in metal-free C\\ufffdH trifluoromethylthiolation with a range of (hetero)arenes.\\nThe method is also applicable to the function-alization of important compound classes, such as ligand derivatives and polyaromatics, and in the late-stage trifluor-omethylthiolation of medicines and agrochemicals.\\nThe iso-lation and characterization of a sulfonium salt intermediate supports an interrupted Pummerer reaction mechanism.\\nIncorporating fluorine into organic compounds is a useful tool in drug design and development.\\nThe fluoro group is well known to improve the pharmacokinetic properties of a mol-ecule and fluorine-18 is an important radioisotope in molec-ular imaging.[1,2] Trifluoromethylthio (SCF3) groups are commonly found in drug molecules and veterinary medi-cines.[3,4]\\nBy combining a fluorinated moiety with a hetero-atom, many have turned to the SCF3 group to impart useful properties, such as high lipophilicity, to a compound of interest.[5]\",\"data_availability\":\"\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.1002/anie.202005531'], citation_context: \\\"15213773, 2020, 37, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/anie.202005531 by California Digital Library University Of California, Wiley Online Library on [09/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\\\"}\"]}",
         "{\"title\":\"Trifluoromethyl Sulfoxides: Reagents for Metal\\u00e2\\u0080\\u0090Free C\\u00e2\\u0088\\u0092H Trifluoromethylthiolation\",\"author\":\"Wang Dong;prefix:Dr.\",\"abstract\":\"Abstract\",\"data_availability\":\"\",\"other_dataset_citations\":[]}"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>dataset_info</th>\n",
       "      <th>xml_type</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>xml_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.17882/4938...</td>\n",
       "      <td>tei</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Marie Barbieux\",\"abstrac...</td>\n",
       "      <td>{\"title\":\"Assessing the variability in the rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>jats</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Daniel Werner\",\"abstract...</td>\n",
       "      <td>{\"title\":\"Effective and Reversible Carbon Diox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>bioc</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Dong Wang\",\"abstract\":\"A...</td>\n",
       "      <td>{\"title\":\"Trifluoromethyl Sulfoxides: Reagents...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                                      pdf_file_path  \\\n",
       "0    10.1002_2017jc013030  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "1  10.1002_anie.201916483  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "2  10.1002_anie.202005531  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                       xml_file_path dataset_type  \\\n",
       "0  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "1  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "2  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "\n",
       "                                        dataset_info xml_type  \\\n",
       "0  [{'dataset_id': 'https://doi.org/10.17882/4938...      tei   \n",
       "1     [{'dataset_id': 'Missing', 'type': 'Missing'}]     jats   \n",
       "2     [{'dataset_id': 'Missing', 'type': 'Missing'}]     bioc   \n",
       "\n",
       "                                            pdf_text  \\\n",
       "0  {\"title\":\"\",\"author\":\"Marie Barbieux\",\"abstrac...   \n",
       "1  {\"title\":\"\",\"author\":\"Daniel Werner\",\"abstract...   \n",
       "2  {\"title\":\"\",\"author\":\"Dong Wang\",\"abstract\":\"A...   \n",
       "\n",
       "                                            xml_text  \n",
       "0  {\"title\":\"Assessing the variability in the rel...  \n",
       "1  {\"title\":\"Effective and Reversible Carbon Diox...  \n",
       "2  {\"title\":\"Trifluoromethyl Sulfoxides: Reagents...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the file paths DataFrame from the CSV file\n",
    "file_paths_df = pd.read_csv(os.path.join(BASE_OUTPUT_DIR, 'file_paths.csv'))\n",
    "# Fill NaN values in the 'xml_type' and 'xml_text' columns with empty strings\n",
    "file_paths_df['xml_file_path'] = file_paths_df['xml_file_path'].fillna('')\n",
    "file_paths_df['xml_text'] = file_paths_df['xml_text'].fillna('')\n",
    "# Display the first few rows of the file paths DataFrame\n",
    "print(f\"File paths DataFrame shape: {file_paths_df.shape}\")\n",
    "display(file_paths_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "76e021a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_info",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "b0e9b731-4ed7-4e1d-af6c-7eeaf20941f3",
       "rows": [
        [
         "0",
         "10.1002_mp.14424",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_mp.14424.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_mp.14424.xml",
         "test",
         "[{'dataset_id': 'https://doi.org/10.7937/k9/tcia.2015.pf0m9rei', 'type': 'Secondary'}, {'dataset_id': 'https://doi.org/10.7937/tcia.2020.6c7y-gq39', 'type': 'Primary'}]",
         "jats",
         "{\"title\":\"\",\"author\":\"Kendall J. Kisera\",\"abstract\":\"PleThora: Pleural effusion and thoracic cavity segmentations in diseased lungs for benchmarking chest CT processing pipelines Kendall J. Kisera)\\nJohn P. and Kathrine G. McGovern Medical School, Houston, TX, USA Center for Precision Health, UTHealth School of Biomedical Informatics, Houston, TX, USA Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA Sara Ahmed and Sonja Stieb Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA Abdallah S. R. Mohamed Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA MD Anderson Cancer Center-UTHealth Graduate School of Biomedical Sciences, Houston, TX, USA Hesham Elhalawani Department of Radiation Oncology, Cleveland Clinic Taussig Cancer Center, Cleveland, OH, USA Peter Y. S. Park, Nathan S. Doyle, and Brandon J. Wang Department of Diagnostic and Interventional Imaging, John P. and Kathrine G. McGovern Medical School, Houston, TX, USA Arko Barman, Zhao Li, and W. Jim Zheng Center for Precision Health, UTHealth School of Biomedical Informatics, Houston, TX, USA Clifton D. Fullera) Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA MD Anderson Cancer Center-UTHealth Graduate School of Biomedical Sciences, Houston, TX, USA Luca Giancardoa) Center for Precision Health, UTHealth School of Biomedical Informatics, Houston, TX, USA Department of Radiation Oncology, Cleveland Clinic Taussig Cancer Center, Cleveland, OH, USA (Received 7 April 2020; revised 22 July 2020; accepted for publication 27 July 2020; published 28 August 2020)\\nThis manuscript describes a dataset of thoracic cavity segmentations and discrete pleural effusion segmentations we have annotated on 402 computed tomography (CT) scans acquired from patients with non-small cell lung cancer.\\nThe segmentation of these anatomic regions precedes fundamental tasks in image analysis pipelines such as lung structure segmentation, lesion detection, and radiomics feature extraction.\\nBilateral thoracic cavity volumes and pleural effusion volumes were manually seg-me\",\"data_availability\":\"24734209, 2020, 11, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.14424 by California Digital Library University Of California, Wiley Online Library on [31/03/2025].\\nSee the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.7937/tcia.2020.6c7y-gq39'], citation_context: \\\"51. Kiser KJ, Ahmed S, Stieb SM, et al. Data from the thoracic volume and pleural effusion segmentations in diseased lungs for benchmarking chest CT processing pipelines Dataset. In: The Cancer Imaging Archive; 2020. https://doi.org/10.7937/tcia.2020.6c7y-gq39 52. Ronneberger O, Fischer P, Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation. Paper presented at: Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015; 5-9 October, 2015; Munich, Germany. https://doi.org/\\\"}\",\"{\\\"dataset_ids\\\": ['10.7937/K9/TCIA.2015.PF0M9REI'], citation_context: \\\"chive (TCIA): maintaining and operating a public information repository. J Digit Imag-ing. 2013;26:1045-1057. 22. Aerts HJWL, Wee L, Rios Velazquez E, et al. Data from NSCLC-Radio-mics Dataset. In: The Cancer Imaging Archive; 2019. https://doi.org/10.7937/K9/TCIA.2015.PF0M9REI 23. Aerts HJ, Velazquez ER, Leijenaar RT, et al. Decoding tumour pheno-type by noninvasive imaging using a quantitative radiomics approach. Nat Commun. 2014;5:4006. 24. Li X, Morgan PS, Ashburner J, Smith J, Rorden C. dcm2niix.exe com-puter program\\\"}\",\"{\\\"dataset_ids\\\": ['10.7937/tcia.2020.6c7y-gq39.51'], citation_context: \\\"We describe PleThora, a dataset of 402 expert-vetted tho-racic cavity segmentations, 78 expert-vetted pleural effusion segmentations, and corresponding clinical and technical metadata made available to the public through TCIA at https://doi.org/10.7937/tcia.2020.6c7y-gq39.51 These seg-mentations have value for preprocessing steps in image analy-sis pipelines built for fundamental quantitative imaging tasks, including but not limited to pathologic lung segmentation, lesion detection, and radiomics feature extraction.\\\"}\",\"{\\\"dataset_ids\\\": ['10.1002/mp.14424'], citation_context: \\\"24734209, 2020, 11, Downloaded from https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.14424 by California Digital Library University Of California, Wiley Online Library on [31/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\\\"}\"]}",
         "{\"title\":\"PleThora: Pleural effusion and thoracic cavity segmentations in diseased lungs for benchmarking chest CT processing pipelines\",\"author\":\"Kiser Kendall J.\",\"abstract\":\"This manuscript describes a dataset of thoracic cavity segmentations and discrete pleural effusion segmentations we have annotated on 402 computed tomography (CT) scans acquired from patients with non\\u2010small cell lung cancer. The segmentation of these anatomic regions precedes fundamental tasks in image analysis pipelines such as lung structure segmentation, lesion detection, and radiomics feature extraction. Bilateral thoracic cavity volumes and pleural effusion volumes were manually segmented on CT scans acquired from The Cancer Imaging Archive \\u201cNSCLC Radiomics\\u201d data collection. Four hundred and two thoracic segmentations were first generated automatically by a U\\u2010Net based algorithm trained on chest CTs without cancer, manually corrected by a medical student to include the complete thoracic cavity (normal, pathologic, and atelectatic lung parenchyma, lung hilum, pleural effusion, fibrosis, nodules, tumor, and other anatomic anomalies), and revised by a radiation oncologist or a radiologist. Seventy\\u2010eight pleural effusions were manually segmented by a medical student and revised by a radiologist or radiation oncologist. Interobserver agreement between the radiation oncologist and radiologist corrections was acceptable. All expert\\u2010vetted segmentations are publicly available in NIfTI format through The Cancer Imaging Archive at  https://doi.org/10.7937/tcia.2020.6c7y\\u2010gq39 . Tabular data detailing clinical and technical metadata linked to segmentation cases are also available. Thoracic cavity segmentations will be valuable for developing image analysis pipelines on pathologic lungs \\u2014 where current automated algorithms struggle most. In conjunction with gross tumor volume segmentations already available from \\u201cNSCLC Radiomics,\\u201d pleural effusion segmentations may be valuable for investigating radiomics profile differences between effusion and primary tumor or training algorithms to discriminate between them.\",\"data_availability\":\"\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.7937/K9/TCIA.2015.PF0M9REI'], citation_context: \\\"Aerts HJWL , Wee L , Rios Velazquez E , et al. Data from NSCLC-Radiomics [Dataset] . In: The Cancer Imaging Archive ; 2019 10.7937/K9/TCIA.2015.PF0M9REI\\\"}\",\"{\\\"dataset_ids\\\": ['10.7937/tcia.2020.6c7y-gq39'], citation_context: \\\"Kiser KJ , Ahmed S , Stieb SM , et al. Data from the thoracic volume and pleural effusion segmentations in diseased lungs for benchmarking chest CT processing pipelines [Dataset] . In: The Cancer Imaging Archive ; 2020 10.7937/tcia.2020.6c7y-gq39\\\"}\"]}"
        ],
        [
         "1",
         "10.1002_ece3.6144",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6144.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.6144.xml",
         "test",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.zw3r22854', 'type': 'Primary'}]",
         "jats",
         "{\"title\":\"\",\"author\":\"Beng\",\"abstract\":\"Ecology and Evolution.\\n2020;10:3463-3476.\\n\\ufeff\\b\\n| 3463 www.ecolevol.org 1 | INTRODUCTION Marine ecosystems can change rapidly in response to both natural and anthropogenic changes in the environment.\\nMonitoring changes in the abundance and distribution of marine organisms is a chal-lenging but critical component of conservation efforts and natural resource management.\\nHuman impacts such as overfishing, coastal development, and pollution can rapidly increase stress on ecosys-tems resulting in an urgent need to develop efficient methods for monitoring spatial and temporal dynamics of biodiversity (Beng et al., 2016).\\nFisheries management has long used ichthyoplankton sur-veys as a component of monitoring because sampling the early life stages of fish reflects the reproductive activities of fish Received: 17 July 2019 | Revised:\\n31 January 2020 |\\nAccepted: 4 February 2020 DOI: 10.1002/ece3.6144 O R\\nI G I N A L R E S E A R C H Efficacy of metabarcoding for identification of fish eggs evaluated with mock communities Elena M. Duke | Ronald S. Burton\\nThis is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited.\\n\\u00a9 2020 The Authors.\\nEcology and Evolution published by John Wiley & Sons Ltd. Marine Biology Research Division, Scripps Institution of Oceanography, University of California, San Diego, La Jolla, California Correspondence Elena M. Duke, Marine Biology Research Division, Scripps Institution of Oceanography, University of California, San Diego, La Jolla, CA.\\nEmail: emduke@ucsd.edu\\nAbstract There is urgent need for effective and efficient monitoring of marine fish popula-tions.\\nMonitor\",\"data_availability\":\"DATA AVAILABILITY STATEMENT\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.1002/ece3.6144'], citation_context: \\\"20457758, 2020, 7, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/ece3.6144, Wiley Online Library on [31/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\\\"}\",\"{\\\"dataset_ids\\\": ['10.1371/journal.pone.0134647'], citation_context: \\\"Hermsmeier, M. C., Rogowski, P. A., Terrill, E., Burton, R. S., & Bernardi, G. (2015). Monitoring spawning ac-tivity in a Southern California marine protected area using molec-ular identification of fish eggs. PLoS ONE, 10(8), 1-21. https://doi. org/10.1371/journal.pone.0134647 Hastings, P. A., & Burton, R. S. (2008). Establishing a DNA Sequence Database for the Marine Fish Fauna of California. Retrieved from https://escholarship.org/uc/item/1ck9b3qs Hatzenbuhler, C., Kelly, J. R., Martinson, J., Okum, S., & Pilgrim, E. (2\\\"}\"]}",
         "{\"title\":\"Efficacy of metabarcoding for identification of fish eggs evaluated with mock communities\",\"author\":\"Duke Elena M.\",\"abstract\":\"Abstract There is urgent need for effective and efficient monitoring of marine fish populations. Monitoring eggs and larval fish may be more informative\\u00a0than that traditional fish surveys since ichthyoplankton surveys reveal the reproductive activities of fish populations, which directly impact their population trajectories. Ichthyoplankton surveys have turned to molecular methods (DNA barcoding & metabarcoding) for identification of eggs and larval fish due to challenges of morphological identification. In this study, we examine the effectiveness of using metabarcoding methods on mock communities of known fish egg DNA. We constructed six mock communities with known ratios of species. In addition, we analyzed two samples from a large field collection of fish eggs and compared metabarcoding results with traditional DNA barcoding results. We examine the ability of our metabarcoding methods to detect species and relative proportion of species identified in each mock community. We found that our metabarcoding methods were able to detect species at very low input proportions; however, levels of successful detection depended on the markers used in amplification, suggesting that the use of multiple markers is desirable. Variability in our quantitative results may result from amplification bias as well as interspecific variation in mitochondrial DNA copy number. Our results demonstrate that there remain significant challenges to using metabarcoding for estimating proportional species composition; however, the results provide important insights into understanding how to interpret metabarcoding data. This study will aid in the continuing development of efficient molecular methods of biological monitoring for fisheries management.\",\"data_availability\":\"DATA AVAILABILITY STATEMENT Data are available at Dryad Digital Repository at:  https://doi.org/10.5061/dryad.zw3r22854 . Two sequences used in our reference library are on GenBank (accession numbers MH714866 and MH718435).\",\"other_dataset_citations\":[]}"
        ],
        [
         "2",
         "10.1002_ejoc.202000139",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejoc.202000139.pdf",
         "",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "unknown",
         "{\"title\":\"\",\"author\":\"Ann Christin Reiers\\u00f8lmoen[a\",\"abstract\":\"Abstract: Studies on gold(III) coordination of a series of pre-pared polydentate pyridine and quinoline based ligands are re-ported.\\nCharacterization (1H, 13C, 15N NMR, and XRD) of the novel gold(III) complexes, prepared in 31-98 % yield, revealed different coordination ability of the pyridine and quinoline nitrogen atoms.\\nTesting of catalytic activity in cyclopropanation\",\"data_availability\":\"10990690, 2020, 19, Downloaded from https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ejoc.202000139 by California Digital Library University Of California, Wiley Online Library on [09/05/2025].\\nSee the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.1002/ejoc.202000139'], citation_context: \\\"10990690, 2020, 19, Downloaded from https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ejoc.202000139 by California Digital Library University Of California, Wiley Online Library on [09/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\\\"}\"]}",
         ""
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>dataset_info</th>\n",
       "      <th>xml_type</th>\n",
       "      <th>pdf_text</th>\n",
       "      <th>xml_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_mp.14424</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.7937/k9/tc...</td>\n",
       "      <td>jats</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Kendall J. Kisera\",\"abst...</td>\n",
       "      <td>{\"title\":\"PleThora: Pleural effusion and thora...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_ece3.6144</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "      <td>jats</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Beng\",\"abstract\":\"Ecolog...</td>\n",
       "      <td>{\"title\":\"Efficacy of metabarcoding for identi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_ejoc.202000139</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>unknown</td>\n",
       "      <td>{\"title\":\"\",\"author\":\"Ann Christin Reiers\\u00f...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                                      pdf_file_path  \\\n",
       "0        10.1002_mp.14424  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "1       10.1002_ece3.6144  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "2  10.1002_ejoc.202000139  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                       xml_file_path dataset_type  \\\n",
       "0  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "1  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "2                                                            test   \n",
       "\n",
       "                                        dataset_info xml_type  \\\n",
       "0  [{'dataset_id': 'https://doi.org/10.7937/k9/tc...     jats   \n",
       "1  [{'dataset_id': 'https://doi.org/10.5061/dryad...     jats   \n",
       "2     [{'dataset_id': 'Missing', 'type': 'Missing'}]  unknown   \n",
       "\n",
       "                                            pdf_text  \\\n",
       "0  {\"title\":\"\",\"author\":\"Kendall J. Kisera\",\"abst...   \n",
       "1  {\"title\":\"\",\"author\":\"Beng\",\"abstract\":\"Ecolog...   \n",
       "2  {\"title\":\"\",\"author\":\"Ann Christin Reiers\\u00f...   \n",
       "\n",
       "                                            xml_text  \n",
       "0  {\"title\":\"PleThora: Pleural effusion and thora...  \n",
       "1  {\"title\":\"Efficacy of metabarcoding for identi...  \n",
       "2                                                     "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_file_paths_df = file_paths_df.copy().sample(3, random_state=42).reset_index(drop=True)\n",
    "sample_file_paths_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2524bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article 0/3: 10.1002_mp.14424, type: XML\n",
      "Preparing input with length: 2784\n",
      "Parsed response: [{'dataset_id': '10.7937/tcia.2020.6c7y-gq39', 'type': 'Primary'}, {'dataset_id': '10.7937/K9/TCIA.2015.PF0M9REI', 'type': 'Secondary'}]\n",
      "Processing article 1/3: 10.1002_ece3.6144, type: XML\n",
      "Preparing input with length: 2184\n",
      "Parsed response: [{'dataset_id': '10.5061/dryad.zw3r22854', 'type': 'Primary'}]\n",
      "Processing article 2/3: 10.1002_ejoc.202000139, type: PDF\n",
      "Preparing input with length: 1383\n",
      "Parsed response: [{'dataset_id': '10.1002/ejoc.202000139', 'type': 'Primary'}, {'dataset_id': '10.1002/ejoc.202000139', 'type': 'Primary'}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_info",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "llm_input",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "llm_response",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "llm_thinking_content",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0c014ded-9383-4015-a567-957dd5d594c8",
       "rows": [
        [
         "0",
         "10.1002_ece3.6144",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.zw3r22854', 'type': 'Primary'}]",
         "XML",
         "Text Content: {\"title\":\"Efficacy of metabarcoding for identification of fish eggs evaluated with mock communities\",\"author\":\"Duke Elena M.\",\"abstract\":\"Abstract There is urgent need for effective and efficient monitoring of marine fish populations. Monitoring eggs and larval fish may be more informative\\u00a0than that traditional fish surveys since ichthyoplankton surveys reveal the reproductive activities of fish populations, which directly impact their population trajectories. Ichthyoplankton surveys have turned to molecular methods (DNA barcoding & metabarcoding) for identification of eggs and larval fish due to challenges of morphological identification. In this study, we examine the effectiveness of using metabarcoding methods on mock communities of known fish egg DNA. We constructed six mock communities with known ratios of species. In addition, we analyzed two samples from a large field collection of fish eggs and compared metabarcoding results with traditional DNA barcoding results. We examine the ability of our metabarcoding methods to detect species and relative proportion of species identified in each mock community. We found that our metabarcoding methods were able to detect species at very low input proportions; however, levels of successful detection depended on the markers used in amplification, suggesting that the use of multiple markers is desirable. Variability in our quantitative results may result from amplification bias as well as interspecific variation in mitochondrial DNA copy number. Our results demonstrate that there remain significant challenges to using metabarcoding for estimating proportional species composition; however, the results provide important insights into understanding how to interpret metabarcoding data. This study will aid in the continuing development of efficient molecular methods of biological monitoring for fisheries management.\",\"data_availability\":\"DATA AVAILABILITY STATEMENT Data are available at Dryad Digital Repository at:  https://doi.org/10.5061/dryad.zw3r22854 . Two sequences used in our reference library are on GenBank (accession numbers MH714866 and MH718435).\",\"other_dataset_citations\":[]}\n",
         "[{'dataset_id': '10.5061/dryad.zw3r22854', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON object with a paper section and wants me to extract dataset IDs. First, I need to parse the data_availability and other_dataset_citations sections.\n\nLooking at the abstract, the data_availability mentions a DOI: https://doi.org/10.5061/dryad.zw3r22854. That looks like a DOI. I need to check if this is a dataset ID. The key rule is to extract DOIs that are explicitly used as dataset IDs. Since the DOI is provided here, and it's in the correct format (starts with 10.), it's a valid dataset ID. So I'll note that.\n\nNext, the other_dataset_citations section is empty. No additional citations are provided. So no other dataset IDs to extract here.\n\nNow, the classification. The abstract mentions \"fish eggs and larval fish\" and uses metabarcoding. The DOI is from Dryad, which is a dataset repository. The original data generation is the authors of the paper, so this should be Primary.\n\nI need to make sure I only include the valid IDs. The DOI is valid and matches the dataset ID criteria. The other_dataset_citations are empty, so no issues there.\n\nSo the final JSON should have the dataset_id with the DOI and the type as Primary.\n</think>"
        ],
        [
         "1",
         "10.1002_ejoc.202000139",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "PDF",
         "Text Content: {\"title\":\"\",\"author\":\"Ann Christin Reiers\\u00f8lmoen[a\",\"abstract\":\"Abstract: Studies on gold(III) coordination of a series of pre-pared polydentate pyridine and quinoline based ligands are re-ported.\\nCharacterization (1H, 13C, 15N NMR, and XRD) of the novel gold(III) complexes, prepared in 31-98 % yield, revealed different coordination ability of the pyridine and quinoline nitrogen atoms.\\nTesting of catalytic activity in cyclopropanation\",\"data_availability\":\"10990690, 2020, 19, Downloaded from https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ejoc.202000139 by California Digital Library University Of California, Wiley Online Library on [09/05/2025].\\nSee the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.1002/ejoc.202000139'], citation_context: \\\"10990690, 2020, 19, Downloaded from https://chemistry-europe.onlinelibrary.wiley.com/doi/10.1002/ejoc.202000139 by California Digital Library University Of California, Wiley Online Library on [09/05/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\\\"}\"]}\n",
         "[{'dataset_id': '10.1002/ejoc.202000139', 'type': 'Primary'}, {'dataset_id': '10.1002/ejoc.202000139', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided a JSON structure with some text content and wants me to extract dataset IDs. Let me start by understanding the input.\n\nFirst, looking at the \"data_availability\" section. The text mentions \"10990690, 2020, 19, Downloaded from...\" which looks like a DOI. The DOI format is [prefix]/[suffix], and the prefix is 10. followed by a number. The suffix here is 19, which is a 2-digit number. So that's a valid DOI. But wait, the user's instruction says to check if the DOI is explicitly used as a dataset ID. Since the DOI is there, and the citation context includes it, I should extract this as a dataset ID. But also, the citation context mentions the same DOI, so maybe it's part of the other_dataset_citations. Let me check the other_dataset_citations list. There's another entry with the same DOI. But the user's task says to process the first instance, so maybe both are valid. However, the user's example shows that when multiple entries have the same DOI, only the first one is considered. So in this case, both entries would be considered as valid dataset IDs. But wait, the data_availability section has only one DOI here. So maybe the other entry is in other_dataset_citations. So in Task 1, both entries would be considered. But the user's input has two entries: the first in data_availability and the second in other_dataset_citations. So both would be valid. But the user's example in the task shows that if multiple entries have the same DOI, only the first is processed. So I need to process the first one. Therefore, both entries would be valid dataset IDs. Now, for classification, looking at the abstract. The abstract mentions \"characterization (1H, 13C, 15N NMR, and XRD) of the novel gold(III) complexes, prepared in 31-98 % yield, revealed different coordination ability...\" which doesn't mention anything about the origin of the data. The citation context also refers to the same DOI. So the classification would be Primary. Wait, but the other dataset citation also has the same DOI. But since the data_availability section only has one DOI, perhaps only the first one is considered. So in Task 1, both entries would be valid. Then, for classification, the abstract doesn't mention origin, so Primary. So the dataset IDs would be both entries, and the types would be Primary. But wait, the data_availability section only has one DOI here. So maybe the other entry is in other_dataset_citations. Therefore, both entries are valid. Now, the user's example shows that if multiple entries have the same DOI, only the first is considered. So in this case, both entries would be valid. Then, the classification would be Primary for both. So the final JSON would have two dataset IDs, both Primary.\n</think>"
        ],
        [
         "2",
         "10.1002_mp.14424",
         "[{'dataset_id': 'https://doi.org/10.7937/k9/tcia.2015.pf0m9rei', 'type': 'Secondary'}, {'dataset_id': 'https://doi.org/10.7937/tcia.2020.6c7y-gq39', 'type': 'Primary'}]",
         "XML",
         "Text Content: {\"title\":\"PleThora: Pleural effusion and thoracic cavity segmentations in diseased lungs for benchmarking chest CT processing pipelines\",\"author\":\"Kiser Kendall J.\",\"abstract\":\"This manuscript describes a dataset of thoracic cavity segmentations and discrete pleural effusion segmentations we have annotated on 402 computed tomography (CT) scans acquired from patients with non\\u2010small cell lung cancer. The segmentation of these anatomic regions precedes fundamental tasks in image analysis pipelines such as lung structure segmentation, lesion detection, and radiomics feature extraction. Bilateral thoracic cavity volumes and pleural effusion volumes were manually segmented on CT scans acquired from The Cancer Imaging Archive \\u201cNSCLC Radiomics\\u201d data collection. Four hundred and two thoracic segmentations were first generated automatically by a U\\u2010Net based algorithm trained on chest CTs without cancer, manually corrected by a medical student to include the complete thoracic cavity (normal, pathologic, and atelectatic lung parenchyma, lung hilum, pleural effusion, fibrosis, nodules, tumor, and other anatomic anomalies), and revised by a radiation oncologist or a radiologist. Seventy\\u2010eight pleural effusions were manually segmented by a medical student and revised by a radiologist or radiation oncologist. Interobserver agreement between the radiation oncologist and radiologist corrections was acceptable. All expert\\u2010vetted segmentations are publicly available in NIfTI format through The Cancer Imaging Archive at  https://doi.org/10.7937/tcia.2020.6c7y\\u2010gq39 . Tabular data detailing clinical and technical metadata linked to segmentation cases are also available. Thoracic cavity segmentations will be valuable for developing image analysis pipelines on pathologic lungs \\u2014 where current automated algorithms struggle most. In conjunction with gross tumor volume segmentations already available from \\u201cNSCLC Radiomics,\\u201d pleural effusion segmentations may be valuable for investigating radiomics profile differences between effusion and primary tumor or training algorithms to discriminate between them.\",\"data_availability\":\"\",\"other_dataset_citations\":[\"{\\\"dataset_ids\\\": ['10.7937/K9/TCIA.2015.PF0M9REI'], citation_context: \\\"Aerts HJWL , Wee L , Rios Velazquez E , et al. Data from NSCLC-Radiomics [Dataset] . In: The Cancer Imaging Archive ; 2019 10.7937/K9/TCIA.2015.PF0M9REI\\\"}\",\"{\\\"dataset_ids\\\": ['10.7937/tcia.2020.6c7y-gq39'], citation_context: \\\"Kiser KJ , Ahmed S , Stieb SM , et al. Data from the thoracic volume and pleural effusion segmentations in diseased lungs for benchmarking chest CT processing pipelines [Dataset] . In: The Cancer Imaging Archive ; 2020 10.7937/tcia.2020.6c7y-gq39\\\"}\"]}\n",
         "[{'dataset_id': '10.7937/tcia.2020.6c7y-gq39', 'type': 'Primary'}, {'dataset_id': '10.7937/K9/TCIA.2015.PF0M9REI', 'type': 'Secondary'}]",
         "<think>\nOkay, let's tackle this problem step by step. The user provided a JSON string containing a paper section with data about a dataset. My job is to extract valid dataset IDs and classify them as Primary or Secondary based on the given guidelines.\n\nFirst, I need to check the `data_availability` section. The abstract mentions CT scans from The Cancer Imaging Archive (NSCLC Radiomics). The DOI provided is 10.7937/tcia.2020.6c7y-gq39. I need to verify if this is a dataset ID. The important rule says only extract DOIs that are explicitly used as dataset IDs. The citation context here says it's a dataset from NSCLC Radiomics, so that's a valid DOI. So dataset_id 10.7937/tcia.2020.6c7y-gq39 is valid.\n\nNext, check the `other_dataset_citations`. The first entry has dataset_ids [\"10.7937/K9/TCIA.2015.PF0M9REI\"] and a citation context. The abstract mentions \"data from NSCLC-Radiomics [Dataset]\" which matches the first DOI. The second entry has another DOI, but the citation context is about another dataset for benchmarking chest CT pipelines, which doesn't seem to be the same dataset. So only the first DOI is valid here.\n\nNow, classify these dataset IDs. The first one, 10.7937/tcia.2020.6c7y-gq39, is from the thoracic volume and pleural effusion segmentations dataset. The abstract mentions that this dataset was annotated and used in the study. The keywords like \"segmentations\" and \"annotated\" indicate that the raw data was generated by the authors of the paper, so it's Primary. The second DOI in other_citations is from a different dataset, so it's Secondary.\n\nWait, but the abstract says the authors used data from NSCLC Radiomics, which is the first dataset. So the first DOI is Primary, the second is Secondary. So the final JSON should have both entries, each classified as Primary and Secondary.\n\nI need to make sure I don't extract any other data. The other_dataset_citations have citations, but they don't provide valid dataset IDs. So only the first and second entries are relevant.\n\nSo the output should be two entries in JSON, each with the dataset_id and type.\n</think>"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_info</th>\n",
       "      <th>text_type</th>\n",
       "      <th>llm_input</th>\n",
       "      <th>llm_response</th>\n",
       "      <th>llm_thinking_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_ece3.6144</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"Efficacy of metabarcod...</td>\n",
       "      <td>[{'dataset_id': '10.5061/dryad.zw3r22854', 'ty...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_ejoc.202000139</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>PDF</td>\n",
       "      <td>Text Content: {\"title\":\"\",\"author\":\"Ann Christ...</td>\n",
       "      <td>[{'dataset_id': '10.1002/ejoc.202000139', 'typ...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_mp.14424</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.7937/k9/tc...</td>\n",
       "      <td>XML</td>\n",
       "      <td>Text Content: {\"title\":\"PleThora: Pleural effu...</td>\n",
       "      <td>[{'dataset_id': '10.7937/tcia.2020.6c7y-gq39',...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                                       dataset_info  \\\n",
       "0       10.1002_ece3.6144  [{'dataset_id': 'https://doi.org/10.5061/dryad...   \n",
       "1  10.1002_ejoc.202000139     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "2        10.1002_mp.14424  [{'dataset_id': 'https://doi.org/10.7937/k9/tc...   \n",
       "\n",
       "  text_type                                          llm_input  \\\n",
       "0       XML  Text Content: {\"title\":\"Efficacy of metabarcod...   \n",
       "1       PDF  Text Content: {\"title\":\"\",\"author\":\"Ann Christ...   \n",
       "2       XML  Text Content: {\"title\":\"PleThora: Pleural effu...   \n",
       "\n",
       "                                        llm_response  \\\n",
       "0  [{'dataset_id': '10.5061/dryad.zw3r22854', 'ty...   \n",
       "1  [{'dataset_id': '10.1002/ejoc.202000139', 'typ...   \n",
       "2  [{'dataset_id': '10.7937/tcia.2020.6c7y-gq39',...   \n",
       "\n",
       "                                llm_thinking_content  \n",
       "0  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "1  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "2  <think>\\nOkay, let's tackle this problem step ...  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_articles_df = evaluate_articles(file_paths_df, inference_model)\n",
    "# Save processed_articles_df to CSV\n",
    "processed_articles_df.to_csv(\"sample_evaluated_articles.csv\", index=False)\n",
    "print(f\"Processed articles DataFrame shape: {processed_articles_df.shape}\")\n",
    "processed_articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d9cf05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset_id(dataset_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Formats the dataset_id by removing any leading/trailing whitespace and ensuring it is a string.\n",
    "    \n",
    "    Args:\n",
    "        dataset_id (str): The dataset identifier to format.\n",
    "        \n",
    "    Returns:\n",
    "        str: The formatted dataset identifier.\n",
    "    \"\"\"\n",
    "    if dataset_id and dataset_id.startswith(\"10.\") and len(dataset_id) > 10:\n",
    "        # If the dataset_id starts with \"10.\" and is longer than 10 characters, it's likely a DOI\n",
    "        dataset_id = \"https://doi.org/\" + dataset_id.lower().strip()\n",
    "    return dataset_id\n",
    "\n",
    "# Create a DataFrame to hold the evaluation results by expaning the 'llm_response' column\n",
    "def expand_evaluation_results(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expands the evaluation results DataFrame by extracting dataset_id and type from the 'llm_response' column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing evaluation results.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with expanded dataset_id and type columns.\n",
    "    \"\"\"\n",
    "    expanded_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        article_id = row['article_id']\n",
    "        article_doi = article_id.replace('_', '/')\n",
    "        datasets = row['llm_response']\n",
    "        if datasets:\n",
    "            for dataset in datasets:\n",
    "                dataset_id = dataset.get('dataset_id', 'Missing')\n",
    "                # Skip if the dataset_id is the same as the article DOI\n",
    "                if dataset_id == article_doi:\n",
    "                    # If the dataset_id is the same as the article DOI add it as Missing\n",
    "                    expanded_rows.append({\n",
    "                        'article_id': article_id,\n",
    "                        'dataset_id': 'Missing',\n",
    "                        'type': 'Missing',\n",
    "                    })\n",
    "                else:\n",
    "                    expanded_rows.append({\n",
    "                        'article_id': article_id,\n",
    "                        'dataset_id': dataset.get('dataset_id', 'Missing'),\n",
    "                        'type': dataset.get('type', 'Missing'),\n",
    "                    })\n",
    "    \n",
    "    # Create a DataFrame from the expanded rows\n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "    expanded_df['dataset_id'] = expanded_df['dataset_id'].apply(format_dataset_id)  # Format dataset_id\n",
    "    expanded_df['type'] = expanded_df['type'].str.strip().str.capitalize()  # Ensure type is capitalized and stripped of whitespace\n",
    "    expanded_df = expanded_df.sort_values(by=[\"article_id\", \"dataset_id\", \"type\"], ascending=False).drop_duplicates(subset=['article_id', 'dataset_id'], keep=\"first\").reset_index(drop=True)\n",
    "    \n",
    "    return expanded_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "5e9bc75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission DataFrame shape: (4, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "adebc829-f8e5-412f-9beb-23024001b6d2",
       "rows": [
        [
         "0",
         "10.1002_mp.14424",
         "https://doi.org/10.7937/tcia.2020.6c7y-gq39",
         "Primary"
        ],
        [
         "1",
         "10.1002_mp.14424",
         "https://doi.org/10.7937/k9/tcia.2015.pf0m9rei",
         "Secondary"
        ],
        [
         "2",
         "10.1002_ejoc.202000139",
         "Missing",
         "Missing"
        ],
        [
         "3",
         "10.1002_ece3.6144",
         "https://doi.org/10.5061/dryad.zw3r22854",
         "Primary"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_mp.14424</td>\n",
       "      <td>https://doi.org/10.7937/tcia.2020.6c7y-gq39</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_mp.14424</td>\n",
       "      <td>https://doi.org/10.7937/k9/tcia.2015.pf0m9rei</td>\n",
       "      <td>Secondary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_ejoc.202000139</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1002_ece3.6144</td>\n",
       "      <td>https://doi.org/10.5061/dryad.zw3r22854</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                                     dataset_id  \\\n",
       "0        10.1002_mp.14424    https://doi.org/10.7937/tcia.2020.6c7y-gq39   \n",
       "1        10.1002_mp.14424  https://doi.org/10.7937/k9/tcia.2015.pf0m9rei   \n",
       "2  10.1002_ejoc.202000139                                        Missing   \n",
       "3       10.1002_ece3.6144        https://doi.org/10.5061/dryad.zw3r22854   \n",
       "\n",
       "        type  \n",
       "0    Primary  \n",
       "1  Secondary  \n",
       "2    Missing  \n",
       "3    Primary  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_submission_df = expand_evaluation_results(processed_articles_df)\n",
    "sample_submission_df.to_csv(os.path.join(BASE_OUTPUT_DIR, 'sample_submission.csv'), index=False)\n",
    "print(f\"Submission DataFrame shape: {sample_submission_df.shape}\")\n",
    "display(sample_submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
