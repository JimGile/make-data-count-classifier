{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "d2db1bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- 0. Environment Setup & Offline Preparation ---\n",
    "\n",
    "# Standard Imports\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections # For deque in parenthesis removal\n",
    "import fitz # PyMuPDF for PDF processing\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.tokenization_utils import PreTrainedTokenizer\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from transformers.training_args import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "from datasets import Dataset # Hugging Face datasets library\n",
    "import kagglehub\n",
    "import json\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = \"cuda\" if torch and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48e263db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for file paths and model configurations\n",
    "BASE_INPUT_DIR = './kaggle/input/make-data-count-finding-data-references'\n",
    "ARTICLE_TRAIN_DIR = os.path.join(BASE_INPUT_DIR, 'train')\n",
    "ARTICLE_TEST_DIR = os.path.join(BASE_INPUT_DIR, 'test')\n",
    "\n",
    "# Define directories for articles in train and test sets\n",
    "LABELED_TRAINING_DATA_CSV_PATH = os.path.join(BASE_INPUT_DIR, 'train_labels.csv')\n",
    "\n",
    "# Define the base model path\n",
    "QWEN_BASE_MODEL_PATH = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "\n",
    "# Output directory for the fine-tuned model and results\n",
    "BASE_OUTPUT_DIR = \"./kaggle/working\"\n",
    "FINE_TUNED_MODEL_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, \"qwen_finetuned_dataset_classifier\")\n",
    "FINAL_RESULTS_CSV_PATH = os.path.join(BASE_OUTPUT_DIR, \"article_dataset_classification.csv\")\n",
    "\n",
    "inference_model = None\n",
    "inference_tokenizer = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "3d64ef00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOI_PATTERN to handle optional 'https://doi.org/' prefix\n",
    "# and to allow EITHER '/' or '.' as the separator between the registrant code and suffix.\n",
    "# This addresses the case like '10.1234.data.archive.'\n",
    "\n",
    "# Explanation of the new pattern:\n",
    "# \\b(                                : Start of the main capturing group for the entire DOI string.\n",
    "#   (?:                             : Start of optional non-capturing group for the URL prefix.\n",
    "#     https?:\\s*//\\s*doi\\s*\\.\\s*org\\s*/\\s* : Matches 'http://doi.org/' or 'https://doi.org/'\n",
    "#                                         : allowing for zero or more whitespace characters (\\s*)\n",
    "#                                         : between each part (e.g., 'https://\\ndoi.org/').\n",
    "#   )?                              : Makes the entire URL prefix optional.\n",
    "#   10\\.\\d{4,9}[./]                 : Matches the standard DOI prefix (e.g., \"10.1002/\")\n",
    "#                                   : BUT now allows EITHER a '/' or a '.' as the separator\n",
    "#                                   : after the 4-9 digits. This is the key change.\n",
    "#   [\\s\\S]*?                        : Matches any character (including newlines and spaces)\n",
    "#                                   : zero or more times, *non-greedily*. This consumes the DOI suffix.\n",
    "# )                                 : End of the main capturing group. This group will contain the full DOI\n",
    "#                                   : including the optional URL prefix and any internal newlines.\n",
    "# (?=                                : Positive lookahead for termination (same as before):\n",
    "#   \\s*\\n\\s*[A-Z]                   : Newline followed by a Capital letter.\n",
    "#   | \\n{2,}                        : Two or more consecutive newlines.\n",
    "#   | [.?!;]\\s*[A-Z]                : Sentence-ending punctuation followed by a Capital letter.\n",
    "#   | \\s{2,}                        : Two or more consecutive spaces.\n",
    "#   | \\b(?:The|This|And|In|For|With|A|An|But|Or|So|If|As|By|On|At|From|To)\\b : Common sentence starters.\n",
    "#   | $                             : End of the string.\n",
    "# )\n",
    "#DOI_PATTERN_ROBUST_MULTILINE = r'\\b((?:https?:\\s*//\\s*doi\\s*\\.\\s*org\\s*/\\s*)?10\\.\\d{4,9}[./][\\s\\S]*?)(?=\\s*\\n\\s*[A-Z]|\\n{2,}|[.?!;]\\s*[A-Z]|\\s{2,}|\\b(?:The|This|And|In|For|With|A|An|But|Or|So|If|As|By|On|At|From|To)\\b|$)'\n",
    "#DOI_PATTERN_ROBUST_MULTILINE = r'\\b((?:https?:\\s*//\\s*doi\\s*\\.\\s*org\\s*/\\s*)?10\\.\\d{4,9}[./][\\s\\S]*?)(?=\\s*\\n\\s*[A-Z]|\\n{2,}|[.?!;]\\s*[A-Z]|\\s{2,}|\\b(?:The|This|And|In|For|With|A|An|But|Or|So|If|As|By|On|At|From|To)\\b|$)'\n",
    "DOI_PATTERN_ROBUST_MULTILINE = r'\\b((?:https?:\\s*//\\s*doi\\s*\\.\\s*org\\s*/\\s*)?10\\.\\s*\\d{4,9}[./][\\s\\S]*?)(?=\\s*\\n\\s*[A-Z]|\\n{2,}|[.?!;]\\s*[A-Z]|\\s{2,}|\\b(?:The|This|And|In|For|With|A|An|But|Or|So|If|As|By|On|At|From|To)\\b|$)'\n",
    "\n",
    "\n",
    "def clean_doi_match(match_obj: re.Match) -> str:\n",
    "    \"\"\"\n",
    "    Replacement function for re.sub().\n",
    "    Takes a match object (which contains the full matched DOI, potentially with newlines\n",
    "    and the optional 'https://doi.org/' prefix) and removes all internal whitespace\n",
    "    characters to produce a clean, continuous DOI string.\n",
    "    \"\"\"\n",
    "    matched_doi = match_obj.group(1) # Get the content of the first (and only) capturing group\n",
    "    cleaned_doi = re.sub(r'\\s+', '', matched_doi) # Remove all whitespace\n",
    "\n",
    "    # Prepend 'https://doi.org/' if it was not already included\n",
    "    if not cleaned_doi.startswith('https://doi.org/'):\n",
    "        cleaned_doi = 'https://doi.org/' + cleaned_doi\n",
    "    return cleaned_doi\n",
    "\n",
    "def clean_doi_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input text by removing unwanted characters and normalizing whitespace.\n",
    "    \"\"\"\n",
    "    # Check if the input text is empty or None\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    # The re.DOTALL flag is essential here because `[\\s\\S]` is used.\n",
    "    return re.sub(DOI_PATTERN_ROBUST_MULTILINE, clean_doi_match, text, flags=re.DOTALL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "fd5e3668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text after cleaning DOIs:\n",
      "\n",
      "This article discusses various research methods.\n",
      "A key reference is found via its DOI: https://doi.org/10.1002/(some.journal)-1234.5678.\n",
      "This is a new sentence starting here.\n",
      "\n",
      "Another important dataset has the DOI: https://doi.org/10.1234.data.archive.9876.5432. The data was analyzed.\n",
      "\n",
      "Yet another: https://doi.org/10.1234/multi.line.prefix.\n",
      "And a final one: https://doi.org/10.9999/last.one.\n",
      "\n",
      "\n",
      "Text after cleaning DOIs 2:\n",
      "\n",
      "Here's a DOI: https://doi.org/10.1234/test.doi. This is a new sentence.\n",
      "Another DOI: https://doi.org/10.5678/another.doi. And a third: https://doi.org/10.9876/third.doi\n",
      "\n",
      "[24] This is a new sentence starting here with a bullet point.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage ---\n",
    "test_pdf_text = \"\"\"\n",
    "This article discusses various research methods.\n",
    "A key reference is found via its DOI: 10.1002/\n",
    "(some.journal)-1234.5678.\n",
    "This is a new sentence starting here.\n",
    "\n",
    "Another important dataset has the DOI: https://doi.org/10.\n",
    "1234.data.archive.\n",
    "9876.5432. The data was analyzed.\n",
    "\n",
    "Yet another: https://\n",
    "doi.org/10.1234/\n",
    "multi.line.\n",
    "prefix.\n",
    "And a final one: 10.9999/last.one.\n",
    "\"\"\"\n",
    "\n",
    "# The re.DOTALL flag is essential here because `[\\s\\S]` is used.\n",
    "cleaned_text = clean_doi_text(test_pdf_text)\n",
    "\n",
    "print(\"\\nText after cleaning DOIs:\")\n",
    "print(cleaned_text)\n",
    "\n",
    "# Test case: DOI with prefix on a single line\n",
    "test_pdf_text_2 = \"\"\"\n",
    "Here's a DOI: https://doi.org/10.1234/test.doi. This is a new sentence.\n",
    "Another DOI: 10.5678/another.doi. And a third: https://doi.org/10.9876/third.doi\n",
    "\n",
    "[24] This is a new sentence starting here with a bullet point.\n",
    "\"\"\"\n",
    "# print(\"\\nOriginal PDF Text 2:\")\n",
    "# print(test_pdf_text_2)\n",
    "cleaned_text_2 = clean_doi_text(test_pdf_text_2)\n",
    "print(\"\\nText after cleaning DOIs 2:\")\n",
    "print(cleaned_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "2e50e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Information Extraction (IE) - Dataset Identification ---\n",
    "\n",
    "# Regex patterns for common dataset identifiers\n",
    "# DOI_PATTERN = r'10\\.\\d{4,5}/[-._;()/:A-Za-z0-9\\u002D\\u2010\\u2011\\u2012\\u2013\\u2014\\u2015]+'\tDOI_PATTERN\n",
    "# DOI_PATTERN = r'10\\.\\s?\\d{4,5}\\/[-._()<>;\\/:A-Za-z0-9]+\\s?(?:(?![A-Z]+)(?!\\d{1,3}\\.))+[-._()<>;\\/:A-Za-z0-9]+'\n",
    "DOI_PATTERN = r'\\bhttps://doi.org/10\\.\\d{4,5}\\/[-._\\/:A-Za-z0-9]+'\n",
    "EPI_PATTERN = r'\\bEPI[-_A-Z0-9]{2,}'\n",
    "SAM_PATTERN = r'\\bSAMN[0-9]{2,}'          # SAMN07159041\n",
    "IPR_PATTERN = r'\\bIPR[0-9]{2,}'\n",
    "CHE_PATTERN = r'\\bCHEMBL[0-9]{2,}'\n",
    "PRJ_PATTERN = r'\\bPRJ[A-Z0-9]{2,}'\n",
    "E_G_PATTERN = r'\\bE-[A-Z]{4}-[0-9]{2,}'   # E-GEOD-19722 or E-PROT-100\n",
    "ENS_PATTERN = r'\\bENS[A-Z]{4}[0-9]{2,}'\n",
    "CVC_PATTERN = r'\\bCVCL_[A-Z0-9]{2,}'\n",
    "EMP_PATTERN = r'\\bEMPIAR-[0-9]{2,}'\n",
    "PXD_PATTERN = r'\\bPXD[0-9]{2,}'\n",
    "HPA_PATTERN = r'\\bHPA[0-9]{2,}'\n",
    "SRR_PATTERN = r'\\bSRR[0-9]{2,}'\n",
    "GSE_PATTERN = r'\\b(GSE|GSM|GDS|GPL)\\d{4,6}\\b' # Example for GEO accession numbers (e.g., GSE12345, GSM12345)\n",
    "GNB_PATTERN = r'\\b[A-Z]{1,2}\\d{5,6}\\b' # GenBank accession numbers (e.g., AB123456, AF000001)\n",
    "CAB_PATTERN = r'\\bCAB[0-9]{2,}'\n",
    "\n",
    "# Combine all patterns into a list\n",
    "DATASET_ID_PATTERNS = [\n",
    "    DOI_PATTERN,\n",
    "    EPI_PATTERN,\n",
    "    SAM_PATTERN,\n",
    "    IPR_PATTERN,\n",
    "    CHE_PATTERN,\n",
    "    PRJ_PATTERN,\n",
    "    E_G_PATTERN,\n",
    "    ENS_PATTERN,\n",
    "    CVC_PATTERN,\n",
    "    EMP_PATTERN,\n",
    "    PXD_PATTERN,\n",
    "    HPA_PATTERN,\n",
    "    SRR_PATTERN,\n",
    "    GSE_PATTERN,\n",
    "    GNB_PATTERN,\n",
    "    CAB_PATTERN,\n",
    "]\n",
    "\n",
    "# Compile all patterns for efficiency\n",
    "COMPILED_DATASET_ID_REGEXES = [re.compile(p) for p in DATASET_ID_PATTERNS]\n",
    "\n",
    "# Data related keywords to look for in the text\n",
    "# These keywords help to ensure that the text is relevant to datasets\n",
    "DATA_RELATED_KEYWORDS = ['data release', 'download', 'program data', 'data availability', 'the data', 'dataset', 'database', 'repository', 'data source', 'data access', 'data archive']\n",
    "\n",
    "def text_has_dataset_id(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the given text contains any dataset identifier.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to check for dataset identifiers.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if any dataset identifier is found, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    occurrences_with_context: list[str] = []\n",
    "    for regex in COMPILED_DATASET_ID_REGEXES:\n",
    "        if regex.search(text):\n",
    "            text_lower = text.lower()\n",
    "            # Check for specific keywords in the text\n",
    "            if any(keyword in text_lower for keyword in DATA_RELATED_KEYWORDS):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def extract_dataset_ids(text: str, context_chars: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Extract dataset identifiers with context from the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to search for dataset identifiers.\n",
    "        context_chars (int): Number of characters to include before and after the match for context.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: A list of extracted dataset identifiers with context.\n",
    "    \"\"\"\n",
    "    # start_sep = \"<dataset_id_with_context>\"\n",
    "    # end_sep = \"</dataset_id_with_context>\"\n",
    "    # mid_sep = end_sep+start_sep\n",
    "    is_small_context = len(text) < context_chars * 2\n",
    "    dataset_ids: list[str] = []\n",
    "    occurrences_with_context: list[str] = []\n",
    "    text_lower = text.lower()\n",
    "    if any(keyword in text_lower for keyword in DATA_RELATED_KEYWORDS):\n",
    "        for regex in COMPILED_DATASET_ID_REGEXES:\n",
    "            matches = regex.finditer(text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                dataset_id = text[match.start() : match.end()]\n",
    "                if is_small_context:\n",
    "                    dataset_ids.append(dataset_id)\n",
    "                else:\n",
    "                    extracted_snippet = text[max(0, match.start() - context_chars): match.end() + context_chars ]\n",
    "                    extracted_snippet_lower = extracted_snippet.lower()\n",
    "                    if any(keyword in extracted_snippet_lower for keyword in DATA_RELATED_KEYWORDS):\n",
    "                        occurrences_with_context.append(\"{\" + f'\"dataset_ids\": {[dataset_id]}, context: \"{extracted_snippet}\"' + \"}\")\n",
    "        if dataset_ids:\n",
    "            occurrences_with_context.append(\"{\" + f'\"dataset_ids\": {dataset_ids}, context: \"{text}\"' + \"}\")\n",
    "    \n",
    "    # If no occurrences found, return an empty string\n",
    "    # Otherwise, join the occurrences with a specific separator\n",
    "    if not occurrences_with_context:\n",
    "        return \"\"\n",
    "    return \",\".join(occurrences_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29351aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 12.8/12.8 MB 74.1 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run this line once to download the model\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "bec3b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load a spaCy model (e.g., 'en_core_web_sm')\n",
    "# python -m spacy download en_core_web_sm \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_sentences_from_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    doc_spacy = nlp(text)\n",
    "    return \"\\n\".join([sent.text.replace('-\\n', '-').replace('_\\n', '_').replace('/\\n', '/').replace('\\n', ' ') for sent in doc_spacy.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a58f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading ---\n",
    "def load_file_paths(dataset_type_dir: str) -> pd.DataFrame: \n",
    "    pdf_path = os.path.join(dataset_type_dir, 'PDF')\n",
    "    xml_path = os.path.join(dataset_type_dir, 'XML')\n",
    "    dataset_type = os.path.basename(dataset_type_dir)\n",
    "    pdf_files = [f for f in os.listdir(pdf_path) if f.endswith('.pdf')]\n",
    "    xml_files = [f for f in os.listdir(xml_path) if f.endswith('.xml')]\n",
    "    df_pdf = pd.DataFrame({\n",
    "        'article_id': [f.replace('.pdf', '') for f in pdf_files],\n",
    "        'pdf_file_path': [os.path.join(pdf_path, f) for f in pdf_files]\n",
    "    })\n",
    "    df_xml = pd.DataFrame({\n",
    "        'article_id': [f.replace('.xml', '') for f in xml_files],\n",
    "        'xml_file_path': [os.path.join(xml_path, f) for f in xml_files]\n",
    "    })\n",
    "    merge_df = pd.merge(df_pdf, df_xml, on='article_id', how='outer', suffixes=('_pdf', '_xml'), validate=\"one_to_many\")\n",
    "    merge_df['dataset_type'] = dataset_type\n",
    "    return merge_df\n",
    "\n",
    "def read_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts all text from a PDF file using PyMuPDF.\"\"\"\n",
    "    text = \"\"\n",
    "    p1 = None\n",
    "    abstrct_block_found = False\n",
    "    num_blocks = 0\n",
    "    if not fitz:\n",
    "        return text  # Return empty string if fitz is not available\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                # Extract text from the page\n",
    "                textpage = page.get_textpage()\n",
    "                if page.number == 0:\n",
    "                    p1 = get_sentences_from_text(textpage.extractTEXT())\n",
    "                    p1 = p1[:int(len(p1)/2)] + \".\\npotential_dataset_ids: [\"\n",
    "\n",
    "                # Extract text from all blocks that have dataset id's\n",
    "                blocks = textpage.extractBLOCKS()\n",
    "                for block in blocks:\n",
    "                    block_text = get_sentences_from_text(block[4])\n",
    "                    if page.number == 0 and len(block_text) > 100 and \"abstract\" in block_text.lower():\n",
    "                        abstrct_block_found = True\n",
    "                        text += block_text + \".\\npotential_dataset_ids: [\"\n",
    "                    else:\n",
    "                        # if \"essd-9-861-2017\" in block[4]:\n",
    "                        #     print(f\"Found essd-9-861-2017 in block: {len(block[4])}:{block[4]}\")\n",
    "                        # Each block is a tuple, where the 4th element is the text\n",
    "                        # Clean the DOI text in the block\n",
    "                        #block_text = clean_doi_text(block[4].replace('\\u200b', '').replace('/\\n', '/').strip())\n",
    "                        #block_text = clean_doi_text(get_sentences_from_text(block[4]))\n",
    "                        #block_text = get_sentences_from_text(block[4])\n",
    "                        context_chars = min(250, len(block_text))  # Use a minimum\n",
    "                        dataset_ids_found = extract_dataset_ids(block_text, context_chars)  # Extract dataset IDs from the block text\n",
    "                        if dataset_ids_found:\n",
    "                            # Append the dataset IDs found in the block to the text\n",
    "                            num_blocks += 1\n",
    "                            #print(f\"Found dataset IDs in block: {len(dataset_ids_found)}:{dataset_ids_found}\")\n",
    "                            text += dataset_ids_found + \",\"  # Ensure the block text is processed for dataset IDs\n",
    "                    \n",
    "                    # # Check if the block has text and contains a regex match for a dataset_id\n",
    "                    # if block_text and text_has_dataset_id(block_text):\n",
    "                    #     num_blocks += 1\n",
    "                    #     print(f\"Found dataset ID in block: {len(block_text)}:{block_text}\")\n",
    "                    #     # Append the text from the block, removing zero-width spaces and stripping whitespace\n",
    "                    #     text += block_text.strip() + \" \"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF {pdf_path}: {e}\")\n",
    "\n",
    "    if p1 and not abstrct_block_found:\n",
    "        text = p1 + text\n",
    "\n",
    "    print(f\"Total blocks with data related dataset IDs found: {num_blocks}\")\n",
    "    print(f\"Extracted text from {pdf_path}. Length: {len(text)} characters\")\n",
    "    return text + \"]\"\n",
    "\n",
    "def read_xml_text(xml_file_path: str) -> str:\n",
    "    \"\"\"Reads and concatenates all text content from an XML file.\"\"\"\n",
    "    all_text_parts = []\n",
    "    try:\n",
    "        tree = ET.parse(xml_file_path)\n",
    "        root = tree.getroot()\n",
    "        for element in root.iter():\n",
    "            if element.text:\n",
    "                cleaned_text = element.text.strip()\n",
    "                if cleaned_text:\n",
    "                    all_text_parts.append(cleaned_text)\n",
    "            if element.tail:\n",
    "                cleaned_tail = element.tail.strip()\n",
    "                if cleaned_tail:\n",
    "                    all_text_parts.append(cleaned_tail)\n",
    "        return \" \".join(all_text_parts) if all_text_parts else \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading XML {xml_file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_article_text(filepath: str) -> str:\n",
    "    \"\"\"\n",
    "    Loads text content from a single article file (PDF or XML).\n",
    "    Returns the text content of the given file.\n",
    "    \"\"\"\n",
    "    text_content = \"\"\n",
    "\n",
    "    if filepath.endswith(\".pdf\"):\n",
    "        text_content = read_pdf_text(filepath)\n",
    "    elif filepath.endswith(\".xml\"):\n",
    "        text_content = read_xml_text(filepath)\n",
    "\n",
    "    return text_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "5f45bff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks with data related dataset IDs found: 1\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1002_esp.5058.pdf. Length: 4011 characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'R E S E A R C H A R T I C L E Past, present and future of a meandering river in the Bolivian Amazon basin Kattia Rubi Arnez Ferrel1 | Jonathan Mark Nelson2 | Yasuyuki Shimizu1 | Tomoko Kyuka1 1Graduate School of Engineering, Hokkaido University, Sapporo, Japan 2U.S. Geological Survey, Golden, Colorado, USA Correspondence Kattia Rubi Arnez Ferrel, Graduate School of Engineering, Hokkaido University, Sapporo 060-0808, Japan. \\nEmail: rubikraf@gmail.com Funding information Nitobe School Project Abstract Field observations on small rivers of the Amazon basin are less common due to their remote location and difficult accessibility.\\nHere we show, through remote sensing analysis and field works, the planform evolution and riverbed topography of a small river located in the upper foreland Amazon basin, the Ichilo River.\\nBy tracking plan-form changes over 30 years, we identified the factors that control meander migration rates in the Ichilo River: cutoffs, climate and human interventions.\\nThe data suggest that neck cutoffs are the main controls in the Ichilo River, with an annual density of 0.022 cutoffs/km.\\nIn addition, climate controls have been identified in the form of high-precipitation events that may have promoted cutoffs, an increase in meander migration rate and channel widening.\\nThe width distribution of the Ichilo River is well represented by general extreme value and inverse Gaussian distributions.\\nThe spatio-temporal variability of meandering migration rates in the Ichilo River is analysed in two locations where neck cutoffs are expected.\\nAnalysing the distance across the neck in these two points, we predict the occurrence of a new cutoff.\\nThe combined methodology of bathymetric surveys and structure from motion photogrammetry shows us the Ichilo riverbed topography and banks at high resolution, where two scour holes were identified.\\nFinally, we discuss the impact of planform changes of the Ichilo River on communities that are established along its riverbanks. \\nK E Y W O R D S bathymetric surveys, Bolivian Amazon basin, Ichilo River, meanders, remote sensing, UAV 1 | INTRODUCTION The Amazon basin exhibits a constantly changing landscape m.\\n potential_dataset_ids: [{\"dataset_ids\": [\\'https://doi.org/10.1002/esp.4637\\'], context: \" Environ. \\nJames, M.R., Chandler, J.H., Eltner, A., Fraser, C., Miller, P.E., Mills, J.P. et al. (2019) Guidelines on the use of structure-from-motion photo-grammetry in geomorphic research.\\nEarth Surface Processes and Land-forms, 44(10), 2081–2084.\\nhttps://doi.org/10.1002/esp.4637 \\n[Dataset] Kinzel, P., Nelson, J.M. & Kattia, R.A.F. (2019)\\nBathymetric survey of the Ichilo and Sajta Rivers, near Puerto Villarroel, Bolivia, May 23–24, 2019.\\nU.S. Geological Survey data release.\\nhttps://doi.org/10.5066/P9FW6E8K Lanzoni, S. & Semi\"},{\"dataset_ids\": [\\'https://doi.org/10.5066/P9FW6E8K\\'], context: \"44(10), 2081–2084.\\nhttps://doi.org/10.1002/esp.4637 \\n[Dataset] Kinzel, P., Nelson, J.M. & Kattia, R.A.F. (2019)\\nBathymetric survey of the Ichilo and Sajta Rivers, near Puerto Villarroel, Bolivia, May 23–24, 2019.\\nU.S. Geological Survey data release.\\nhttps://doi.org/10.5066/P9FW6E8K Lanzoni, S. & Seminara, G. (2006)\\nOn the nature of meander instability. Journal of Geophysical Research - Earth Surface, 111(F4), 1–14. \\nhttps://doi.org/10.1029/2005JF000416 Li, J., Grenfell, M.C., Wei, H., Tooth, S. & Ngiem, S. (2020) Chute cutoff-\"},{\"dataset_ids\": [\\'https://doi.org/10.1029/2005JF000416\\'], context: \"erto Villarroel, Bolivia, May 23–24, 2019.\\nU.S. Geological Survey data release.\\nhttps://doi.org/10.5066/P9FW6E8K Lanzoni, S. & Seminara, G. (2006)\\nOn the nature of meander instability. Journal of Geophysical Research - Earth Surface, 111(F4), 1–14. \\nhttps://doi.org/10.1029/2005JF000416 Li, J., Grenfell, M.C., Wei, H., Tooth, S. & Ngiem, S. (2020) Chute cutoff-driven abandonment and sedimentation of meander bends along a fine-grained, non-vegetated, ephemeral river on the Bolivian Alti-plano. \\nGeomorphology, 350, 106917. \\nhttps://d\"},]'"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test loading a PDF file\n",
    "#pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1002_2017jc013030.pdf')\n",
    "#pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1017_rdc.2022.19.pdf')\n",
    "#pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1017_s0007123423000601.pdf')\n",
    "#pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.3389_fcimb.2024.1292467.pdf')\n",
    "pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1002_esp.5058.pdf') # This one is big\n",
    "#pdf_file_path = os.path.join(ARTICLE_TRAIN_DIR, 'PDF', '10.1002_esp.5059.pdf') # This one is big\n",
    "pdf_text = read_pdf_text(pdf_file_path)\n",
    "pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "0e8ad257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RESEARCH ARTICLE\\nhttps://doi.org/10.1002/2017JC013030\\nAssessing the Variability in the Relationship Between the\\nParticulate Backscattering Coefficient and the Chlorophyll a\\nConcentration From a Global Biogeochemical-Argo Database\\nMarie Barbieux1\\n, Julia Uitz1, Annick Bricaud1, Emanuele Organelli1,2\\n, Antoine Poteau1\\n,\\nCatherine Schmechtig3\\n, Bernard Gentili1, Grigor Obolensky4, Edouard Leymarie1\\n,\\nChristophe Penkerc’h1, Fabrizio D’Ortenzio1\\n, and Herv�e Claustre1\\n1Sorbonne Universit�es, UPMC Univ Paris 06, CNRS, Observatoire Oc�eanologique de Villefranche, Laboratoire\\nd’Oc�eanographie de Villefranche, Villefranche-sur-Mer, France, 2Plymouth Marine Laboratory, Prospect Place, The Hoe,\\nPlymouth, United Kingdom, 3OSU Ecce Terra, UMS 3455, CNRS and Universit�e Pierre et Marie Curie, Paris 6, Paris, France,\\n4ERIC Euro-Argo, 29280 Plouzan�e, France\\nAbstract Characterizing phytoplankton distribution and dynamics in the world’s open oceans requires in\\nsitu observations over a broad range of space and time scales. In addition to temperature/salinity measure-\\nments, Biogeochemical-Argo (BGC-Argo) profiling floats are capable of autonomously observing at high-\\nfrequency bio-optical properties such as the chlorophyll fluorescence, a proxy of the chlorophyll a concen-\\ntration (Chla), the particulate backscattering coefficient (bbp), a proxy of the stock of particulate organic car-\\nbon, and the light available for photosynthesis. We analyzed an unprecedented BGC-Argo database of more\\nthan 8,500 multivariable profiles collected in various oceanic conditions, from subpolar waters to subtropical\\ngyres. Our objective is to refine previously established Chla versus bbp relationships and gain insights into\\nthe sources of vertical, seasonal, and regional variability in this relationship. Despite some regional, seasonal\\nand vertical variations, a general covariation occurs at a global sca <dataset_id_with_context>rom a\\nglobal Biogeochemical-Argo database.\\nJournal of Geophysical Research:\\nOceans, 123, 1229–1250. https://doi.org/10.1002/2017JC013030</dataset_id_with_context>  <dataset_id_with_context>ntribute to\\nmake the data freely and publicly\\navailable. Data referring to Organelli\\net al. (2016a; https://doi.org/10.17882/47142)andBarbieuxetal.(2017;https://doi.org/10.17882/49388)arefreelyavailableonSEANOE.</dataset_id_with_context>  '"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "29d7f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labeled training data from: ./kaggle/input/make-data-count-finding-data-references\\train_labels.csv\n",
      "Training labels shape: (1028, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a0f79aca-eae4-4920-964a-c17318836aac",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "https://doi.org/10.17882/49388",
         "Primary"
        ],
        [
         "1",
         "10.1002_anie.201916483",
         "Missing",
         "Missing"
        ],
        [
         "2",
         "10.1002_anie.202005531",
         "Missing",
         "Missing"
        ],
        [
         "3",
         "10.1002_anie.202007717",
         "Missing",
         "Missing"
        ],
        [
         "4",
         "10.1002_chem.201902131",
         "Missing",
         "Missing"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/49388</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1002_anie.202007717</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1002_chem.201902131</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id                      dataset_id     type\n",
       "0    10.1002_2017jc013030  https://doi.org/10.17882/49388  Primary\n",
       "1  10.1002_anie.201916483                         Missing  Missing\n",
       "2  10.1002_anie.202005531                         Missing  Missing\n",
       "3  10.1002_anie.202007717                         Missing  Missing\n",
       "4  10.1002_chem.201902131                         Missing  Missing"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the labeled training data CSV file\n",
    "print(f\"Loading labeled training data from: {LABELED_TRAINING_DATA_CSV_PATH}\")\n",
    "train_labels_df = pd.read_csv(LABELED_TRAINING_DATA_CSV_PATH)\n",
    "\n",
    "print(f\"Training labels shape: {train_labels_df.shape}\")\n",
    "display(train_labels_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c538010a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped dataset ID counts:\n",
      "   dataset_id_trim  article_id\n",
      "52             htt         325\n",
      "29             Mis         309\n",
      "20             EPI          64\n",
      "47             SAM          41\n",
      "25             IPR          33\n",
      "11             CHE          29\n",
      "41             PRJ          26\n",
      "16             E-G          25\n",
      "19             ENS          21\n",
      "26             K02          20\n"
     ]
    }
   ],
   "source": [
    "# Create a new column 'dataset_id' by extracting the first 3 characters of the 'dataset_id' column\n",
    "train_labels_df['dataset_id_trim'] = train_labels_df['dataset_id'].str[:3]\n",
    "# Find the most frequent types of dataset_id's\n",
    "freq_dataset_id_df = train_labels_df.groupby('dataset_id_trim').count().reset_index()\n",
    "freq_dataset_id_df = freq_dataset_id_df[['dataset_id_trim', 'article_id']].sort_values(by='article_id', ascending=False)\n",
    "print(f\"Grouped dataset ID counts:\\n{freq_dataset_id_df.head(10)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2c96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_id_trim",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "be175d7b-7e40-480e-87b8-022e551c0d97",
       "rows": [
        [
         "372",
         "10.1128_JVI.01717-21",
         "EPI_ISL_291131",
         "Primary",
         "EPI"
        ],
        [
         "373",
         "10.1128_JVI.01717-21",
         "EPI_ISL_293286",
         "Primary",
         "EPI"
        ],
        [
         "374",
         "10.1128_JVI.01717-21",
         "EPI_ISL_293287",
         "Primary",
         "EPI"
        ],
        [
         "375",
         "10.1128_JVI.01717-21",
         "EPI_ISL_293288",
         "Primary",
         "EPI"
        ],
        [
         "376",
         "10.1128_JVI.01717-21",
         "EPI_ISL_293289",
         "Primary",
         "EPI"
        ],
        [
         "377",
         "10.1128_JVI.01717-21",
         "EPI_ISL_293290",
         "Primary",
         "EPI"
        ],
        [
         "378",
         "10.1128_JVI.01717-21",
         "EPI_ISL_293291",
         "Primary",
         "EPI"
        ],
        [
         "379",
         "10.1128_JVI.01717-21",
         "EPI_ISL_332358",
         "Primary",
         "EPI"
        ],
        [
         "380",
         "10.1128_JVI.01717-21",
         "EPI_ISL_332395",
         "Primary",
         "EPI"
        ],
        [
         "381",
         "10.1128_JVI.01717-21",
         "EPI_ISL_332396",
         "Primary",
         "EPI"
        ],
        [
         "382",
         "10.1128_JVI.01717-21",
         "EPI_ISL_332399",
         "Primary",
         "EPI"
        ],
        [
         "383",
         "10.1128_JVI.01717-21",
         "EPI_ISL_332401",
         "Primary",
         "EPI"
        ],
        [
         "384",
         "10.1128_JVI.01717-21",
         "EPI_ISL_376123",
         "Primary",
         "EPI"
        ],
        [
         "385",
         "10.1128_JVI.01717-21",
         "EPI_ISL_445001",
         "Primary",
         "EPI"
        ],
        [
         "386",
         "10.1128_JVI.01717-21",
         "EPI_ISL_445002",
         "Primary",
         "EPI"
        ],
        [
         "387",
         "10.1128_JVI.01717-21",
         "EPI_ISL_445051",
         "Primary",
         "EPI"
        ],
        [
         "388",
         "10.1128_JVI.01717-21",
         "EPI_ISL_445052",
         "Primary",
         "EPI"
        ],
        [
         "389",
         "10.1128_JVI.01717-21",
         "EPI_ISL_445089",
         "Primary",
         "EPI"
        ],
        [
         "390",
         "10.1128_JVI.01717-21",
         "EPI_ISL_445090",
         "Primary",
         "EPI"
        ],
        [
         "391",
         "10.1128_JVI.01717-21",
         "EPI_ISL_445091",
         "Primary",
         "EPI"
        ],
        [
         "392",
         "10.1128_JVI.01717-21",
         "EPI_ISL_445092",
         "Primary",
         "EPI"
        ],
        [
         "393",
         "10.1128_JVI.01717-21",
         "EPI_ISL_445093",
         "Primary",
         "EPI"
        ],
        [
         "830",
         "10.3389_fcimb.2024.1292467",
         "EPI_ISL_10271777",
         "Secondary",
         "EPI"
        ],
        [
         "831",
         "10.3389_fcimb.2024.1292467",
         "EPI_ISL_13295050",
         "Secondary",
         "EPI"
        ],
        [
         "832",
         "10.3389_fcimb.2024.1292467",
         "EPI_ISL_1626969",
         "Secondary",
         "EPI"
        ],
        [
         "833",
         "10.3389_fcimb.2024.1292467",
         "EPI_ISL_17371329",
         "Secondary",
         "EPI"
        ],
        [
         "834",
         "10.3389_fcimb.2024.1292467",
         "EPI_ISL_2340570",
         "Secondary",
         "EPI"
        ],
        [
         "835",
         "10.3389_fcimb.2024.1292467",
         "EPI_ISL_2340575",
         "Secondary",
         "EPI"
        ],
        [
         "836",
         "10.3389_fcimb.2024.1292467",
         "EPI_ISL_2340581",
         "Secondary",
         "EPI"
        ],
        [
         "837",
         "10.3389_fcimb.2024.1292467",
         "EPI_ISL_2340614",
         "Secondary",
         "EPI"
        ],
        [
         "838",
         "10.3389_fcimb.2024.1292467",
         "EPI_ISL_2340615",
         "Secondary",
         "EPI"
        ],
        [
         "839",
         "10.3389_fcimb.2024.1292467",
         "EPI_ISL_2340626",
         "Secondary",
         "EPI"
        ],
        [
         "840",
         "10.3389_fcimb.2024.1292467",
         "EPI_ISL_2340662",
         "Secondary",
         "EPI"
        ],
        [
         "841",
         "10.3389_fcimb.2024.1292467",
         "EPI_ISL_2365356",
         "Secondary",
         "EPI"
        ],
        [
         "842",
         "10.3389_fcimb.2024.1292467",
         "EPI_ISL_2402571",
         "Secondary",
         "EPI"
        ],
        [
         "900",
         "10.3390_v11060565",
         "EPI1018205",
         "Secondary",
         "EPI"
        ],
        [
         "901",
         "10.3390_v11060565",
         "EPI1018206",
         "Secondary",
         "EPI"
        ],
        [
         "902",
         "10.3390_v11060565",
         "EPI1018207",
         "Secondary",
         "EPI"
        ],
        [
         "903",
         "10.3390_v11060565",
         "EPI1018208",
         "Secondary",
         "EPI"
        ],
        [
         "904",
         "10.3390_v11060565",
         "EPI1018210",
         "Secondary",
         "EPI"
        ],
        [
         "905",
         "10.3390_v11060565",
         "EPI1018211",
         "Secondary",
         "EPI"
        ],
        [
         "906",
         "10.3390_v11060565",
         "EPI1019551",
         "Secondary",
         "EPI"
        ],
        [
         "907",
         "10.3390_v11060565",
         "EPI1019552",
         "Secondary",
         "EPI"
        ],
        [
         "908",
         "10.3390_v11060565",
         "EPI1019553",
         "Secondary",
         "EPI"
        ],
        [
         "909",
         "10.3390_v11060565",
         "EPI1019554",
         "Secondary",
         "EPI"
        ],
        [
         "910",
         "10.3390_v11060565",
         "EPI1019555",
         "Secondary",
         "EPI"
        ],
        [
         "911",
         "10.3390_v11060565",
         "EPI1019556",
         "Secondary",
         "EPI"
        ],
        [
         "912",
         "10.3390_v11060565",
         "EPI1019558",
         "Secondary",
         "EPI"
        ],
        [
         "913",
         "10.3390_v11060565",
         "EPI1104284",
         "Secondary",
         "EPI"
        ],
        [
         "914",
         "10.3390_v11060565",
         "EPI1104285",
         "Secondary",
         "EPI"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 64
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>type</th>\n",
       "      <th>dataset_id_trim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>10.1128_JVI.01717-21</td>\n",
       "      <td>EPI_ISL_291131</td>\n",
       "      <td>Primary</td>\n",
       "      <td>EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>10.1128_JVI.01717-21</td>\n",
       "      <td>EPI_ISL_293286</td>\n",
       "      <td>Primary</td>\n",
       "      <td>EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>10.1128_JVI.01717-21</td>\n",
       "      <td>EPI_ISL_293287</td>\n",
       "      <td>Primary</td>\n",
       "      <td>EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>10.1128_JVI.01717-21</td>\n",
       "      <td>EPI_ISL_293288</td>\n",
       "      <td>Primary</td>\n",
       "      <td>EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>10.1128_JVI.01717-21</td>\n",
       "      <td>EPI_ISL_293289</td>\n",
       "      <td>Primary</td>\n",
       "      <td>EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>10.3390_v11060565</td>\n",
       "      <td>EPI954554</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>10.3390_v11060565</td>\n",
       "      <td>EPI954555</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>10.3390_v11060565</td>\n",
       "      <td>EPI954556</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>10.3390_v11060565</td>\n",
       "      <td>EPI954557</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>EPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>10.3390_v11060565</td>\n",
       "      <td>EPI954559</td>\n",
       "      <td>Secondary</td>\n",
       "      <td>EPI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               article_id      dataset_id       type dataset_id_trim\n",
       "372  10.1128_JVI.01717-21  EPI_ISL_291131    Primary             EPI\n",
       "373  10.1128_JVI.01717-21  EPI_ISL_293286    Primary             EPI\n",
       "374  10.1128_JVI.01717-21  EPI_ISL_293287    Primary             EPI\n",
       "375  10.1128_JVI.01717-21  EPI_ISL_293288    Primary             EPI\n",
       "376  10.1128_JVI.01717-21  EPI_ISL_293289    Primary             EPI\n",
       "..                    ...             ...        ...             ...\n",
       "924     10.3390_v11060565       EPI954554  Secondary             EPI\n",
       "925     10.3390_v11060565       EPI954555  Secondary             EPI\n",
       "926     10.3390_v11060565       EPI954556  Secondary             EPI\n",
       "927     10.3390_v11060565       EPI954557  Secondary             EPI\n",
       "928     10.3390_v11060565       EPI954559  Secondary             EPI\n",
       "\n",
       "[64 rows x 4 columns]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_df[train_labels_df['dataset_id_trim'] == 'EPI'].sample(3)  # Display the first 10 rows where dataset_id_trim is 'htt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "560a47e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example grouped training data for article_id '10.1002_2017jc013030': [{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]\n"
     ]
    }
   ],
   "source": [
    "# Group training data by article_id to get all datasets for each article\n",
    "# This creates a dictionary where keys are article_ids and values are lists of dataset dicts\n",
    "grouped_training_data = {}\n",
    "for article_id, group_df in train_labels_df.groupby('article_id'):\n",
    "    grouped_training_data[article_id] = group_df[['dataset_id', 'type']].to_dict('records')\n",
    "\n",
    "# Example usage of grouped_training_data\n",
    "print(f\"Example grouped training data for article_id '10.1002_2017jc013030': {grouped_training_data['10.1002_2017jc013030']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "34f61a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train files paths shape: (494, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_info",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "fb8a983d-79cb-4664-8d7a-a98a89da05f2",
       "rows": [
        [
         "218",
         "10.1186_s11658-018-0120-2",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1186_s11658-018-0120-2.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\train\\XML\\10.1186_s11658-018-0120-2.xml",
         "train",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]"
        ],
        [
         "61",
         "10.1029_2020jf005675",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1029_2020jf005675.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\train\\XML\\10.1029_2020jf005675.xml",
         "train",
         "[{'dataset_id': 'https://doi.org/10.5066/p9bu8faq', 'type': 'Primary'}]"
        ],
        [
         "404",
         "10.1590_0034-737x201966050002",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1590_0034-737x201966050002.pdf",
         null,
         "train",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>dataset_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>10.1186_s11658-018-0120-2</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>train</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>10.1029_2020jf005675</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>train</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5066/p9bu8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>10.1590_0034-737x201966050002</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        article_id  \\\n",
       "218      10.1186_s11658-018-0120-2   \n",
       "61            10.1029_2020jf005675   \n",
       "404  10.1590_0034-737x201966050002   \n",
       "\n",
       "                                         pdf_file_path  \\\n",
       "218  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "61   ./kaggle/input/make-data-count-finding-data-re...   \n",
       "404  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                         xml_file_path dataset_type  \\\n",
       "218  ./kaggle/input/make-data-count-finding-data-re...        train   \n",
       "61   ./kaggle/input/make-data-count-finding-data-re...        train   \n",
       "404                                                NaN        train   \n",
       "\n",
       "                                          dataset_info  \n",
       "218     [{'dataset_id': 'Missing', 'type': 'Missing'}]  \n",
       "61   [{'dataset_id': 'https://doi.org/10.5066/p9bu8...  \n",
       "404     [{'dataset_id': 'Missing', 'type': 'Missing'}]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test files paths shape: (30, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_info",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "bfb9674f-2f5c-447d-92c6-dba554de88a0",
       "rows": [
        [
         "29",
         "10.1007_jhep07(2018)134",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1007_jhep07(2018)134.pdf",
         null,
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]"
        ],
        [
         "0",
         "10.1002_2017jc013030",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_2017jc013030.xml",
         "test",
         "[{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]"
        ],
        [
         "7",
         "10.1002_chem.202001412",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202001412.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_chem.202001412.xml",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>dataset_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.1007_jhep07(2018)134</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.17882/4938...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.1002_chem.202001412</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 article_id  \\\n",
       "29  10.1007_jhep07(2018)134   \n",
       "0      10.1002_2017jc013030   \n",
       "7    10.1002_chem.202001412   \n",
       "\n",
       "                                        pdf_file_path  \\\n",
       "29  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "0   ./kaggle/input/make-data-count-finding-data-re...   \n",
       "7   ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                        xml_file_path dataset_type  \\\n",
       "29                                                NaN         test   \n",
       "0   ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "7   ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "\n",
       "                                         dataset_info  \n",
       "29     [{'dataset_id': 'Missing', 'type': 'Missing'}]  \n",
       "0   [{'dataset_id': 'https://doi.org/10.17882/4938...  \n",
       "7      [{'dataset_id': 'Missing', 'type': 'Missing'}]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load file paths for training and testing datasets\n",
    "train_file_paths_df = load_file_paths(ARTICLE_TRAIN_DIR)\n",
    "test_file_paths_df = load_file_paths(ARTICLE_TEST_DIR)\n",
    "\n",
    "# Remove rows in train_file_paths_df that have a corresponding article_id in test_file_paths_df\n",
    "train_file_paths_df = train_file_paths_df[~train_file_paths_df['article_id'].isin(test_file_paths_df['article_id'])]\n",
    "\n",
    "# Merge the file paths with the grouped_training_data\n",
    "train_file_paths_df['dataset_info'] = train_file_paths_df['article_id'].map(grouped_training_data)\n",
    "test_file_paths_df['dataset_info'] = test_file_paths_df['article_id'].map(grouped_training_data)\n",
    "\n",
    "print(f\"Train files paths shape: {train_file_paths_df.shape}\")\n",
    "display(train_file_paths_df.sample(3))\n",
    "print(f\"Test files paths shape: {test_file_paths_df.shape}\")\n",
    "display(test_file_paths_df.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "2640c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QwenModelEval Class ---\n",
    "# kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "#max_new_tokens=32768\n",
    "class QwenModelEval:\n",
    "    def __init__(self, model_name, sys_prompt, enable_thinking=True, max_new_tokens=1024):\n",
    "        print(f\"Loading Qwen model and tokenizer from: {model_name}\")\n",
    "        self.model_name = model_name\n",
    "        self.sys_prompt = sys_prompt\n",
    "        self.enable_thinking = enable_thinking  # Enable or disable thinking mode\n",
    "        self.max_new_tokens = max_new_tokens  # Set the maximum number of new tokens to generate\n",
    "        # Load the tokenizer and model\n",
    "        # Using trust_remote_code=True to allow custom model code execution\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\", trust_remote_code=True)\n",
    "        self.model.eval() # Set the model to evaluation mode here.\n",
    "\n",
    "    def generate_response(self, user_input):  \n",
    "        inputs = self._get_inputs(user_input)\n",
    "        # Disable gradient calculation during inference\n",
    "        # Generate the response using the model\n",
    "        with torch.no_grad(): \n",
    "            generated_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.convert_tokens_to_ids(\"<|im_end|>\"),\n",
    "                # do_sample=False, # Use greedy decoding (fastest)\n",
    "                # num_beams=1,     # Do not use beam search (fastest)\n",
    "                # temperature=0.0, # Make output deterministic (if do_sample=False, this has no effect)                \n",
    "                temperature=0.6 if self.enable_thinking else 0.7,\n",
    "                top_p=0.95 if self.enable_thinking else 0.8,\n",
    "                top_k=20,\n",
    "                min_p=0\n",
    "            )\n",
    "        # Parse the response and thinking content\n",
    "        return self._parse_response(inputs, generated_ids)\n",
    "\n",
    "    def _get_inputs(self, user_input):\n",
    "        \"\"\"Prepare the input for the model based on user input.\"\"\"\n",
    "        # Trim the user input to a maximum length for better performance\n",
    "        user_input = user_input[:4096]  # Limit input length to 4096 characters\n",
    "        print(f\"Preparing input with length: {len(user_input)}\")\n",
    "        # Create the messages for the chat template\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        text = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=self.enable_thinking\n",
    "        )\n",
    "        return self.tokenizer(text, return_tensors=\"pt\").to(self.model.device)\n",
    "    \n",
    "    def _parse_response(self, inputs, generated_ids):\n",
    "        print(\"Parsing response from generated IDs...\")\n",
    "        # Extract the output IDs from the generated IDs\n",
    "        output_ids = generated_ids[0][len(inputs.input_ids[0]):].tolist()\n",
    "        try:\n",
    "            index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "\n",
    "        thinking_content = self.tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "        raw_response = self.tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "        response = self._parse_json(raw_response)\n",
    "        return response, thinking_content\n",
    "    \n",
    "    def _parse_json(self, raw_response: str) -> list[dict[str,str]]:\n",
    "        # Remove code block markers and leading/trailing whitespace\n",
    "        cleaned = raw_response.strip()\n",
    "        if cleaned.startswith(\"```json\"):\n",
    "            cleaned = cleaned[len(\"```json\"):].strip()\n",
    "        if cleaned.endswith(\"```\"):\n",
    "            cleaned = cleaned[:-3].strip()\n",
    "\n",
    "        # Now parse as JSON\n",
    "        try:\n",
    "            return json.loads(cleaned)\n",
    "        except json.JSONDecodeError as e:\n",
    "            return []        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "74e5a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the one-shot reasoning, task, and example prompt\n",
    "# This prompt is designed to guide the model through a structured reasoning process\n",
    "\n",
    "# reason_prompt =  '''\n",
    "# You are an advanced AI reasoning assistant tasked with delivering a comprehensive analysis of a specific problem or question.  Your goal is to outline your reasoning process in a structured and transparent manner, with each step reflecting a thorough examination of the issue at hand, culminating in a well-reasoned conclusion.\n",
    "\n",
    "# ### Key Instructions:\n",
    "# 1.  Conduct **at least 5 distinct reasoning steps**, each building on the previous one.\n",
    "# 2.  **Acknowledge the limitations** inherent to AI, specifically what you can accurately assess and what you may struggle with.\n",
    "# 3.  **Adopt multiple reasoning frameworks** to resolve the problem or derive conclusions, such as:\n",
    "# - **Deductive reasoning** (drawing specific conclusions from general principles)\n",
    "# - **Inductive reasoning** (deriving broader generalizations from specific observations)\n",
    "# - **Abductive reasoning** (choosing the best possible explanation for the given evidence)\n",
    "# - **Analogical reasoning** (solving problems through comparisons and analogies)\n",
    "# 4.  **Critically analyze your reasoning** to identify potential flaws, biases, or gaps in logic.\n",
    "# 5.  When reviewing, apply a **fundamentally different perspective or approach** to enhance your analysis.\n",
    "# 6.  **Employ at least 2 distinct reasoning methods** to derive or verify the accuracy of your conclusions.\n",
    "# 7.  **Incorporate relevant domain knowledge** and **best practices** where applicable, ensuring your reasoning aligns with established standards.\n",
    "# 8.  **Quantify certainty levels** for each step and your final conclusion, where applicable.\n",
    "# 9.  Consider potential **edge cases or exceptions** that could impact the outcome of your reasoning.\n",
    "# 10.  Provide **clear justifications** for dismissing alternative hypotheses or solutions that arise during your analysis.\n",
    "# '''\n",
    "\n",
    "reason_prompt =  '''\n",
    "You are an advanced AI research assistant that is skilled in identifying and classifying datasets used within academic research papers.\n",
    "Be as accurate as possible but don't over think it.\n",
    "'''\n",
    "\n",
    "task_prompt = '''\n",
    "You are given an article_id and the associated text of an academic research paper.\n",
    "Within the text of the paper, you are given an abstract and a list of potential_dataset_ids that contains dataset_ids and their associated context within the paper.\n",
    "You have 3 tasks:\n",
    "\n",
    "1. Your first task is to identify all citations of datasets used in the research for this article. An article may cite zero or many datasets.\n",
    "Datasets in an article can be cited within the context using various terms such as \"data release\", \"data availability\", \"dataset\", \"database\", \"repository\", \"data source\", \"data access\", \"data archive\".\n",
    "Each dataset has a unique, persistent identifier to represent it called a dataset_id. If you find more than one citation of the same dataset_id, only process the first one.\n",
    "\n",
    "There are 2 ways to identify a dataset_id:\n",
    "The first way to identify a dataset_id is via a Digital Object Identifier (DOI). DOIs are used for all papers and some datasets. We want to identify DOIs that are used as dataset_id's, not the DOI of the paper itself or any other papers.\n",
    "They take the following form: https://doi.org/[prefix]/[suffix]. The prefix always starts with \"10.\" and is followed by a 4 or 5 digit number. The suffix can contain letters, numbers, and special characters.\n",
    "Examples of DOI dataset_ids (just for reference):\n",
    "https://doi.org/10.12345/12345\n",
    "https://doi.org/10.1234/xxxx.1x1x-xx11\n",
    "\n",
    "The second way to identify a dataset_id is via an Accession ID. Accession ID's vary in form by individual data repository where the data live. \n",
    "Examples of Accession ID's (just for reference):\n",
    "\"EPI_ISL_10271777\" (EPI dataset)\n",
    "\"IPR000264\" (InterPro dataset)\n",
    "\"SAMN07159041\" (NCBI Sequence Read Archive dataset)\n",
    "\"CHEMBL1782574\" (ChEMBL dataset)\n",
    "\n",
    "\n",
    "2. Your second task is to classify the type of each dataset_id that you find as \"Primary\" or \"Secondary\" as it is used within the context of the paper.\n",
    "Primary - raw or processed data generated as part of this paper, specifically for this study\n",
    "Secondary - raw or processed data derived or reused from existing records or published data\n",
    "\n",
    "3. Your third task is to return your results in a JSON format.\n",
    "If an article does not refer to any dataset_id's, return a single JSON object with the following structure:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"dataset_id\": \"Missing\",\n",
    "        \"type\": \"Missing\"\n",
    "    }\n",
    "]\n",
    "If an article refers to one or more dataset_id's, you need to classify the type of each dataset as \"Primary\" or \"Secondary\" and\n",
    "return every dataset found in a JSON array of objects, where each object has the following structure:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"dataset_id\": dataset_id here,\n",
    "        \"type\": type here\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "'''\n",
    "\n",
    "SYS_PROMPT = reason_prompt + task_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a43a2780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_info",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "08c2451a-0774-4674-8339-871085f8d6c1",
       "rows": [
        [
         "52",
         "10.1017_rdc.2022.19",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1017_rdc.2022.19.pdf",
         null,
         "train",
         "[{'dataset_id': 'https://doi.org/10.11588/data/10100', 'type': 'Secondary'}]"
        ],
        [
         "53",
         "10.1017_s0007123423000601",
         "./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1017_s0007123423000601.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\train\\XML\\10.1017_s0007123423000601.xml",
         "train",
         "[{'dataset_id': 'https://doi.org/10.7910/dvn/fwckse', 'type': 'Primary'}]"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>dataset_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>10.1017_rdc.2022.19</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>train</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.11588/data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>10.1017_s0007123423000601</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>train</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.7910/dvn/f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   article_id  \\\n",
       "52        10.1017_rdc.2022.19   \n",
       "53  10.1017_s0007123423000601   \n",
       "\n",
       "                                        pdf_file_path  \\\n",
       "52  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "53  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                        xml_file_path dataset_type  \\\n",
       "52                                                NaN        train   \n",
       "53  ./kaggle/input/make-data-count-finding-data-re...        train   \n",
       "\n",
       "                                         dataset_info  \n",
       "52  [{'dataset_id': 'https://doi.org/10.11588/data...  \n",
       "53  [{'dataset_id': 'https://doi.org/10.7910/dvn/f...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_file_paths_df['text'] = train_file_paths_df['pdf_file_path'].apply(load_article_text)\n",
    "# test_file_paths_df['text'] = test_file_paths_df['pdf_file_path'].apply(load_article_text)\n",
    "\n",
    "#Create a new df from train_file_paths_df where the string representation of dataset_info does not contain 'Missing'.\n",
    "train_file_paths_df_2 = train_file_paths_df[~train_file_paths_df['dataset_info'].astype(str).str.contains('Missing')]\n",
    "train_file_paths_df_3 = train_file_paths_df_2[train_file_paths_df_2['article_id'].astype(str).str.contains('10.1017_')]\n",
    "train_file_paths_df_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "f8a8a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen model and tokenizer from: C:\\Users\\jim\\.cache\\kagglehub\\models\\qwen-lm\\qwen-3\\transformers\\0.6b\\1\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the QwenModelEval class with the model path and system prompt\n",
    "inference_model = QwenModelEval(QWEN_BASE_MODEL_PATH, sys_prompt=SYS_PROMPT, enable_thinking=True, max_new_tokens=1576)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "749ca3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_articles(file_paths_df: pd.DataFrame, model) -> pd.DataFrame:\n",
    "    results = []\n",
    "    for i, row in file_paths_df.iterrows():\n",
    "        article_id = row['article_id']\n",
    "        pdf_file_path = row['pdf_file_path']\n",
    "        xml_file_path = row['xml_file_path']\n",
    "\n",
    "        print(f\"Processing article {i}/{len(file_paths_df)}: {article_id}\")\n",
    "\n",
    "        # Load the text content from the PDF or XML file\n",
    "        text_content = load_article_text(pdf_file_path) if pdf_file_path else load_article_text(xml_file_path)\n",
    "\n",
    "        # Prepare the user input for the model\n",
    "        user_input = f\"Article ID: {article_id}\\nText Content: {text_content}\\n\"\n",
    "\n",
    "        # Generate response from the model\n",
    "        response, thinking_content = model.generate_response(user_input)\n",
    "\n",
    "        results.append({\n",
    "            'article_id': article_id,\n",
    "            'llm_input': user_input,\n",
    "            'llm_response': response,\n",
    "            'llm_thinking_content': thinking_content\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(by=[\"article_id\"]).reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "4d2524bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article 52/2: 10.1017_rdc.2022.19\n",
      "Total blocks with data related dataset IDs found: 1\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1017_rdc.2022.19.pdf. Length: 1934 characters\n",
      "Preparing input with length: 1982\n",
      "Parsing response from generated IDs...\n",
      "Processing article 53/2: 10.1017_s0007123423000601\n",
      "Total blocks with data related dataset IDs found: 1\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\train\\PDF\\10.1017_s0007123423000601.pdf. Length: 1248 characters\n",
      "Preparing input with length: 1302\n",
      "Parsing response from generated IDs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "llm_input",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "llm_response",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "llm_thinking_content",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "056093f4-8300-4674-8aee-aadd2246cb29",
       "rows": [
        [
         "0",
         "10.1017_rdc.2022.19",
         "Article ID: 10.1017_rdc.2022.19\nText Content: ABSTRACT.\nIn the frame of the IAEA-CRP (Coordinated Research Projects): Enhancing Nuclear Analytical Techniques to Meet the Needs of Forensic Sciences, an intercomparison exercise was organized between three AMS laboratories.\nAim of the program is to promote the use of nuclear and accelerator-based techniques in routine forensics practice.\nIn this view, one of the key points is the assessment of the precision and accuracy levels achievable on material of forensic interest.\nWe review the general structure and status of the project, with emphasis on results obtained in the analysis of wines of different grape varieties and grounded coffee beans from different locations such as Brazil, Spain, and Italy.\nThe three laboratories processed the samples according to different chemical protocols and performed the 14C measurements using different systems: MICADAS in Zurich and Debrecen and a HVEE 4130HC 3 MV Tandetron in Lecce.\nWithin the quoted uncertainty, the results showed good reproducibility, indicating that uncertainty level of the order of 0.3% are achieved by AMS on a single sample while multiple sample analyses results in precision down to 0.1–0.2%.\nThe measured 14C concentrations on coffee and wine samples resulted to be consistent with atmospheric 14C levels in the growing years. .\n potential_dataset_ids: [{\"dataset_ids\": ['https://doi.org/10.11588/data/10100'], context: \" L, Fedi M, Friedrich R, Maspero F, Sava T. 2019.\nradiocarbon dating and the protection of cultural heritage.\nRadiocarbon 61(5):1133–1134. \nHammer S, Levin I. 2017. \nMonthly mean atmospheric D14CO2 at Jungfraujoch and Schauinsland from 1986 to 2016.\nhttps://doi.org/10.11588/data/10100 heiDATA: Heidelberg Research Data Repository [Distributor] V2 [Version]. \nHandlos P, Svetlik I, Horáčková L, Fejgl M, Kotik L, Brychová V, Megisova N, Marecová K. 2018. \nBomb peak: radiocarbon dating of skeletal remains in routine forensic medical p\"},]\n",
         "[{'dataset_id': 'https://doi.org/10.11588/data/10100', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem step by step. The user provided an article with an ID and a text abstract, along with a list of potential dataset_ids and their context. The goal is to identify all citations of datasets, classify their type, and return the results in JSON.\n\nFirst, I need to look through the text of the paper to find any mentions of datasets. The text mentions three laboratories processing samples using different chemical protocols and measuring 14C. The DOI provided is \"https://doi.org/10.11588/data/10100\". \n\nNext, I need to check if this DOI matches the format for a dataset ID. The DOI format is 10.1... with a prefix of 10. So the example given in the problem has \"https://doi.org/10.11588/data/10100\". The DOI here starts with \"10.\" followed by a 4 or 5-digit number. The suffix part includes \"data\" and \"10100\". The problem mentions that the dataset_id is derived from the DOI. \n\nNow, looking at the potential_dataset_ids, the first one is the DOI. The context says that this is a dataset related to radiocarbon dating and cultural heritage. The other entries in the potential_dataset_ids are Accession IDs like \"EPI_ISL_10271777\" and \"IPR000264\", but those seem to be from different repositories. The user's text mentions that the three laboratories used 14C measurements. \n\nWait, the problem's text says that the three laboratories processed samples using different systems. The 14C measurements are part of the analysis. The DOI in the text is likely a dataset related to the radiocarbon dating. The other Accession IDs might be from other datasets, but the user's context includes the DOI as part of the dataset. \n\nSo, the dataset_id here is \"https://doi.org/10.11588/data/10100\". The type is \"Primary\" because it's a dataset used in the analysis, not just raw data. The other Accession IDs in the potential_dataset_ids are probably not relevant here. \n\nTherefore, the JSON should include this dataset_id and type as Primary. Since there's only one dataset found, the JSON array will have that object. The user's example response shows a similar structure, so I need to follow that format.\n</think>"
        ],
        [
         "1",
         "10.1017_s0007123423000601",
         "Article ID: 10.1017_s0007123423000601\nText Content: Abstract Like much of the European centre-left, Britain’s Labour Party has struggled to appeal to its former core working class support base in recent years.\nHowever, this is largely a failure to connect with the ‘white working class’ (WWC) specifically, whereas support among ethnic minorities remains robust.\nWe hypothesise that Labour could be experiencing a ‘trade-off’, whereby efforts to cater to minorities harm its perceived ability to represent WWC interests.\nWe test this thesis by examining whether WWC voters are more likely to view minority and working class representation in zero-sum terms and shun Labour when they associate the party with minority interests.\nWe show that the WWC are somewhat less likely to view working class and ethnic minority representation as strongly correlated, and Labour’s perceived ability to represent minorities is negatively associated with WWC support.\nThis is not (primarily) about ethnocentrism.\nInstead, we suggest that ‘relative political deprivation’ is crucial. .\n potential_dataset_ids: [{\"dataset_ids\": ['https://doi.org/10.7910/DVN/FWCKSE'], context: \"Data availability statement.\nReplication data for this article can be found in Harvard Dataverse at: https://doi.org/10.7910/DVN/FWCKSE \"},]\n",
         "[{'dataset_id': 'https://doi.org/10.7910/DVN/FWCKSE', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this step by step. The user provided an article with an ID and a text content. The potential_dataset_ids include a DOI and an Accession ID. First, I need to check if the DOI in the text is a dataset_id. The DOI given is https://doi.org/10.7910/DVN/FWCKSE. The format starts with 10. followed by a number, which matches the structure of a DOI. So that's a valid dataset_id. \n\nNext, the Accession ID mentioned is \"SAMN07159041\" and \"IPR000264\". The user's potential_dataset_ids list has one of these, but I need to check if they are present in the text. The text mentions \"data availability statement\" and \"Harvard Dataverse\" but doesn't directly mention the Accession ID. However, the example given in the potential_dataset_ids includes \"SAMN07159041\" as an Accession ID. Since the text does not explicitly state the Accession ID, but the example includes it, I should consider that as a possible dataset_id. But the user's potential_dataset_ids list has only one entry, so maybe the Accession ID is included there. However, the user's input might have a mistake in the potential_dataset_ids list. Let me double-check.\n\nThe user's potential_dataset_ids list shows ['https://doi.org/10.7910/DVN/FWCKSE', 'SAMN07159041'] as the dataset_ids. The context for the first one is \"Data availability statement\" and \"Harvard Dataverse\". The second one's context is not provided, but the user's example includes it. However, the user's actual input might have the Accession ID in the potential_dataset_ids. Since the user's text doesn't mention the Accession ID, but the example does, perhaps the correct dataset_id is the DOI. But the user's potential_dataset_ids list includes both. However, the user's actual input might have the Accession ID in the potential_dataset_ids list. Wait, looking back, the user's potential_dataset_ids list has 'SAMN07159041' as an Accession ID. The context for that is not provided, but the example given in the potential_dataset_ids includes it. So, perhaps the correct dataset_id is the Accession ID. But since the user's text doesn't mention it, maybe the correct answer is to include both? Wait, the user's potential_dataset_ids list has both entries. The first is a DOI, the second is an Accession ID. But the user's text mentions the DOI and the Accession ID in the context. However, the user's potential_dataset_ids list includes only one entry. Wait, no, the user's potential_dataset_ids list shows both entries. But maybe the Accession ID is not present in the text. Let me check again.\n\nThe text content mentions \"data availability statement\" and \"Harvard Dataverse\" but does not mention the Accession ID. However, the potential_dataset_ids list includes \"SAMN07159041\" as an Accession ID. So, even though the text doesn't mention it, the user's potential_dataset_ids list includes it. Therefore, I should process both entries. But the user's potential_dataset_ids list has both. So, in the JSON, both entries would be included, with their respective types. However, the user's potential_dataset_ids list has only one entry. Wait, no, looking back, the user's potential_dataset_ids list is:\n\npotential_dataset_ids: [{\"dataset_ids\": ['https://doi.org/10.7910/DVN/FWCKSE'], context: \"Data availability statement. Replication data for this article can be found in Harvard Dataverse at: https://doi.org/10.7910/DVN/FWCKSE \"},]\n\nand the other entry is SAMN07159041. Wait, no, the user's potential_dataset_ids list is only one entry. Wait, the user's input shows the potential_dataset_ids as:\n\npotential_dataset_ids: [{\"dataset_ids\": ['https://doi.org/10.7910/DVN/FWCKSE'], context: \"Data availability statement. Replication data for this article can be found in Harvard Dataverse at: https://doi.org/10.7910/DVN/FWCKSE \"},]\n\nand then maybe another entry? Wait, no, looking again, the user's potential_dataset_ids list is only the first entry. So, perhaps the Accession ID is not present in the text. Therefore, the only dataset_id found is the DOI. Therefore, the JSON should include that dataset_id and its type. However, the user's potential_dataset_ids list includes the Accession ID, but the text doesn't mention it. So, perhaps the correct answer is only the DOI. But the user's potential_dataset_ids list includes both entries. This is a bit confusing. Alternatively, maybe the Accession ID is part of the dataset_ids provided, but the text doesn't mention it. Therefore, the correct answer is to process only the DOI. But since the user's potential_dataset_ids list includes both, perhaps I should check again. Let me confirm once more.\n\nThe user's potential_dataset_ids list is:\n\npotential_dataset_ids: [{\"dataset_ids\": ['https://doi.org/10.7910/DVN/FWCKSE'], context: \"Data availability statement. Replication data for this article can be found in Harvard Dataverse at: https://doi.org/10.7910/DVN/FWCKSE \"},]\n\nand then maybe another entry? Wait, no, the user's input shows only the first entry. Therefore, only the DOI is present. So, the dataset_id is the DOI, and the type is \"Primary\" since it's a dataset for data availability. The Accession ID might not be present, so the JSON should include only that. Therefore, the final answer is a JSON with the dataset_id and type.\n</think>"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>llm_input</th>\n",
       "      <th>llm_response</th>\n",
       "      <th>llm_thinking_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1017_rdc.2022.19</td>\n",
       "      <td>Article ID: 10.1017_rdc.2022.19\\nText Content:...</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.11588/data...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1017_s0007123423000601</td>\n",
       "      <td>Article ID: 10.1017_s0007123423000601\\nText Co...</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.7910/DVN/F...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this step by step....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  article_id  \\\n",
       "0        10.1017_rdc.2022.19   \n",
       "1  10.1017_s0007123423000601   \n",
       "\n",
       "                                           llm_input  \\\n",
       "0  Article ID: 10.1017_rdc.2022.19\\nText Content:...   \n",
       "1  Article ID: 10.1017_s0007123423000601\\nText Co...   \n",
       "\n",
       "                                        llm_response  \\\n",
       "0  [{'dataset_id': 'https://doi.org/10.11588/data...   \n",
       "1  [{'dataset_id': 'https://doi.org/10.7910/DVN/F...   \n",
       "\n",
       "                                llm_thinking_content  \n",
       "0  <think>\\nOkay, let's tackle this problem step ...  \n",
       "1  <think>\\nOkay, let's tackle this step by step....  "
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_articles_df = process_articles(train_file_paths_df_3, inference_model)\n",
    "processed_articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "e9bb547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed_articles_df to CSV\n",
    "processed_articles_df.to_csv(\"processed_articles.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "c263cb88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'dataset_id': 'https://doi.org/10.11588/data/10100', 'type': 'Primary'}]"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "#response_text = processed_articles_df[processed_articles_df['article_id']=='10.1017_rdc.2022.19']['llm_response']\n",
    "response_text = processed_articles_df.loc[processed_articles_df['article_id'] == '10.1017_rdc.2022.19', 'llm_response'].values\n",
    "response_text[0]\n",
    "# parsed_json = json.loads(response_text[0])\n",
    "# print(parsed_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "8ae6e4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing article 0/30: 10.1002_2017jc013030\n",
      "Total blocks with data related dataset IDs found: 3\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf. Length: 4335 characters\n",
      "Preparing input with length: 4096\n",
      "Parsing response from generated IDs...\n",
      "Processing article 1/30: 10.1002_anie.201916483\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.201916483.pdf. Length: 789 characters\n",
      "Preparing input with length: 840\n",
      "Parsing response from generated IDs...\n",
      "Processing article 2/30: 10.1002_anie.202005531\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202005531.pdf. Length: 1088 characters\n",
      "Preparing input with length: 1139\n",
      "Parsing response from generated IDs...\n",
      "Processing article 3/30: 10.1002_anie.202007717\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202007717.pdf. Length: 1130 characters\n",
      "Preparing input with length: 1181\n",
      "Parsing response from generated IDs...\n",
      "Processing article 4/30: 10.1002_chem.201902131\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.201902131.pdf. Length: 564 characters\n",
      "Preparing input with length: 615\n",
      "Parsing response from generated IDs...\n",
      "Processing article 5/30: 10.1002_chem.201903120\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.201903120.pdf. Length: 784 characters\n",
      "Preparing input with length: 835\n",
      "Parsing response from generated IDs...\n",
      "Processing article 6/30: 10.1002_chem.202000235\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202000235.pdf. Length: 610 characters\n",
      "Preparing input with length: 661\n",
      "Parsing response from generated IDs...\n",
      "Processing article 7/30: 10.1002_chem.202001412\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202001412.pdf. Length: 887 characters\n",
      "Preparing input with length: 938\n",
      "Parsing response from generated IDs...\n",
      "Processing article 8/30: 10.1002_chem.202001668\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202001668.pdf. Length: 322 characters\n",
      "Preparing input with length: 373\n",
      "Parsing response from generated IDs...\n",
      "Processing article 9/30: 10.1002_chem.202003167\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202003167.pdf. Length: 827 characters\n",
      "Preparing input with length: 878\n",
      "Parsing response from generated IDs...\n",
      "Processing article 10/30: 10.1002_cssc.202201821\n",
      "Total blocks with data related dataset IDs found: 1\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_cssc.202201821.pdf. Length: 3253 characters\n",
      "Preparing input with length: 3304\n",
      "Parsing response from generated IDs...\n",
      "Processing article 11/30: 10.1002_ece3.3985\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.3985.pdf. Length: 1881 characters\n",
      "Preparing input with length: 1927\n",
      "Parsing response from generated IDs...\n",
      "Processing article 12/30: 10.1002_ece3.4466\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.4466.pdf. Length: 1804 characters\n",
      "Preparing input with length: 1850\n",
      "Parsing response from generated IDs...\n",
      "Processing article 13/30: 10.1002_ece3.5260\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.5260.pdf. Length: 1945 characters\n",
      "Preparing input with length: 1991\n",
      "Parsing response from generated IDs...\n",
      "Processing article 14/30: 10.1002_ece3.5395\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.5395.pdf. Length: 1940 characters\n",
      "Preparing input with length: 1986\n",
      "Parsing response from generated IDs...\n",
      "Processing article 15/30: 10.1002_ece3.6144\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6144.pdf. Length: 1799 characters\n",
      "Preparing input with length: 1845\n",
      "Parsing response from generated IDs...\n",
      "Processing article 16/30: 10.1002_ece3.6303\n",
      "Total blocks with data related dataset IDs found: 2\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6303.pdf. Length: 2121 characters\n",
      "Preparing input with length: 2167\n",
      "Parsing response from generated IDs...\n",
      "Processing article 17/30: 10.1002_ece3.6784\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6784.pdf. Length: 1859 characters\n",
      "Preparing input with length: 1905\n",
      "Parsing response from generated IDs...\n",
      "Processing article 18/30: 10.1002_ece3.961\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.961.pdf. Length: 2278 characters\n",
      "Preparing input with length: 2323\n",
      "Parsing response from generated IDs...\n",
      "Processing article 19/30: 10.1002_ece3.9627\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.9627.pdf. Length: 1800 characters\n",
      "Preparing input with length: 1846\n",
      "Parsing response from generated IDs...\n",
      "Processing article 20/30: 10.1002_ecs2.1280\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.1280.pdf. Length: 2565 characters\n",
      "Preparing input with length: 2611\n",
      "Parsing response from generated IDs...\n",
      "Processing article 21/30: 10.1002_ecs2.4619\n",
      "Total blocks with data related dataset IDs found: 2\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.4619.pdf. Length: 3015 characters\n",
      "Preparing input with length: 3061\n",
      "Parsing response from generated IDs...\n",
      "Processing article 22/30: 10.1002_ejic.201900904\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejic.201900904.pdf. Length: 532 characters\n",
      "Preparing input with length: 583\n",
      "Parsing response from generated IDs...\n",
      "Processing article 23/30: 10.1002_ejoc.202000139\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejoc.202000139.pdf. Length: 402 characters\n",
      "Preparing input with length: 453\n",
      "Parsing response from generated IDs...\n",
      "Processing article 24/30: 10.1002_ejoc.202000916\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejoc.202000916.pdf. Length: 215 characters\n",
      "Preparing input with length: 266\n",
      "Parsing response from generated IDs...\n",
      "Processing article 25/30: 10.1002_esp.5058\n",
      "Total blocks with data related dataset IDs found: 1\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_esp.5058.pdf. Length: 4011 characters\n",
      "Preparing input with length: 4056\n",
      "Parsing response from generated IDs...\n",
      "Processing article 26/30: 10.1002_esp.5090\n",
      "Total blocks with data related dataset IDs found: 2\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_esp.5090.pdf. Length: 3655 characters\n",
      "Preparing input with length: 3700\n",
      "Parsing response from generated IDs...\n",
      "Processing article 27/30: 10.1002_mp.14424\n",
      "Total blocks with data related dataset IDs found: 3\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_mp.14424.pdf. Length: 4011 characters\n",
      "Preparing input with length: 4056\n",
      "Parsing response from generated IDs...\n",
      "Processing article 28/30: 10.1002_nafm.10870\n",
      "Total blocks with data related dataset IDs found: 2\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_nafm.10870.pdf. Length: 3080 characters\n",
      "Preparing input with length: 3127\n",
      "Parsing response from generated IDs...\n",
      "Processing article 29/30: 10.1007_jhep07(2018)134\n",
      "Total blocks with data related dataset IDs found: 0\n",
      "Extracted text from ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1007_jhep07(2018)134.pdf. Length: 723 characters\n",
      "Preparing input with length: 775\n",
      "Parsing response from generated IDs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "llm_input",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "llm_response",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "llm_thinking_content",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "aee76f34-17ac-4bd3-b1e1-9fdf24eb719b",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "Article ID: 10.1002_2017jc013030\nText Content: Abstract Characterizing phytoplankton distribution and dynamics in the world’s open oceans requires in situ observations over a broad range of space and time scales.\nIn addition to temperature/salinity measure-ments, Biogeochemical-Argo (BGC-Argo) profiling floats are capable of autonomously observing at high-frequency bio-optical properties such as the chlorophyll fluorescence, a proxy of the chlorophyll a concen-tration (Chla), the particulate backscattering coefficient (bbp), a proxy of the stock of particulate organic car-bon, and the light available for photosynthesis.\nWe analyzed an unprecedented BGC-Argo database of more than 8,500 multivariable profiles collected in various oceanic conditions, from subpolar waters to subtropical gyres.\nOur objective is to refine previously established Chla versus bbp relationships and gain insights into the sources of vertical, seasonal, and regional variability in this relationship.\nDespite some regional, seasonal and vertical variations, a general covariation occurs at a global scale.\nWe distinguish two main contrasted situations: (1) concomitant changes in Chla and bbp that correspond to actual variations in phytoplankton biomass, e.g., in subpolar regimes; (2) a decoupling between the two variables attributed to photoacclima-tion or changes in the relative abundance of nonalgal particles, e.g., in subtropical regimes.\nThe variability in the bbp:Chla ratio in the surface layer appears to be essentially influenced by the type of particles and by photoacclimation processes.\nThe large BGC-Argo database helps identifying the spatial and temporal scales at which this ratio is predominantly driven by one or the other of these two factors. .\n potential_dataset_ids: [{\"dataset_ids\": ['https://doi.org/10.17882/49388'], context: \"rbieux, M., Organelli, E., Claustre, H., Schmechtig, C., Poteau, A., Boss, E., . . .\nXing, X. (2017).\nA global database of vertical profiles derived from Biogeochemical Argo float measurements for biogeochemical and bio-optical applications.\nSEANOE.\nhttps://doi.org/10.17882/49388 Barton, A. D., Lozier, M. S., & Williams, R. G. (2015).\nPhysical controls of variability in North Atlantic phytoplankton communities.\nLimnology and Oceanography, 60(1), 181–197.\nhttps://doi.org/10.1002/lno.10011 Beckmann, A., & Hense, I. (2007).\nBen\"},{\"dataset_ids\": ['https://doi.org/10.17882/47142'], context: \"e, Canada) are acknowledged for useful comments and fruitful discussion.\nWe also thank the International Argo Program and the CORIOLIS project that contribute to make the data freely and publicly available.\nData referring to Organelli et al. (2016a; https://doi.org/10.17882/47142) and Barbieux et al. (2017; https://doi.org/10.17882/49388) are freely available on SEANOE. \"},{\"dataset_ids\": ['https://doi.org/10.17882/49388'], context: \"l discussion.\nWe also thank the International Argo Program and the CORIOLIS project that contribute to make the data freely and publicly available.\nData referring to Organelli et al. (2016a; https://doi.org/10.17882/47142) and Barbieux et al. (2017; https://doi.org/10.17882/49388) are freely available on SEANOE. \"},{\"dataset_ids\": ['https://doi.org/10.1016/j.rse.'], context: \"0319 Navarro, G., Alvain, S., Vantrepotte, V., & Huertas, I. E. (2014).\nIdentification of dominant phytoplankton functional types in the Mediterra-nean Sea based on a regionalized remote sensing approach.\nRemote Sensing of Environment, 152, 557–575.\nhttps://doi.org/10.1016/j.rse. \n2014.06.029 Organelli, E., Barbieux, M., Claustre, H., Schmechtig, C., Poteau, A., Bricaud, A., . . .\nDall’Olmo, G. (2016a).\nA global bio-optical database derived from Biogeochemical Argo float measurements within the layer of interest for field an\"},{\"dataset_ids\": ['https://doi.org/10.17882/47142'], context: \"laustre, H., Schmechtig, C., Poteau, A., Bricaud, A., . . .\nDall’Olmo, G. (2016a).\nA global bio-optical database derived from Biogeochemical Argo float measurements within the layer of interest for field and remote ocean colour applications.\nSEANOE.\nhttps://doi.org/10.17882/47142 Organelli, E., Barbieux, M., Claustre, H., Schmechtig, C., Poteau, A., Bricaud, A., . . .\nXing, X. (2017b).\nTwo databases derived from BGC-Argo float measurements for marine biogeochemical and bio-optical applications.\nEarth System Science Data, 9, \"},]\n",
         "[{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}, {'dataset_id': 'https://doi.org/10.17882/47142', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this step by step. The user provided an article ID and a text content, along with potential_dataset_ids. My task is to identify citations of datasets, classify their types as Primary or Secondary, and return them in JSON.\n\nFirst, I need to look through the text content to find any dataset IDs. The text mentions \"Biogeochemical-Argo (BGC-Argo) profiling floats\" and \"BGC-Argo database of over 8,500 profiles\". The DOI here is https://doi.org/10.17882/49388. That's one dataset.\n\nNext, I check the potential_dataset_ids. The first entry has a DOI, so that's one dataset. The second entry is an Accession ID, which might be a different dataset. The third and fourth entries are also DOIs, but the context mentions \"Organelli et al.\" and \"Barbieux et al.\" which are in the same paper. The fifth entry is another DOI, but the context says it's for a different paper. \n\nWait, the text says \"data referring to Organelli et al. (2016a; https://doi.org/10.17882/47142) and Barbieux et al. (2017; https://doi.org/10.17882/49388) are freely available on SEANOE.\" So these two are from the same paper. The first dataset ID is the DOI for the BGC-Argo database, which is part of the paper's content. The second dataset ID is another DOI, but it's for a different paper. \n\nSo, in the potential_dataset_ids, there are two entries: one with the DOI and another with the Accession ID. The first one is for the dataset, and the second might be a different dataset. However, the user's task is to classify the type. The first dataset is part of the BGC-Argo database, which is primary data (since it's the database used for analysis). The second dataset is another DOI, but the context doesn't specify its type. Wait, the context says \"data referring to Organelli et al. (2016a; ...)\" so maybe that's a secondary dataset? Or perhaps both are primary?\n\nThe user's instructions say that Primary is raw or processed data generated as part of this study, specifically for this study. So if the dataset is part of the BGC-Argo database, which is the dataset used for analysis, then it's primary. The second dataset is another DOI, but since it's part of the same paper, maybe it's also primary. However, the user's potential_dataset_ids include both, so I need to check which ones are actually from the paper.\n\nThe text mentions \"BGC-Argo database\" in the abstract and the context, so that's the primary dataset. The other two entries are perhaps from other papers or different datasets. So, in the JSON, I should have one dataset with type Primary and the others as Missing or maybe Secondary if they are not part of the study's data.\n\nWait, the user's third task says to return the dataset IDs classified as Primary or Secondary. If the first DOI is from the BGC-Argo database, which is used in the study, then it's primary. The other two entries might be from different datasets, so they would be classified as Missing or perhaps Secondary. But according to the user's potential_dataset_ids, they are there. But the user's instruction says to classify each dataset as Primary or Secondary. So if there are multiple dataset_ids, I need to check which ones are from the paper and assign their types.\n\nIn the text, the first dataset ID is used in the context of the paper's analysis. The other two entries are from other papers, so they are not part of this study's data. Therefore, the JSON should have one dataset with type Primary and the rest as Missing. But wait, the user's potential_dataset_ids include multiple entries. So, perhaps the first one is the dataset, and the others are not. Therefore, the JSON should include that dataset and leave the others as Missing.\n</think>"
        ],
        [
         "1",
         "10.1002_anie.201916483",
         "Article ID: 10.1002_anie.201916483\nText Content: Abstract: The homoleptic pyrazolate complexes [CeIII 4-(Me2pz)12] and [CeIV(Me2pz)4]2 quantitatively insert CO2 to give [CeIII 4(Me2pz·CO2)12] and [CeIV(Me2pz·CO2)4], respec-tively (Me2pz = 3,5-dimethylpyrazolato).\nThis process is rever-sible for both complexes, as observed by in situ IR and NMR spectroscopy in solution and by TGA in the solid state.\nBy adjusting the molar ratio, one molecule of CO2 per [CeIV-(Me2pz)4] complex could be inserted to give trimetallic [Ce3-(Me2pz)9(Me2pz·CO2)3(thf)].\nBoth the cerous and ceric in-sertion products catalyze the formation of cyclic carbonates from epoxides and CO2 under mild conditions.\nIn the absence of epoxide, the ceric catalyst is prone to reduction by the co-catalyst tetra-n-butylammonium bromide (TBAB). .\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's start by looking at the article ID provided, which is 10.1002_anie.201916483. The text content is about homoleptic pyrazolate complexes [CeIII 4-(Me2pz)12] and [CeIV(Me2pz)4]2 inserting CO2. \n\nFirst, I need to identify any dataset_ids mentioned in the text. The user mentioned two ways to find dataset_ids: DOIs and Accession IDs. The text doesn't mention any DOIs or Accession IDs. The abstract talks about the synthesis of complexes and their reactions, but nothing about datasets. \n\nLooking at the potential_dataset_ids list, there are no entries provided. The user says that if there are no dataset_ids, return a JSON with \"Missing\" for both. Since the text doesn't mention any datasets, I should return the default JSON structure.\n\nWait, but maybe I missed something. Let me double-check. The abstract is about chemical processes, not datasets. The potential_dataset_ids are empty. So yes, the correct response is the missing JSON array.\n</think>"
        ],
        [
         "2",
         "10.1002_anie.202005531",
         "Article ID: 10.1002_anie.202005531\nText Content: Abstract: Trifluoromethyl sulfoxides are a new class of trifluoromethylthiolating reagent.\nThe sulfoxides engage in metal-free C�H trifluoromethylthiolation with a range of (hetero)arenes.\nThe method is also applicable to the function-alization of important compound classes, such as ligand derivatives and polyaromatics, and in the late-stage trifluor-omethylthiolation of medicines and agrochemicals.\nThe iso-lation and characterization of a sulfonium salt intermediate supports an interrupted Pummerer reaction mechanism. \nIncorporating fluorine into organic compounds is a useful tool in drug design and development.\nThe fluoro group is well known to improve the pharmacokinetic properties of a mol-ecule and fluorine-18 is an important radioisotope in molec-ular imaging.[1,2] Trifluoromethylthio (SCF3) groups are commonly found in drug molecules and veterinary medi-cines.[3,4]\nBy combining a fluorinated moiety with a hetero-atom, many have turned to the SCF3 group to impart useful properties, such as high lipophilicity, to a compound of interest.[5] .\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's start by looking at the article ID and the text provided. The article ID is 10.1002_anie.202005531, which seems like a paper title or abstract. The text content is about trifluoromethyl sulfoxides (SCF3) as a reagent for metal-free C-H trifluoromethylthiolation. It mentions applications in ligand derivatives, polyaromatics, and in the late-stage trifluoromethylthiolation of medicines and agrochemicals. There's also a part about sulfonium salt intermediates supporting an interrupted Pummerer reaction mechanism. The text also talks about fluorine improving pharmacokinetics and fluorine-18 for imaging. The potential_dataset_ids list is empty.\n\nNow, the task is to identify citations of datasets. The first way to find dataset IDs is via DOIs. The text mentions \"data availability\" and \"data release\" but there's no specific DOI given. The second way is Accession IDs, but the text doesn't mention any. Since there are no dataset IDs mentioned in the abstract, I should return that there are no dataset citations. Therefore, the JSON would have a dataset_id of \"Missing\" and type \"Missing\".\n</think>"
        ],
        [
         "3",
         "10.1002_anie.202007717",
         "Article ID: 10.1002_anie.202007717\nText Content: Abstract: Metal-ligand cooperativity (MLC) had a remarkable impact on transition metal chemistry and catalysis.\nBy use of the calix[4]pyrrolato aluminate, [1]�, which features a square-planar AlIII, we transfer this concept into the p-block and fully elucidate its mechanisms by experiment and theory.\nComple-mentary to transition metal-based MLC (aromatization upon substrate binding), substrate binding in [1]�occurs by dearomatization of the ligand.\nThe aluminate trapps carbonyls by the formation of C�C and Al�O bonds, but the products maintain full reversibility and outstanding dynamic exchange rates.\nRemarkably, the C�C bonds can be formed or cleaved by the addition or removal of lithium cations, permitting unpre-cedented control over the system�s constitutional state.\nMore-over, the metal-ligand cooperative substrate interaction allows to twist the kinetics of catalytic hydroboration reactions in a unique sense.\nUltimately, this work describes the evolution of an anti-van�t Hoff/Le Bel species from their being as a struc-tural curiosity to their application as a reagent and catalyst. .\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's start by looking at the article ID and the text content provided. The article is about metal-ligand cooperativity (MLC) in transition metal chemistry, specifically using a calix[4]pyrrolato aluminate structure. The abstract mentions that this structure allows for the formation and cleavage of C-C and Al-O bonds, and the products maintain reversibility and dynamic exchange rates.\n\nNow, the potential_dataset_ids given are empty. But maybe there's a dataset related to this structure. Let me think. The example for a dataset ID includes something like \"calix[4]pyrrolato aluminate\" as part of the dataset. However, the provided potential_dataset_ids list is empty. Wait, maybe the user expects me to infer the dataset ID from the text. The abstract mentions \"calix[4]pyrrolato aluminate\" which is a chemical structure. But the potential_dataset_ids are empty. Hmm, the user might have provided a list that's empty, so I need to check if there's any dataset mentioned in the text. \n\nLooking again, the text doesn't mention any dataset IDs. The abstract talks about the structure and its effects, but no dataset identifier is given. Therefore, according to the instructions, if there are no dataset IDs found, I should return a JSON object with \"dataset_id\" and \"type\" as \"Missing\". Since the potential_dataset_ids list is empty, that's the case here.\n</think>"
        ],
        [
         "4",
         "10.1002_chem.201902131",
         "Article ID: 10.1002_chem.201902131\nText Content: Abstract: A new synthetic route toward the tetrazole core is described, which is based on a general fragmentation pat-tern that was found in a range of compounds featuring geminal diazido units.\nThrough a simple two-step procedure, the synthesis of structurally diverse target compounds con-taining a tetrazole, such as tetrazoloquinoxalinones, benzoy-laryltetrazoles, tetrazolotriazinones, and tetrazoloazepinones, was easily accomplished, starting from broadly accessible substrates (i.e., oxindoles, diarylethanones, pyrazolones, and .\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's start by looking at the article provided. The article ID is 10.1002_chem.201902131. The text content is about a synthetic route for tetrazole-based compounds. The abstract mentions using a general fragmentation pattern from compounds with geminal diazido units. Then, through a two-step process, they synthesize various target compounds like tetrazoloquinoxalinones, etc., starting from substrates like oxindoles, diarylethanones, etc.\n\nNow, the potential_dataset_ids are listed as empty. Wait, the user said that the potential_dataset_ids are part of the paper, but in the given text, there are no dataset_ids mentioned. So perhaps the potential_dataset_ids are empty, meaning no datasets are cited here. But the user's task is to identify any dataset citations. Since there are no dataset_ids in the text, maybe the answer is that there are no dataset citations. But wait, maybe I'm missing something. Let me check again.\n\nThe abstract says \"synthesis of structurally diverse target compounds containing a tetrazole, such as tetrazoloquinoxalinones, benzoy-laryltetrazoles, tetrazolotriazinones, and tetrazoloazepinones\". The substrates are oxindoles, diarylethanones, pyrazolones, etc. But these are organic compounds, not datasets. So there's no dataset_id mentioned here. Therefore, the answer should be that there are no dataset citations. But the user's instruction says if there are dataset_ids, classify them. Since there are none, the JSON should have Missing for both dataset_id and type. But let me make sure.\n\nWait, maybe I'm misunderstanding. Could the substrates be considered datasets? Like, if the paper is using a dataset of substrates, but the text doesn't mention anything about datasets. The abstract talks about the synthesis method, not about the datasets used. So yes, there's no citation. Therefore, the correct response is to return the JSON with the missing dataset_id and type.\n</think>"
        ],
        [
         "5",
         "10.1002_chem.201903120",
         "Article ID: 10.1002_chem.201903120\nText Content: Abstract: Systematic variation of the 1,4-dioxane (dx) con-centration during the precipitation of sparingly soluble [MgBr2(dx)2] from ethereal Grignard solutions of RMgBr has allowed the structural investigation of crystallized [R2Mg(dx)n] (n=1, 1.5, 2, and 3), which form during this diox-ane method, depending on the bulkiness of R. The number-ing of the complexes explored in this study is based on the number n of dioxane molecules per magnesium atom, fol-lowed by the substituent R; an apostrophe denotes coordi-nation polymers.\nThe following derivatives were studied by X-ray crystal-structure determination and NMR spectroscopy: n=1: [Me2Mg(m-dx)]1 (1’-Me) and [nPr2Mg(m-dx)]1 (1’-nPr); n=1.5: [{iPr2Mg(dx)}2(m-dx)] (1.5-iPr), [{oTol2Mg(dx)}2(m-dx)] .\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided an article with an ID and a text abstract. They also have a list of potential dataset_ids. My job is to identify citations of datasets, classify their type as Primary or Secondary, and return the results in JSON.\n\nFirst, looking at the article ID, 10.1002_chem.201903120. I need to check if this ID is a DOI or an Accession ID. The first part is a hyphen, so that's probably a DOI. The DOI format starts with 10. So the prefix is 10. The suffix can be something like 12345 or 1234. The example given is https://doi.org/10.12345/12345. The user's article has a DOI, but the potential_dataset_ids list is empty. Wait, the potential_dataset_ids list is empty in the user's input? Wait, no, looking back, the user says: \"potential_dataset_ids: []\" in their input. So there are no dataset_ids mentioned here.\n\nWait, but maybe there's a dataset_id that's not listed here. Let me check the text again. The abstract mentions studies on derivatives. The text says \"the following derivatives were studied by X-ray crystal-structure determination and NMR spectroscopy: n=1: [Me2Mg(m-dx)]1 (1’-Me) and [nPr2Mg(m-dx)]1 (1’-nPr); n=1.5: [{iPr2Mg(dx)}2(m-dx)] (1.5-iPr), [{oTol2Mg(dx)}2(m-dx)]\". Hmm, these are names of compounds, not dataset_ids. The user's task says to identify dataset_ids based on the context. But the abstract doesn't mention any dataset IDs. The potential_dataset_ids list is empty. So according to the user's input, there are no dataset_ids found here. Therefore, the JSON should return a single object with dataset_id \"Missing\" and type \"Missing\".\n\nWait, but maybe I'm missing something. Let me double-check. The abstract talks about using X-ray and NMR spectroscopy, which are methods, not datasets. The potential_dataset_ids list is empty. Therefore, no dataset citations are found. So the answer is that there's no dataset used, hence the JSON object with both fields as Missing.\n</think>"
        ],
        [
         "6",
         "10.1002_chem.202000235",
         "Article ID: 10.1002_chem.202000235\nText Content: Abstract: The reaction of bis(silylenyl)-substituted ferro-cene 1 with two molar equivalents of BPh3 yields the cor-responding bis(silylene–borane) Lewis adduct 2.\nThe latter is capable to activate CO2 to furnish the borane-stabilized bis(silanone) 3 through mono-oxygenation of the dative SiII!B silicon centers under release of CO.\nRemoval of BPh3 from 3 with PMe3 affords the corresponding 1,3,2,4-cyclodisiloxane and the Me3P�BPh3 adduct.\nAll isolated new compounds were characterized and their molecular structures were determined by single-crystal X-ray diffrac-tion analyses. .\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's see. The user provided an article ID and a text content. They also have potential_dataset_ids, which are empty in this case. But wait, the article is about a chemical reaction, and the text mentions \"dataset\" and \"borane-stabilized bis(silanone)\" and \"corresponding bis(silylene–borane) Lewis adduct 2\". Hmm, maybe the datasets used here are related to the chemical compounds being analyzed. But looking at the potential_dataset_ids, there are none listed. So, according to task 1, there are no citations of datasets. Task 2 also says no datasets found. So the answer should be a single JSON object with \"dataset_id\" and \"type\" both missing. Wait, but the user's potential_dataset_ids are empty. So the JSON should be as specified.\n</think>"
        ],
        [
         "7",
         "10.1002_chem.202001412",
         "Article ID: 10.1002_chem.202001412\nText Content: Abstract: Reactions of a,b-unsaturated aromatic thioke-tones 1 (thiochalcones) with Fe3(CO)12 leading to h4-1-thia-1,3-diene iron tricarbonyl complexes 2, [FeFe] hydroge-nase mimics 3, and the thiopyrane adduct 4 are described. \nObtained products have been characterized by X-ray crys-tallography and by computational methods.\nCompletely regio- and diastereoselective formation of the five-mem-bered ring system in products 3, containing four stereo-genic centers, can be explained by an unprecedented, stepwise (3+2)-cycloaddition of two thiochalcone mole-cules mediated by Fe3(CO)12.\nQuantum chemical calcula-tions aimed at elucidation of the reaction mechanism, suggest that the formal (3+2)-cycloaddition proceeds via sequential intramolecular radical transfer events upon ho-molytic cleavage of one carbon-sulfur bond leading to a diradical intermediate. .\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's start by looking at the article ID provided, which is 10.1002_chem.202001412. The text content given is about reactions involving a,b-unsaturated aromatic thioketones (thiochalcones) with Fe3(CO)12, leading to specific products. The abstract mentions products 2, 3, and 4, and their characterization methods like X-ray crystallography and computational methods.\n\nNow, the potential_dataset_ids list is empty. The user's task is to check if any datasets are referenced. The article seems to be a chemical reaction paper, not involving datasets. The text doesn't mention any dataset IDs like DOIs or Accession IDs. \n\nSo, for task 1, there are no dataset citations found. For task 2, since there are no dataset_ids mentioned, both \"Missing\" and \"Missing\" are used. The JSON should reflect that. I need to make sure not to include any dataset IDs, just the empty array.\n</think>"
        ],
        [
         "8",
         "10.1002_chem.202001668",
         "Article ID: 10.1002_chem.202001668\nText Content: Abstract: We have added the {ReIVX5}�(X=Br, Cl) synthon to a pocket-based ligand to provide supramolecular design using halogen···halogen interactions within an FeIII system that has the potential to undergo spin crossover (SCO).\nBy removing the solvent from the crystal lattice, we “switch on” .\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's start by looking at the article ID and the text provided. The article ID is 10.1002_chem.202001668. The text mentions adding the {ReIVX5}�(X=Br, Cl) synthon to a pocket-based ligand. \n\nFirst, I need to check if there are any dataset_ids mentioned in the text. The user provided a list of potential_dataset_ids, but in this case, the text doesn't mention any dataset IDs like \"dataset\" or \"repository\". The text talks about molecular synthesis and ligand design, which doesn't seem related to datasets. \n\nSo, according to task 1, there are no citations of datasets in this article. Therefore, I should return the JSON with \"dataset_id\" as \"Missing\" and \"type\" as \"Missing\". \n\nWait, but maybe I missed something. Let me double-check. The abstract states that they added a synthon to a ligand, which might relate to data. But the dataset_ids provided in the potential_dataset_ids list are empty. Since there's no citation for any dataset_id, the answer should be as specified.\n</think>"
        ],
        [
         "9",
         "10.1002_chem.202003167",
         "Article ID: 10.1002_chem.202003167\nText Content: Abstract: Metal-only Lewis pairs (MOLPs) in which the two metal fragments are solely connected by a dative M!M bond represent privileged architectures to acquire funda-mental understanding of bimetallic bonding.\nThis has impor-tant implications in many catalytic processes or supramolec-ular systems that rely on synergistic effects between two metals.\nHowever, a systematic experimental/computational approach on a well-defined class of compounds is lacking. \nHere we report a family of MOLPs constructed around the RhI precursor\n[(h5-C5Me5)Rh(PMe3)2] (1) with a series of s, p and d-block metals, mostly from the main group elements, and investigate their bonding by computational means. \nAmong the new MOLPs, we have structurally characterized those formed by dative bonding between 1 and MgMeBr, .\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's start by looking at the article ID provided, which is 10.1002_chem.202003167. The text content mentions a family of MOLPs constructed around the RhI precursor [(h5-C5Me5)Rh(PMe3)2] (1) with various metals. \n\nFirst, I need to check if there are any dataset_ids mentioned in the text. The potential_dataset_ids list is empty here. The text talks about experimental and computational methods, but there's no mention of specific datasets like data releases, repositories, or identifiers. \n\nLooking at the abstract, it says \"systematic experimental/computational approach on a well-defined class of compounds is lacking.\" But the new MOLPs are described as constructed around the precursor and investigated computationally. However, there's no explicit dataset identifier mentioned. \n\nSince the potential_dataset_ids are empty, I have to conclude that no datasets were cited. Therefore, the JSON object should include \"dataset_id\": \"Missing\" and \"type\": \"Missing\" for both entries.\n</think>"
        ],
        [
         "10",
         "10.1002_cssc.202201821",
         "Article ID: 10.1002_cssc.202201821\nText Content: Development of a Mg/O ReaxFF Potential to describe the Passivation Processes in Magnesium-Ion Batteries** Florian Fiesinger,[a]\nDaniel Gaissmaier,[a, b, c]\nMatthias van den Borg,[a] Julian Beßner,[a] Adri C. T. van Duin,[d] and Timo Jacob*[a, b, c] One of the key challenges preventing the breakthrough of magnesium-ion batteries (MIB) is the formation of a passivating boundary layer at the Mg anode.\nTo describe the initial steps of Mg anode degradation by O2 impurities, a Mg/O ReaxFF (force field for reactive systems) parameter set was developed capable of accurately modeling the bulk, surface, adsorption, and diffusion properties of metallic Mg and the salt MgO. It is shown that O2 immediately dissociates upon first contact with the Mg anode (modeled as Mg(0001), Mg(101̄0)A, and Mg(101̄1)), heating the surface to several 1000 K. The high temperature assists the further oxidation and forms a rock salt interphase intersected by several grain boundaries.\nAmong the Mg surface terminations, Mg(101̄0)A is the most reactive, forming an MgO layer with a thickness of up to 25 Å. The trained force field can be used to model the ongoing reactions in Mg–air batteries but also to study the oxidation of magnesium metal in general. \nIntroduction The search for new energy storage technologies to promote electromobility and reduce global warming is one of the greatest challenges of our time.\nMagnesium-ion batteries (MIB) are attracting increasing attention as an alternative to conven-tional lithium-ion batteries (LIB) due to the unique properties of magnesium metal anodes, such as high volumetric capacity, low cost, and safe operation.[1–5] However, mass market adoption of MIBs is currently hindered by significant challenges. \nOne of the biggest challenges is the formation of a passivating interface on the Mg surface which is impermeable to Mg cations and leads to battery failure.[6–8]\nSun et al. have identified three reasons for Mg degradation in a recent review article.[9] First, Mg has a higher charge density due to its bi-valency, which leads not only to stronger coordination with solvent molecules and salt anions but also facilitates solvent decom-position at the anode.[10] For example, Lu et al. have shown that the salt anion ClO4 is chemically reduced at the Mg anode in carbonate-containing solvents, forming a solid-electrolyte inter-face (SEI) consisting of Mg-halides, MgO, Mg(OH)2, MgCO3, and Mg(ROCO2)2.[11] Second, short circuits are induced by non-uniform Mg growth, which can be further subdivided into hemispherical,[12] porous,[13] or dendritic[14] growth morphologies.[9] Hemispherical 3D particles with different shapes have been detected in TFSI-based electrolytes as a function of the applied current density.[12]\nIn an Mg(TFSI)2 salt electrolyte with MgCl2 as additive, continuous stripping and plating leads to fracturing and reformation of the SEI .\n potential_dataset_ids: [{\"dataset_ids\": ['https://doi.org/10.5281/zenodo.7074790.'], context: \"The data that support the findings of this study are openly available in zenodo at https://doi.org/10.5281/zenodo.7074790. \nFurthermore, the KVIK optimization routine is openly available in the GitHub repository at https://github.com/shk11/KVIKOp-timizer. \"},]\n",
         "[{'dataset_id': 'https://doi.org/10.5281/zenodo.7074790', 'type': 'Primary'}]",
         "<think>\nOkay, let's start by looking at the article ID 10.1002_cssc.202201821. The text mentions a Mg/O ReaxFF potential and some references to datasets. The potential_dataset_ids provided have two entries.\n\nFirst, the DOI: https://doi.org/10.5281/zenodo.7074790. This seems like a dataset ID. The context says it's the data supporting the study. So that's a dataset. Then, the second entry is a GitHub repository: https://github.com/shk11/KVIKOp-timizer. The context mentions the KVIK optimization routine, which is an algorithm or tool. But the problem is, the user's task is to identify datasets. The first entry is a dataset (the data from Zenodo), and the second entry refers to an algorithm or tool, which might not be a dataset but a methodology. \n\nWait, the first dataset ID is a dataset. The second one is a GitHub repository, which might be a dataset as well. But the user's potential_dataset_ids list includes both. However, the first entry's context mentions \"data that support the findings of this study\" which is a dataset. The second entry's context says \"KVIK optimization routine is openly available in the GitHub repository\", which might be an algorithm or tool, not a dataset. But maybe it's considered a dataset if it's a repository. However, the problem states that datasets are those with a persistent identifier. The first entry's DOI is a dataset ID, and the second entry's URL might not be a dataset ID. So the first entry is a dataset, the second is not. \n\nTherefore, the correct classification would be one dataset (the first) and possibly another if the second is considered a dataset. But according to the problem statement, the user's potential_dataset_ids include both. However, the first one is a dataset, and the second might not be. Wait, the second entry's context says \"the KVIK optimization routine is openly available in the GitHub repository\". If the GitHub repository is a dataset, then maybe it's a dataset. But the first entry's DOI is a dataset. So the answer would have one dataset. But maybe the second is a dataset. However, the problem says to use the given potential_dataset_ids. So the first entry is a dataset, and the second might not be. Therefore, the JSON would have that dataset classified as \"Primary\" because it's the data from the study. The second entry's context mentions an algorithm, which isn't a dataset but a tool. So the final JSON would include the first dataset_id and type \"Primary\".\n</think>"
        ],
        [
         "11",
         "10.1002_ece3.3985",
         "Article ID: 10.1002_ece3.3985\nText Content: Ecology and Evolution.\n2018;8:4525–4533.  ﻿\b   |  4525 www.ecolevol.org 1 | INTRODUCTION Polyandry, where females mate with two or more males, is tax-onomically widespread (Birkhead & Møller, 1998) despite poten-tial costs to females, such as wasted time and energy (Watson,  Arnqvist, & Stallmann, 1998), increased predation risk (Rowe, 1994),  exposure to disease (Thrall, Antonovics, & Dobson, 2000), and risk  of injuries (Crudgington & Siva-­Jothy, 2000).\nThe maintenance of  polyandry has been attributed to material (direct) and genetic (in-direct) benefits for females.\nDirect benefits include fertility assur-ance, provision of resources and parental care for the offspring  (Sheldon 1994).\nIndirect benefits include genetic incompatibility  avoidance, increased genetic diversity of offspring, and the en-hanced survival and reproductive success of offspring (Jennions  & Petrie, 2000; Neff & Pitcher, 2005; Tregenza & Wedell, 2000).  \nPolyandry may also occur in the absence of benefits to females in    Received: 21 January 2018 | Revised: 7 February 2018 | Accepted: 9 February 2018 DOI: 10.1002/ece3.3985 O R\nI G I N A L  R E S E A R C H High levels of polyandry, but limited evidence for multiple  paternity, in wild populations of the western rock lobster  (Panulirus cygnus) \nJacqueline Loo1 | Winn Jason Kennington1 | Simon de Lestang2 | Jason How2 |   Jonathan P. Evans1 This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium,  provided the original work is properly cited. \n© 2018 The Authors.\nEcology and Evolution published by John Wiley & Sons Ltd. 1Centre for Evolutionary Biology, School  of Biological Sciences, The University of  Western Australia, Crawley, WA, Australia 2Western Australian Fisheries and Marine  Research Labo.\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem step by step. First, I need to look at the given article ID and the text content to identify any dataset IDs. The article is about ecology and evolution, specifically discussing polyandry in the western rock lobster. The potential_dataset_ids list is empty here, which suggests that the article doesn't cite any datasets. \n\nWait, but maybe I should double-check. The text mentions \"data availability\" and \"data release\" terms. The DOI format is important here. The example given is a DOI like 10.1234/xxxx.1x1x-xx11. But in the provided text, there's no DOI. The only mentions are \"data availability\" and \"dataset\", but those are general terms. \n\nSince the potential_dataset_ids are empty, it means no datasets are referenced. Therefore, I should return a JSON object with \"dataset_id\" as \"Missing\" and \"type\" as \"Missing\" to indicate no datasets were found. \n\nI need to make sure I'm not missing any other potential dataset terms. The text doesn't mention any specific dataset IDs like EPI_ISL_10271777 or similar. The focus is on the study's context rather than citing datasets. So, no dataset IDs are present here. \n\nTherefore, the correct response is to return the specified JSON structure with both missing entries.\n</think>"
        ],
        [
         "12",
         "10.1002_ece3.4466",
         "Article ID: 10.1002_ece3.4466\nText Content: Ecology and Evolution.\n2018;8:9827–9833.  ﻿\b   |  9827 www.ecolevol.org 1 | INTRODUCTION The fundamental biological process of sex determination has been  under intense scientific research for decades, mainly because of its  profound implications on the phenotype of individuals and the for-mation of sex ratios in natural populations (West, Reece, & Sheldon,  2002).\nIt is therefore a process of major evolutionary significance.  \nThe mechanisms of sex determination have, for a long time, been  classified discretely as either genetic (GSD) or environmental (ESD).  \nHowever, researchers are now in agreement that genotypic and en-vironmental sex determination does not necessarily constitute a di-chotomy, but rather two extremes of a continuum (Sarre, Georges, &  Quinn, 2004). \nSex allocation studies on birds and mammals are numerous  (reviewed in Wapstra et al., 2007) and show that the majority of  these species have genetic sex determination.\nReptiles (squamates    Received: 11 August 2017 | Revised: 13 June 2018 | Accepted: 6 July 2018 DOI: 10.1002/ece3.4466 O R\nI G I N A L  R E S E A R C H Incubation temperature and parental identity determine sex in  the Australian agamid lizard Ctenophorus pictus Alexander Hansson  | Mats Olsson This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium,  provided the original work is properly cited. \n© 2018 The Authors.\nEcology and Evolution published by John Wiley & Sons Ltd. Department of Biological and Environmental  Sciences, University of Gothenburg,  Gothenburg, Sweden Correspondence Alexander Hansson, Department of  Biological and Environmental Sciences,  University of Gothenburg, Gothenburg,  Sweden. \nEmail: ha.\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's start by looking at the article ID provided, which is 10.1002/ece3.4466. The text content mentions \"Ecology and Evolution\" in the title and abstract, and then there's a section about sex determination in birds and mammals, as well as reptiles. \n\nThe user's task is to identify citations of datasets used in this research. The text doesn't mention any dataset IDs or DOI references. The potential_dataset_ids list is empty. So, there are no dataset citations here. \n\nFor the classification, since there are no dataset_ids found, I should return the JSON with the Missing entries for both dataset_id and type. The structure requires a single JSON object if no datasets are found. \n\nI need to make sure I'm not missing any parts of the text. The abstract talks about sex determination, but nothing about datasets. The potential_dataset_ids are empty, so no further action is needed. \n\nYes, that's all. The answer should reflect that no datasets were cited.\n</think>"
        ],
        [
         "13",
         "10.1002_ece3.5260",
         "Article ID: 10.1002_ece3.5260\nText Content: Ecology and Evolution.\n2019;9:6933–6948.  \n﻿\b   |  6933 www.ecolevol.org 1 | INTRODUCTION \nThe phylogenomics revolution is underway with targeted capture  and enrichment approaches (Mamanova et al., 2010; McCormack,  Hird, Zellmer, Carstens, & Brumfield, 2013) proving an effective  and improved method for phylogenetic studies relative to multilo‐ cus Sanger techniques (Blaimer et al., 2015).\nAmong the genomic  components that have been targeted, ultraconserved elements  (UCEs sensu Faircloth et al., 2012) have demonstrated their util‐ ity in reconstructing phylogenies across diverse vertebrate\n[e.g.,  fishes (Faircloth, Sorenson, Santini, & Alfaro, 2013), birds (Hosner,  Tobias, Braun, & Kimball, 2017; McCormack et al., 2013), reptiles  (Crawford et al., 2012; Grismer et al., 2016; Streicher & Wiens,  2017), amphibians (Alexander et al., 2017; Streicher et al., 2018),    Received: 8 April 2019 | Revised: 26 April 2019 | Accepted: 28 April 2019 DOI: 10.1002/ece3.5260   O R\nI G I N A L  R E S E A R C H Ultraconserved element (UCE) probe set design: Base genome  and initial design parameters critical for optimization Grey T. Gustafson1,2  |   Alana Alexander2,3  |   John S. Sproul4,5  |   James M. Pflug4\n|   David R. Maddison4  |   Andrew E. Z. Short1,2 This is an open access article under the terms of the Creat​ive Commo​ns Attri​bution License, which permits use, distribution and reproduction in any medium,  provided the original work is properly cited. \n© 2019 The Authors.\nEcology and Evolution published by John Wiley & Sons Ltd. 1Department of Ecology and Evolutionary  Biology, University of Kansas, Lawrence,  Kansas 2Biodiversity Institute, University of Kansas,  Lawrence, Kansas 3Department of Anatomy, School of  Biomedical Sciences, University of Otago,  Dunedin, New Zealand 4Department of Integrative Biology, Oregon  State University, Corvallis, Oregon 5Department of Biology, Univ.\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's start by looking at the article ID provided, which is 10.1002/ece3.5260. The text mentions \"Ultraconserved element (UCE) probe set design\" in the context of the study. The abstract and potential_dataset_ids list seems to be empty, which might mean there are no dataset IDs listed. However, I need to check if there's any mention of datasets in the text.\n\nLooking at the text: The authors are named Grey T. Gustafson, Alana Alexander, John S. Sproul, and James M. Pflug. Then there's David R. Maddison and Andrew E. Z. Short. The abstract talks about genomic components targeted, specifically UCEs, and mentions the design of UCE probe sets. The DOI provided is for the paper, not a dataset. The potential_dataset_ids list is empty here. \n\nSince there's no mention of any dataset IDs in the provided text, I should return a JSON object with \"dataset_id\": \"Missing\" and \"type\": \"Missing\" for this article. The other datasets mentioned in the abstract are not part of the potential_dataset_ids provided. So the correct response is to indicate no datasets are found.\n</think>"
        ],
        [
         "14",
         "10.1002_ece3.5395",
         "Article ID: 10.1002_ece3.5395\nText Content: Ecology and Evolution.\n2019;9:8945–8952.  \n﻿\b   |  8945 www.ecolevol.org 1 | INTRODUCTION Migration is a ubiquitous phenomenon that has evolved as an ad‐ aptation to seasonally changing environments (Newton, 2008).  \nBut within the same species, or even population, we often observe  tremendous variation in specific migration features (Chapman,  Brönmark, Nilsson, & Hansson, 2011;\nFlack et al., 2016).\nThis may  be the outcome of trade‐offs between the costs and benefits of  different migration strategies, leading to differences in individual  fitness, and subsequently influencing demographic dynamics (Lok,  Overdijk, & Piersma, 2015; Palacín, Alonso, Martín, & Alonso,  2017).\nLife in the Anthropocene is challenging many ancient evo‐ lutionary adaptations including migration.\nSelection pressures  change quickly, favoring individuals that are better adapted to  human‐modified landscapes (Otto, 2018).\nThese human‐shaped  selection pressures are caused by hunting, poisoning, electrocu‐ tion, food subsidies or global habitat and climate change, and many  more (Oro, Genovart, Tavecchia, Fowler, & Martínez‐Abraín, 2013;    Received: 18 April 2019 | Revised: 28 May 2019 | Accepted: 4 June 2019 DOI: 10.1002/ece3.5395   O R\nI G I N A L  R E S E A R C H “Closer‐to‐home” strategy benefits juvenile survival in a long‐ distance migratory bird Yachang Cheng1,2  |   Wolfgang Fiedler1,2\n|   Martin Wikelski1,2,3 |   Andrea Flack1,2,3 \nThis is an open access article under the terms of the Creat​ive Commo​ns Attri​bution License, which permits use, distribution and reproduction in any medium,  provided the original work is properly cited. \n© 2019 The Authors.\nEcology and Evolution published by John Wiley & Sons Ltd. 1Department of Migration, Max Planck  Institute of Animal Behavior, Radolfzell,  Germany 2Department of Biology, University of  Konstanz, Konstanz, Germany 3Centre for the Advanced Study o.\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem step by step. The user provided an article with ID 10.1002/ece3.5395 and a text content. They also have a list of potential_dataset_ids that I need to process. \n\nFirst, I need to check if any dataset IDs are mentioned in the text. The text starts with \"Ecology and Evolution. 2019;9:8945–8952.\" Then there's an abstract about migration and selection pressures. The abstract mentions \"Yachang Cheng\" and \"Wolfgang Fiedler\" and \"Martin Wikelski\" and \"Andrea Flack\". \n\nLooking at the potential_dataset_ids list, there are no entries with dataset IDs. The user's list is empty, so according to the instructions, if there are no dataset IDs, I should return a JSON object with \"Missing\" for both dataset_id and type. \n\nWait, but maybe I missed something? Let me double-check. The abstract doesn't mention any datasets or their identifiers. The potential_dataset_ids are empty. So the answer should be the JSON object as specified. No dataset_ids found, so the result is just that single object.\n</think>"
        ],
        [
         "15",
         "10.1002_ece3.6144",
         "Article ID: 10.1002_ece3.6144\nText Content: Ecology and Evolution.\n2020;10:3463–3476.  ﻿\b   |  3463 www.ecolevol.org 1 | INTRODUCTION Marine ecosystems can change rapidly in response to both natural  and anthropogenic changes in the environment.\nMonitoring changes  in the abundance and distribution of marine organisms is a chal-lenging but critical component of conservation efforts and natural  resource management.\nHuman impacts such as overfishing, coastal  development, and pollution can rapidly increase stress on ecosys-tems resulting in an urgent need to develop efficient methods for  monitoring spatial and temporal dynamics of biodiversity (Beng  et al., 2016). \nFisheries management has long used ichthyoplankton sur-veys as a component of monitoring because sampling the early  life stages of fish reflects the reproductive activities of fish    Received: 17 July 2019 | Revised:\n31 January 2020 |\nAccepted: 4 February 2020 DOI: 10.1002/ece3.6144   O R\nI G I N A L  R E S E A R C H Efficacy of metabarcoding for identification of fish eggs  evaluated with mock communities Elena M. Duke  |   Ronald S. Burton \nThis is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium,  provided the original work is properly cited. \n© 2020 The Authors.\nEcology and Evolution published by John Wiley & Sons Ltd. Marine Biology Research Division, Scripps  Institution of Oceanography, University of  California, San Diego, La Jolla, California Correspondence Elena M. Duke, Marine Biology  Research Division, Scripps Institution of  Oceanography, University of California, San  Diego, La Jolla, CA. \nEmail: emduke@ucsd.edu \nAbstract There is urgent need for effective and efficient monitoring of marine fish popula-tions.\nM.\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem step by step. The user provided an article ID and a text content, along with potential dataset IDs that need to be analyzed. The goal is to identify citations of datasets, classify their types as Primary or Secondary, and present the results in JSON.\n\nFirst, I need to look through the text of the paper to find any mentions of datasets. The abstract mentions \"fish eggs\" and \"metabarcoding,\" which are terms related to biological studies. Now, looking at the potential_dataset_ids provided, there are no entries related to datasets. The user's potential_dataset_ids list is empty. \n\nSince there are no dataset IDs mentioned in the abstract, I should check if there are any DOI or Accession IDs. The text doesn't include any DOI or Accession IDs, so neither can be used. Therefore, the dataset_id is \"Missing,\" and the type is also \"Missing\" because no datasets are referenced. \n\nI need to make sure I'm not missing any possible references. The abstract talks about monitoring marine organisms, which might be data sources, but the potential_dataset_ids list is empty. So, the final JSON should reflect that no datasets were found.\n</think>"
        ],
        [
         "16",
         "10.1002_ece3.6303",
         "Article ID: 10.1002_ece3.6303\nText Content: Ecology and Evolution.\n2020;10:5617–5630.  ﻿\b   |  5617 www.ecolevol.org   Received: 26 August 2019 | Revised: 25 March 2020 | Accepted: 30 March 2020 DOI: 10.1002/ece3.6303   O R\nI G I N A L  R E S E A R C H Can polyploidy confer invasive plants with a wider climatic  tolerance?\nA test using Solidago canadensis Jizhong Wan1,2 |   Ayub M. O. Oduor1,3  |   \nRobin Pouteau1  |   Beilei Wang1 |    Luxi Chen1 |   \nBeifen Yang1 |   Feihai Yu1 |   \nJunmin Li1 \nThis is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium,  provided the original work is properly cited. \n© 2020 The Authors.\nEcology and Evolution published by John Wiley & Sons Ltd. 1Zhejing Provincial Key Laboratory of Plant  Evolutionary Ecology and Conservation,  Taizhou University, Taizhou, P.R. China 2State Key Laboratory of Plateau Ecology  and Agriculture, Qinghai University, Xining,  \nP.R. China 3Department of Applied Biology, Technical  University of Kenya, Nairobi, Kenya Correspondence Junmin Li, Zhejiang Provincial Key  Laboratory of Plant Evolutionary Ecology  and Conservation, Taizhou 318000, China. \nEmail: lijmtzc@126.com Funding information Ten Thousand Talent Program of Zhejiang  Province; National Natural Science  Foundation of China, Grant/Award Number:  31270461; National Key Research and  Development Program, Grant/Award  Number: 2016YFC1201100 Abstract Polyploidy can cause variation in plant functional traits and thereby generate indi-viduals that can adapt to fluctuating environments and exploit new environments.  \nHowever, few empirical studies have tested for an association between ploidy level  and climatic tolerance of invasive cytotypes relative to conspecific native-range cy-totypes.\nHere, we used an invasive plant Solidago canadensis to test whether invas.\n potential_dataset_ids: [{\"dataset_ids\": ['MN107084', 'MN107151'], context: \"(NCBI) database (accession numbers: MN107084–MN107151,  \"},{\"dataset_ids\": ['MN107151', 'MN335287', 'MN335290'], context: \"MN107084–MN107151, MN335287–MN335290).\nThe data that  \"},]\n",
         "[{'dataset_id': 'MN107084', 'type': 'Primary'}, {'dataset_id': 'MN107151', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem step by step. The user provided an article ID and a text content, along with potential dataset_ids. My job is to identify citations of datasets used in the research, classify their types as Primary or Secondary, and present the results in JSON.\n\nFirst, I need to look through the text of the paper to find any dataset IDs mentioned. The text mentions \"Solidago canadensis\" and \"MN107084, MN107151\" in the context of the dataset. The context says they are from the NCBI database with accession numbers. So, MN107084 and MN107151 are the dataset IDs here.\n\nNow, checking the dataset_ids list provided, the first entry is MN107084 and MN107151. The second entry includes MN107151, MN335287, MN335290. So, the first two dataset IDs are from the NCBI database. \n\nNext, I need to determine if these datasets are primary or secondary. Primary would be raw or processed data from the study, while secondary would be data reused or from existing records. Since the study is testing ploidy and climatic tolerance, and the datasets are related to the plant species, it's likely that these are raw data used in the study. Therefore, both MN107084 and MN107151 would be classified as Primary.\n\nI should also check if there are any other dataset IDs in the list. The third entry mentions MN335287 and MN335290, but they are part of the same NCBI database. However, the user's dataset IDs list includes these, but the first two are already identified. The problem states that if multiple citations of the same dataset_id exist, process only the first one. Here, both MN107084 and MN107151 are from the same dataset, so they are considered as two separate citations for the same dataset. \n\nSo, the final JSON should include both dataset IDs as Primary. I need to make sure there are no other dataset IDs mentioned and that the classification is accurate based on the study context.\n</think>"
        ],
        [
         "17",
         "10.1002_ece3.6784",
         "Article ID: 10.1002_ece3.6784\nText Content: Ecology and Evolution.\n2020;10:11467–11487.  ﻿\b   |  11467 www.ecolevol.org   Received: 25 June 2020 | Revised: 25 August 2020 | Accepted: 27 August 2020 DOI: 10.1002/ece3.6784   O R\nI G I N A L  R E S E A R C H Ecomorphology of the pectoral girdle in anurans (Amphibia,  Anura): Shape diversity and biomechanical considerations Karolin Engelkes1  \n|   Lena Kath1 |   Thomas Kleinteich2\n|   Jörg U. Hammel3,4  |    André Beerlink5 |   \nAlexander Haas1 \nThis is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium,  provided the original work is properly cited. \n© 2020 The Authors.\nEcology and Evolution published by John Wiley & Sons Ltd 1Center of Natural History (CeNak),  Universität Hamburg, Hamburg, Germany 2TPW Prüfzentrum GmbH, Neuss, Germany 3Institute of Materials Research, Helmholtz-Zentrum Geesthacht, Geesthacht, Germany 4Institut für Zoologie und  Evolutionsforschung mit Phyletischem  Museum, Ernst-Hackel-Haus und  Biologiedidaktik, Friedrich-Schiller-Universität Jena, Jena, Germany \n5YXLON International GmbH, Hamburg,  Germany Correspondence Karolin Engelkes, Center of Natural History  (CeNak), Universität Hamburg, Martin-Luther-King-Platz 3, 20146 Hamburg,  Germany. \nEmail: karolin.engelkes@uni-hamburg.de Funding information Wilhelm-Peters-Fonds of the Deutsche  Gesellschaft für Herpetologie und  Terrarienkunde e.V (DGHT); Deutsche  Forschungsgemeinschaft, Grant/Award  Number: 387723284 Abstract Frogs and toads (Lissamphibia: Anura) show a diversity of locomotor modes that  allow them to inhabit a wide range of habitats.\nThe different locomotor modes are  likely to be linked to anatomical specializations of the skeleton within the typical frog  Bauplan.\nWhile such anatomical adaptations of the hind lim.\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's start by looking at the article ID provided, which is 10.1002/ece3.6784. The text content is about the ecological and evolutionary study of pectoral girdles in anurans. The abstract mentions Karolin Engelkes and others, and the paper is titled \"comorphology of the pectoral girdle in anurans... shape diversity and biomechanical considerations.\" \n\nNow, the task is to identify dataset_ids. The article mentions \"data release\" and \"dataset\" in the abstract. But looking at the provided text, there's no specific mention of a dataset_id like a DOI or Accession ID. The abstract states that the study uses data from various sources, but there's no direct citation to a dataset. \n\nLooking at the potential_dataset_ids list, there are no entries that match the content. The user provided an empty list, so I need to return a JSON object with \"Missing\" for both dataset_id and type. Since there are no dataset_ids cited, the result should reflect that.\n</think>"
        ],
        [
         "18",
         "10.1002_ece3.961",
         "Article ID: 10.1002_ece3.961\nText Content: The effects of insects, nutrients, and plant invasion on community structure and function above- and belowground Phoebe Wright1, Melissa A. Cregger1,2, Lara Souza1,3, Nathan J. Sanders1,4 & Aim�ee T. Classen1,4 1Department of Ecology and Evolutionary Biology, University of Tennessee, 569 Dabney Hall, Knoxville, Tennessee 37996 2Institute for Genomic Biology, University of Illinois, 1206 W Gregory Rm 3405, Urbana, Illinois 61801 3Oklahoma Biological Survey and Department Microbiology and Plant Biology, University of Oklahoma, 111 E Chesapeake St., Norman, Oklahoma 73019 4Center for Macroecology Evolution and Climate, Natural History Museum of Denmark, University of Copenhagen, Copenhagen DK-2100, Denmark Keywords aboveground biomass, bacteria, fungi, insect, invasive plant, nitrogen-fixer, nutrient amendment, old-field ecosystem, soil enzyme activity. \nCorrespondence Aim�ee T. Classen, Center for Macroecology, Evolution and Climate, Natural History Museum of Denmark, University of Copenhagen, DK-2100 Copenhagen, Denmark.\nTel: +45 35-32-22-22; Fax: +45 46-82-74-83; E-mail: atclassen@gmail.com Funding Information The Department of Ecology and Evolutionary Biology and the Office of Research at The University of Tennessee funded this work. \nReceived:\n21 November 2013; Revised: 2 December 2013; Accepted: 3 December 2013 Ecology and Evolution 2014; 4(6): 732– 742 doi: 10.1002/ece3.961 Abstract Soil nutrient availability, invasive plants, and insect presence can directly alter ecosystem structure and function, but less is known about how these factors may interact.\nIn this 6-year study in an old-field ecosystem, we manipulated insect abundance (reduced and control), the propagule pressure of an invasive nitrogen-fixing plant (propagules added and control), and soil nutrient avail-ability (nitrogen added, nitrogen reduced and control) in a fully crossed, com-pletely randomized plot design.\nWe found that nutrient amendment and, occasionally, insect abundance interacted with the propagule pressure of an invasive plant to alter above- and belowground structure and function at our site.\nNot surprisingly, nutrient amendment had a direct effect on aboveground biomass and soil nutrient mineralization.\nThe introduction of invasi.\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem step by step. The user provided an article ID and a text content, along with potential dataset_ids that need to be analyzed. The goal is to identify citations of datasets, classify their type as Primary or Secondary, and present the results in a JSON format.\n\nFirst, I need to look through the text content for any mentions of datasets. The article is about ecological studies, specifically focusing on soil nutrients, invasive plants, and insects. The abstract mentions \"nutrient amendment\" and \"invasive plant\", which are terms that might correspond to dataset IDs.\n\nLooking at the potential_dataset_ids provided, none of them seem to match the terms used in the abstract. The example dataset IDs are like DOI or Accession IDs, but the abstract talks about ecological factors and not datasets. The text doesn't mention any specific dataset identifiers like \"EPI_ISL_\" or \"SAMN_...\", so there are no dataset citations here.\n\nNext, I need to check if there are any other terms in the text that could indicate dataset usage. The abstract states \"nutrient availability\" and \"soil enzyme activity\", which are more about environmental factors rather than datasets. The keywords don't point to specific datasets, so there's no indication of dataset citations.\n\nSince there are no dataset_ids mentioned in the text, I should return a JSON object with \"dataset_id\" as \"Missing\" and \"type\" as \"Missing\". The user's instructions specify that if no dataset citations are found, return a single JSON object with both fields missing. That makes sense here because the text doesn't reference any datasets.\n</think>"
        ],
        [
         "19",
         "10.1002_ece3.9627",
         "Article ID: 10.1002_ece3.9627\nText Content: Ecology and Evolution.\n2022;12:e9627.  \n﻿    | 1 of 15 https://doi.org/10.1002/ece3.9627 www.ecolevol.org Received: 5 June 2022 | Revised: 14 October 2022 | Accepted: 20 November 2022 DOI: 10.1002/ece3.9627   R E S E A R C H  A R T I C L E Inferring predator–­prey interactions from camera traps:  A Bayesian co-­abundance modeling approach Zachary Amir1,2  |   Adia Sovie3  |   Matthew Scott Luskin1,2 This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium,  provided the original work is properly cited. \n© 2022 The Authors.\nEcology and Evolution published by John Wiley & Sons Ltd. 1School of Biological Sciences, University  of Queensland, St. Lucia, Queensland,  Australia 2Centre for Biodiversity and Conservation  Science, University of Queensland, St.  Lucia, Queensland, Australia 3Department of Fisheries and Wildlife,  Michigan State University, East Lansing,  Michigan, USA Correspondence Zachary Amir, School of Biological  Sciences, University of Queensland,  534 Goddard Hall, St. Lucia 4067, QLD,  Australia. \nEmail: z.amir@uq.edu.au Abstract Predator–­prey dynamics are a fundamental part of ecology, but directly studying inter-actions has proven difficult.\nThe proliferation of camera trapping has enabled the col-lection of large datasets on wildlife, but researchers face hurdles inferring interactions  from observational data.\nRecent advances in hierarchical co-­abundance models infer  species interactions while accounting for two species' detection probabilities, shared  responses to environmental covariates, and propagate uncertainty throughout the  entire modeling process.\nHowever, current approaches remain unsuitable for interact-ing specie.\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem step by step. The user provided an article ID and a text content, along with potential dataset IDs that are associated with the paper. My job is to identify citations of datasets, classify their type, and return the results in a specific JSON format.\n\nFirst, I need to look at the text content. The article is about inferring predator-prey interactions using Bayesian co-abundance modeling. The DOI given is 10.1002/ece3.9627. The first thing to check is if this DOI is a dataset ID. The format for DOIs is usually like https://doi.org/[prefix]/[suffix], where the prefix starts with 10. and the suffix includes letters, numbers, or special characters. The DOI here is 10.1002/ece3.9627. Let me check the prefix: 10.1002/ece3.9627. The prefix is 10.1002, which matches the format, but the suffix is 9627. The DOI is for a paper, not a dataset. So this is not a dataset ID. \n\nNext, the potential dataset IDs list is empty. The user provided \"potential_dataset_ids: []\". So there are no dataset IDs mentioned in the text. Therefore, there are no citations of datasets to process. \n\nWait, but maybe I'm missing something. Let me double-check the text again. The abstract mentions Bayesian co-abundance modeling, which is a statistical method. The DOI is for a paper, not a dataset. The dataset IDs provided in the potential list are all empty. So according to the given data, there are no dataset IDs to classify. Therefore, the JSON should have a single entry with dataset_id as \"Missing\" and type as \"Missing\".\n\nI need to make sure that the user's instructions are followed. The first task is to identify citations of datasets. Since there are none, the second and third tasks are to return the JSON accordingly. The answer should be a single JSON object as specified.\n</think>"
        ],
        [
         "20",
         "10.1002_ecs2.1280",
         "Article ID: 10.1002_ecs2.1280\nText Content: Abstract.   \nUnderstanding the impact of herbivory on plant populations is a fundamental goal of ecology.  \nDamage to individual plants can be visually striking and affect the fates of individuals, but these impacts  do not necessarily translate into population-­level differences in vital rates (survival, growth, or fecundity)  or population growth rates.\nIn biological control of weeds, quantitative assessments of population-­level  impacts of released agents on both target invasive plants and native, nontarget plants are needed to inform  evaluations of the benefits and risks of releasing agents into new regions.\nHere we present a 3-­yr exper-imental demographic field study using the European root-­feeding biocontrol weevil, Mogulones crucifer,  first released in Canada in 1997 to control the invasive weed Cynoglossum officinale (Boraginaceae).\nMogu-lones crucifer is an effective “search and destroy” agent in Canada, but sporadically feeds, oviposits, and  develops on native nontarget Boraginaceae.\nWe investigated the population-­level impacts of this biocon-trol insect on its target weed and a native nontarget plant, Hackelia micrantha (Boraginaceae), by releasing  large numbers of weevils into naturally occurring patches of H. micrantha growing isolated from or inter-spersed with C. officinale.\nWe followed the fates of individual plants on release and nonrelease (control)  sites for two transition years, developed matrix models to project population growth rates (λ) for each  plant species, and examined the contributions from differences in vital rates to changes in λ using life table  response experiments (LTRE).\nIn contrast to studies of the insect–plant interaction in its native range, as a  biocontrol agent, M. crucifer increased mortality of C. officinale rosettes in the year immediately following  release, depressing the weed's λ to below the population replacement level.\nHowever, λ for H. micrantha  was never depressed below the replacement level, and any differences between release and nonrelease  sites in the nontarget could not be explained by significant contributions from vital rates in the LTRE.\nThis  study is the first to simultaneously and experimentally examine target and nontarget population-­level im-pacts of a weed biocontrol insect in the field, and supports the theoretical prediction that plant life history  characteristics and uneven herbivore host preferences can interact to produce differences in population-­ level impacts between target and nontarget plant species. .\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided an article ID and a text content. They want me to identify citations of datasets used in the research and classify each dataset as Primary or Secondary. The potential_dataset_ids are given, but in this case, the potential_dataset_ids list is empty. Wait, no, looking back, the user's potential_dataset_ids are actually empty, but maybe I need to check again. Wait, the user's potential_dataset_ids are listed as an empty array. Hmm, but maybe there's a typo or mistake here. Let me check again.\n\nThe user wrote: \"potential_dataset_ids: []\" but in the problem statement, the potential_dataset_ids are given as a list. Wait, looking at the original problem, the user says: \"Within the text of the paper, you are given an abstract and a list of potential_dataset_ids that contains dataset_ids and their associated context within the paper.\" So the potential_dataset_ids here are empty. But the user is asking to process this. So if the potential_dataset_ids are empty, then according to the instructions, if there are no dataset_ids, return a single JSON object with \"dataset_id\": \"Missing\" and \"type\": \"Missing\". But maybe there's a misunderstanding here. Let me check the article again.\n\nThe article is about a field study on a biocontrol weevil, Mogulones crucifer, affecting weeds and native plants. The text mentions \"data availability\" and \"dataset\" but there are no specific dataset IDs mentioned. The potential_dataset_ids list is empty. So according to the instructions, if there are no dataset_ids, return the JSON as specified. Therefore, the correct response would be the empty array with the missing dataset IDs. But wait, maybe I'm missing something. Let me check again.\n\nThe abstract mentions \"data availability\" and \"dataset\", but there's no specific dataset_id in the potential_dataset_ids. So the answer should be the empty array. Therefore, the JSON would be:\n\n```json\n[\n    {\n        \"dataset_id\": \"Missing\",\n        \"type\": \"Missing\"\n    }\n]\n```\nBut the user's potential_dataset_ids list is empty. So I need to follow the instructions exactly. The user's potential_dataset_ids is empty, so no dataset_ids found. Therefore, the JSON is as above.\n</think>"
        ],
        [
         "21",
         "10.1002_ecs2.4619",
         "Article ID: 10.1002_ecs2.4619\nText Content: A R T I C L E M a c r o s y s t e m s E c o l o g y Interactions between temperature and predation impact insect emergence in alpine lakes Caroline H. Owens1 | Michelle J. Lee1 \n| Melissa Grim1,2 | John Schroeder1 | Hillary S. Young1 1Department of Ecology Evolution and Marine Biology, University of California, Santa Barbara, Santa Barbara, California, USA 2Department of Environmental Science and Policy, University of California, Davis, Davis, California, USA Correspondence Caroline H. Owens Email: owens@ucsb.edu Funding information National Science Foundation; University of California Natural Reserve System; University of California, Santa Barbara Handling Editor:\nManuel T. Lerdau Abstract While insect populations are simultaneously threatened by many local- and global-scale stressors, the interacting effects of these factors remain poorly understood.\nThese interactions between stressors, whether additive or antagonis-tic, may have profound effects on our assumptions about and predictions for any given system.\nHere we address this gap by exploring the interactive effects of introduced predators and elevated temperatures on emerging aquatic insects across alpine lakes.\nUsing a crossed factorial field survey, we examine whether warmer temperatures either directly affect the magnitude or diversity of insect emergence, or mediate the impacts of predation via interactions with predator presence.\nBased on data from more than 11,000 insects collected across a two-year period, we find a 71% reduction in insect emergence associated with an additional 3.6�C of water temperature in oligotrophic alpine lakes.\nWhile our work confirms pri.\n potential_dataset_ids: [{\"dataset_ids\": ['https://doi.org/10.25349/D9QW5X.'], context: \"DATA AVAILABILITY STATEMENT Data and novel code (Owens et al., 2023) are available from Dryad: https://doi.org/10.25349/D9QW5X. \"},{\"dataset_ids\": ['https://doi.org/10.1002/2015GL066235.'], context: \"g/package=vegan. \nO’Reilly, C. M., S. Sharma, D. K. Gray, S. E. Hampton, J. S. Read, R. J. Rowley, P. Schneider, et al. 2015.\n“Rapid and Highly Variable Warming of Lake Surface Waters around the Globe.” Geophysical Research Letters 42(24): 10773–81.\nhttps://doi.org/10.1002/2015GL066235. \nOwens, C., M. Lee, M. Grim, J. Schroeder, and H. Young. 2023. \n“Data for: Interactions between Temperature and Predation Impact Insect Emergence in Alpine Lakes.”\nDryad.\nDataset. \nhttps://doi.org/10.25349/D9QW5X. Piovia-Scott, J., S. Sadro, R. A. K\"},{\"dataset_ids\": ['https://doi.org/10.25349/D9QW5X.'], context: \"arch Letters 42(24): 10773–81.\nhttps://doi.org/10.1002/2015GL066235. \nOwens, C., M. Lee, M. Grim, J. Schroeder, and H. Young. 2023. \n“Data for: Interactions between Temperature and Predation Impact Insect Emergence in Alpine Lakes.”\nDryad.\nDataset. \nhttps://doi.org/10.25349/D9QW5X. Piovia-Scott, J., S. Sadro, R. A. Knapp, J. Sickman, K. L. Pope, and S. Chandra.\n2016.\n“Variation in Reciprocal Subsidies between Lakes and Land: Perspectives from the Mountains of \"},]\n",
         "[{'dataset_id': 'https://doi.org/10.25349/D9QW5X.', 'type': 'Primary'}, {'dataset_id': 'https://doi.org/10.25349/D9QW5X.', 'type': 'Primary'}, {'dataset_id': 'https://doi.org/10.25349/D9QW5X.', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided an article with an ID and a text content. They also have a list of potential dataset_ids and their contexts. My task is to identify citations of datasets, classify their types, and present the results in JSON.\n\nFirst, I need to look through the text content. The abstract mentions data availability and a dataset from Dryad. The DOI is given as \"https://doi.org/10.25349/D9QW5X.\" The first dataset ID is from Dryad. The context says it's the dataset available. So that's one dataset, type \"Primary\" since it's the data used.\n\nLooking at the other dataset IDs, there are two more. The second one is from a different DOI, which might be another dataset. But the context says \"g/package=vegan\" which doesn't seem related to the study. The third one is from a different context, but the user might have included duplicates. However, the user mentioned that if multiple citations of the same dataset_id, process the first. So maybe there are two dataset IDs here, but the context varies. \n\nWait, the third dataset's context is \"Variation in Reciprocal Subsidies between Lakes and Land: Perspectives from the Mountains of \" which seems unrelated. The fourth dataset's context is \"Piovia-Scott, J., S. Sadro, R. A. Knapp, J. Sickman, K. L. Pope, and S. Chandra. 2016.\" which is also unrelated. So maybe only the first dataset ID from Dryad is relevant. \n\nBut the user's potential_dataset_ids have multiple entries. The first one is from Dryad, the second from a different DOI, third from another, and fourth from another. However, the contexts vary. The key is to find the dataset IDs that are actually cited in the paper. The first dataset ID is clearly a citation. The others might be from different sources or duplicates. \n\nSince the user wants to process each dataset ID as per their context, even if the context is different, but the dataset_id is the same. So the first dataset_id is from Dryad, and the other entries might be duplicates. However, the user's task says to process the first citation of each dataset_id. So even if there are multiple entries, only the first one is considered. \n\nTherefore, the JSON should include the first dataset_id from Dryad as \"Primary\" and maybe the others if they are also present, but the user's potential_dataset_ids list has multiple entries. However, the user's instruction says to process the first citation of each dataset_id. So perhaps the first entry in the potential_dataset_ids list is the correct one. \n\nWait, looking back at the user's potential_dataset_ids, the first entry has the dataset_id as \"https://doi.org/10.25349/D9QW5X.\" and the context as \"DATA AVAILABILITY STATEMENT Data and novel code (Owens et al., 2023) are available from Dryad: ...\". Then the next entry is \"https://doi.org/10.10.25349/D9QW5X.\" but that might be a typo. The third entry is \"https://doi.org/10.25349/D9QW5X.\" again. So all these are the same dataset_id. So even though the context varies, the dataset_id is the same. \n\nTherefore, the correct classification would be that the first dataset_id (the same one) is Primary, and others might be duplicates. However, the user's instruction says to process the first citation of each dataset_id. Since all three potential_dataset_ids have the same dataset_id, maybe all three are considered. But the user's task says to process each dataset_id once. \n\nWait, the user's potential_dataset_ids list has multiple entries with the same dataset_id. So for each of those entries, even if the context is different, they all refer to the same dataset. Therefore, all three entries would be considered, but the user's task says to process the first citation. So maybe the answer includes all three, but the user's JSON structure needs to have each dataset_id once. However, since all three are the same, perhaps only the first one is included. \n\nBut looking at the example, the user's JSON structure expects an array of objects. So if there are multiple dataset_ids, each with their own type, but all are the same, then the array would have all three. However, the user's potential_dataset_ids list has multiple entries. \n\nWait, the user's potential_dataset_ids list includes three entries, all with the same dataset_id. Therefore, the correct classification would be that all three are \"Primary\" or \"Secondary\". But since they are the same dataset, the type is the same. But the user's task says to classify each dataset as Primary or Secondary. So if there are multiple citations of the same dataset_id, they all have the same type. Therefore, the JSON array would have all three entries. \n\nBut the user's example response shows that even if there are multiple entries, they are included. So the final JSON would include all three entries. However, the user's original potential_dataset_ids list might have duplicates. \n\nIn any case, based on the given data, the first dataset_id is the correct one, and the others are duplicates. Therefore, the JSON array should include all three entries with \"Primary\" as the type.\n</think>"
        ],
        [
         "22",
         "10.1002_ejic.201900904",
         "Article ID: 10.1002_ejic.201900904\nText Content: Abstract: Chemical properties and biological activity of Au(III) compounds obtained from dichlorido[2-(dimethylaminometh-yl)phenyl-C1,N]gold(III),\n[Au(damp-C1,N)Cl2], and halogenated, potentially tridentate thiosemicarbazones have been studied. \nThe results of this work show that the complexation of the halo-genated thiosemicarbazones with Au(III) enhances their stability against hydrolysis and retains or enhances their anti-parasitic activity.\nFluorination in the periphery of the ligands has expect-.\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's start by looking at the provided article and the potential_dataset_ids. The article ID is 10.1002_ejic.201900904, which seems like a journal article. The text content is about studying chemical properties and biological activity of Au(III) compounds, specifically those with different ligands. \n\nFirst, I need to check if there are any dataset_ids mentioned. The user provided a list of potential_dataset_ids, but in the given text, there's no mention of any dataset IDs. The abstract talks about the compounds but doesn't refer to any specific datasets. \n\nTask 1: Identify citations of datasets. The text doesn't use any dataset terms like \"dataset\", \"repository\", \"data source\", etc. So there are no dataset IDs cited here. \n\nTask 2: Classify the type of each dataset. Since there are no dataset IDs mentioned, both \"Primary\" and \"Secondary\" are missing for each dataset. \n\nTask 3: Return the results in JSON. Since there are no dataset IDs, the JSON object should have \"dataset_id\": \"Missing\" and \"type\": \"Missing\". \n\nWait, but maybe I should check again. The abstract mentions \"dichlorido[2-(dimethylaminometh-yl)phenyl-C1,N]gold(III), [Au(damp-C1,N)Cl2], and halogenated, potentially tridentate thiosemicarbazones\". These are compounds, not datasets. So no dataset IDs are referenced here. \n\nTherefore, the correct JSON would be an empty array with the two missing fields. But according to the problem statement, if there are no dataset_ids, return a single JSON object with the structure given. So the answer should be the empty array.\n</think>"
        ],
        [
         "23",
         "10.1002_ejoc.202000139",
         "Article ID: 10.1002_ejoc.202000139\nText Content: Abstract: Studies on gold(III) coordination of a series of pre-pared polydentate pyridine and quinoline based ligands are re-ported.\nCharacterization (1H, 13C, 15N NMR, and XRD) of the novel gold(III) complexes, prepared in 31–98 % yield, revealed different coordination ability of the pyridine and quinoline nitrogen atoms.\nTesting of catalytic activity in cyclopropanation .\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's start by looking at the article provided. The article ID is 10.1002_ejoc.202000139. The text content mentions studies on gold(III) coordination with pyridine and quinoline ligands. The abstract states that characterization methods like NMR and XRD were used to study the coordination ability. \n\nNow, the potential_dataset_ids list is empty. That means there are no datasets cited in this article. \n\nFor task 1, I need to check if there are any dataset IDs mentioned. The text doesn't use any DOI or Accession ID formats. So, there are no dataset citations here. \n\nTask 2 says to classify each dataset as Primary or Secondary. Since there are no dataset IDs mentioned, both \"Missing\" and \"Missing\" are appropriate. \n\nTask 3 requires a JSON array. Since no datasets are found, the result is an empty array with the two missing entries. \n\nWait, but maybe I should double-check. The abstract talks about characterization methods, which are NMR and XRD. Are those considered datasets? The problem states that datasets can be cited using specific terms. If the methods themselves are datasets, then they would be classified as \"Primary\" or \"Secondary\". But according to the instructions, we need to look for dataset IDs. The text doesn't mention any dataset IDs, so it's safe to assume no datasets are used. \n\nTherefore, the correct JSON is as described.\n</think>"
        ],
        [
         "24",
         "10.1002_ejoc.202000916",
         "Article ID: 10.1002_ejoc.202000916\nText Content: Abstract: We herein report an ammonium salt-catalyzed proto-col for the regioselective ring opening of aryl-aziridines with �-keto esters.\nThe reaction gives access to a variety of highly .\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's start by looking at the article provided. The article ID is 10.1002_ejoc.202000916, and the abstract mentions a reaction involving ammonium salt-catalyzed proto-col for ring-opening of aryl-aziridines with ketone esters. The text also says \"a variety of highly potential_dataset_ids\". \n\nFirst, I need to check if there are any dataset IDs mentioned. The potential_dataset_ids list is empty, which suggests that the article doesn't reference any datasets. So, according to task 1, there are no citations of datasets. \n\nFor task 2, since there are no dataset IDs found, I should classify them as \"Missing\" for both type. The third task requires a JSON object if no datasets are found. The example shows that if no datasets are found, return a single object with both Missing and Missing. \n\nWait, but maybe I should double-check if the abstract mentions any datasets. The abstract talks about a reaction, not datasets. The potential_dataset_ids list is empty, so no dataset IDs are present. Therefore, the correct JSON would be the one with both entries as Missing.\n</think>"
        ],
        [
         "25",
         "10.1002_esp.5058",
         "Article ID: 10.1002_esp.5058\nText Content: R E S E A R C H A R T I C L E Past, present and future of a meandering river in the Bolivian Amazon basin Kattia Rubi Arnez Ferrel1 | Jonathan Mark Nelson2 | Yasuyuki Shimizu1 | Tomoko Kyuka1 1Graduate School of Engineering, Hokkaido University, Sapporo, Japan 2U.S. Geological Survey, Golden, Colorado, USA Correspondence Kattia Rubi Arnez Ferrel, Graduate School of Engineering, Hokkaido University, Sapporo 060-0808, Japan. \nEmail: rubikraf@gmail.com Funding information Nitobe School Project Abstract Field observations on small rivers of the Amazon basin are less common due to their remote location and difficult accessibility.\nHere we show, through remote sensing analysis and field works, the planform evolution and riverbed topography of a small river located in the upper foreland Amazon basin, the Ichilo River.\nBy tracking plan-form changes over 30 years, we identified the factors that control meander migration rates in the Ichilo River: cutoffs, climate and human interventions.\nThe data suggest that neck cutoffs are the main controls in the Ichilo River, with an annual density of 0.022 cutoffs/km.\nIn addition, climate controls have been identified in the form of high-precipitation events that may have promoted cutoffs, an increase in meander migration rate and channel widening.\nThe width distribution of the Ichilo River is well represented by general extreme value and inverse Gaussian distributions.\nThe spatio-temporal variability of meandering migration rates in the Ichilo River is analysed in two locations where neck cutoffs are expected.\nAnalysing the distance across the neck in these two points, we predict the occurrence of a new cutoff.\nThe combined methodology of bathymetric surveys and structure from motion photogrammetry shows us the Ichilo riverbed topography and banks at high resolution, where two scour holes were identified.\nFinally, we discuss the impact of planform changes of the Ichilo River on communities that are established along its riverbanks. \nK E Y W O R D S bathymetric surveys, Bolivian Amazon basin, Ichilo River, meanders, remote sensing, UAV 1 | INTRODUCTION The Amazon basin exhibits a constantly changing landscape m.\n potential_dataset_ids: [{\"dataset_ids\": ['https://doi.org/10.1002/esp.4637'], context: \" Environ. \nJames, M.R., Chandler, J.H., Eltner, A., Fraser, C., Miller, P.E., Mills, J.P. et al. (2019) Guidelines on the use of structure-from-motion photo-grammetry in geomorphic research.\nEarth Surface Processes and Land-forms, 44(10), 2081–2084.\nhttps://doi.org/10.1002/esp.4637 \n[Dataset] Kinzel, P., Nelson, J.M. & Kattia, R.A.F. (2019)\nBathymetric survey of the Ichilo and Sajta Rivers, near Puerto Villarroel, Bolivia, May 23–24, 2019.\nU.S. Geological Survey data release.\nhttps://doi.org/10.5066/P9FW6E8K Lanzoni, S. & Semi\"},{\"dataset_ids\": ['https://doi.org/10.5066/P9FW6E8K'], context: \"44(10), 2081–2084.\nhttps://doi.org/10.1002/esp.4637 \n[Dataset] Kinzel, P., Nelson, J.M. & Kattia, R.A.F. (2019)\nBathymetric survey of the Ichilo and Sajta Rivers, near Puerto Villarroel, Bolivia, May 23–24, 2019.\nU.S. Geological Survey data release.\nhttps://doi.org/10.5066/P9FW6E8K Lanzoni, S. & Seminara, G. (2006)\nOn the nature of meander instability. Journal of Geophysical Research - Earth Surface, 111(F4), 1–14. \nhttps://doi.org/10.1029/2005JF000416 Li, J., Grenfell, M.C., Wei, H., Tooth, S. & Ngiem, S. (2020) Chute cutoff-\"},{\"dataset_ids\": ['https://doi.org/10.1029/2005JF000416'], context: \"erto Villarroel, Bolivia, May 23–24, 2019.\nU.S. Geological Survey data release.\nhttps://doi.org/10.5066/P9FW6E8K Lanzoni, S. & Seminara, G. (2006)\nOn the nature of meander instability. Journal of Geophysical Research - Earth Surface, 111(F4), 1–14. \nhttps://doi.org/10.1029/2005JF000416 Li, J., Grenfell, M.C., Wei, H., Tooth, S. & Ngiem, S. (2020) Chute cutoff-driven abandonment and sedimentation of meander bends along a fine-grained, non-vegetated, ephemeral river on the Bolivian Alti-plano. \nGeomorphology, 350, 106917. \nhttps://d\"},]\n",
         "[{'dataset_id': 'https://doi.org/10.1002/esp.4637', 'type': 'Primary'}, {'dataset_id': 'https://doi.org/10.5066/P9FW6E8K', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided an article with an ID and a text, along with a list of potential dataset_ids and their contexts. My task is to identify citations of datasets, classify their types, and present the results in JSON.\n\nFirst, I need to look through the text of the paper to see where datasets might be mentioned. The abstract mentions field observations, remote sensing, and data analysis. The text also includes some DOI links and Accession IDs. \n\nLooking at the potential_dataset_ids, the first entry has a DOI: https://doi.org/10.1002/esp.5058. The context here is \"Environ.\" and another entry has a DOI linked to a paper by James et al. Then there's another dataset with a DOI linked to a paper by Lanzoni and Seminara. \n\nWait, the text mentions \"data availability\" in the abstract, but the DOI is for the paper. The user's instructions say to look for DOIs as dataset_ids. So the first dataset ID I found is from the DOI in the text. Let me check if there are any other dataset IDs. The second entry has a DOI from the same paper, but the user's potential_dataset_ids list includes that as well. But the context says \"Environ.\" and another entry under \"44(10), 2081–2084.\" So maybe there are multiple citations here. \n\nWait, the user's potential_dataset_ids list has three entries. The first one is the DOI from the paper, the second one is the same DOI, and the third has a different DOI. So there are two dataset IDs here. The first two entries are the same DOI. But according to the instructions, if multiple citations of the same dataset_id, only the first one is considered. So the first dataset ID is from the first entry, and the second is the same DOI. But maybe the second entry is a different dataset. Let me check the context again. \n\nThe first entry's context is \"Environ.\" and the second's is \"44(10), 2081–2084.\" The third entry's context is \"erto Villarroel, Bolivia, May 23–24, 2019.\" So the third entry's DOI is from a different paper. \n\nSo, there are two dataset IDs here. The first one is from the DOI in the text, and the second one is the same DOI. But the user's potential_dataset_ids list includes both. So in the JSON, I need to list both. However, the user's instructions say to process the first citation of each dataset_id. Since there are two citations for the same DOI, but the first one is processed. \n\nWait, the first entry's context is \"Environ.\" and the second's is \"44(10), 2081–2084.\" The third entry's context is about a different paper. So the first dataset ID is from the first entry, and the second is the same DOI. But the user's potential_dataset_ids list includes both. So in the JSON, I should include both dataset IDs. \n\nTherefore, the JSON should have two entries: one with the first dataset ID and one with the second. Each with their type. The first dataset is from the DOI, which is a primary dataset since it's data from the study. The second is the same DOI, so same type. But according to the instructions, if multiple citations of the same dataset_id, process only the first. So even though there are two citations, only the first one is considered. \n\nSo the final JSON would have two entries: one for each dataset ID found. Each with the type \"Primary\" since they are data from the study. Let me check the original text again. The data mentions remote sensing analysis and field works, which are primary data. The bathymetric surveys and photogrammetry data are also primary. So both dataset IDs are primary.\n</think>"
        ],
        [
         "26",
         "10.1002_esp.5090",
         "Article ID: 10.1002_esp.5090\nText Content: R E S E A R C H A R T I C L E Paleoseismology of the 2016 MW 6.1 Petermann earthquake source: Implications for intraplate earthquake behaviour and the geomorphic longevity of bedrock fault scarps in a low strain-rate cratonic region Tamarah Rosellen King1,2 | Mark Quigley1 | Dan Clark3 | Albert Zondervan4 | Jan-Hendrik May5 | Abaz Alimanovic1 1School of Earth Sciences, The University of Melbourne, Melbourne, Victoria, Australia 2COMET, Department of Earth Sciences, University of Oxford, Oxford, UK 3Geoscience Australia, Canberra, Australian Capital Territory, Australia 4Rafter Radiocarbon Laboratory, GNS Science, Lower Hutt, New Zealand 5School of Geography, The University of Melbourne, Melbourne, Victoria, Australia Correspondence Tamarah Rosellen King, School of Earth Sciences, The University of Melbourne, Melbourne, VIC 3010, Australia. \nEmail: tamarah.king@earth.ox.ac.uk Funding information British Geological Survey; University of Melbourne; Australian Research Council, Grant/Award Number:\nDP170103350 Abstract The 20 May 2016 MW 6.1 Petermann earthquake in central Australia generated a 21 km surface rupture with 0.1 to 1 m vertical displacements across a low-relief landscape.\nNo paleo-scarps or potentially analogous topographic features are evident in pre-earthquake Worldview-1 and Worldview-2 satellite data.\nTwo excavations across the surface rupture expose near-surface fault geometry and mixed aeolian-sheetwash sediment faulted only in the 2016 earthquake.\nA 10.6 ± 0.4 ka optically stimulated luminescence (OSL) age of sheetwash sediment provides a minimum estimate for the period of quiescence prior to 2016 rupture.\nSeven cosmogenic beryllium-10 (10Be) bedrock erosion rates are derived for samples < 5 km distance from the surface rupture on the hanging-wall and foot-w.\n potential_dataset_ids: [{\"dataset_ids\": ['https://doi.org/10.1080/08120090802546977'], context: \"W., Kohn, B.P. & Cummins, P.R. (2009) Constraints on the current rate of deformation and surface uplift of the Australian continent from a new seismic database and low-T thermochronological data. \nAustralian Journal of Earth Sciences, 56(2), 99–110.\nhttps://doi.org/10.1080/08120090802546977 Calais, E., Camelbeeck, T., Stein, S., Liu, M. & Craig, T.J. (2016)\nA new para-digm for large earthquakes in stable continental plate interiors. \nGeophysical Research Letters, 43(20), 10,621–10,637. https://doi.org/10.1002/2016GL070815 Calais, E., M\"},{\"dataset_ids\": ['https://doi.org/10.1038/nature04428'], context: \"rs. \nGeophysical Research Letters, 43(20), 10,621–10,637. https://doi.org/10.1002/2016GL070815 Calais, E., Mattioli, G., DeMets, C., Nocquet, J.-M., Stein, S., Newman, A. & Rydelek, P. (2005)\nTectonic strain in plate interiors?\nNature, 438, E9–E10. \nhttps://doi.org/10.1038/nature04428 \n[online] Available from: http://www.nature.com/articles/nature04428 Clark, D. (2012) Neotectonic Features Database. \nCanberra, ACT: Geoscience Australia, Commonwealth of Australia. \nClark, D. (2018)\nWhat have we learned in the 50 years since the 19\"},{\"dataset_ids\": ['https://doi.org/10.1016/S0264-3707'], context: \"aleoseismicity in stable continental regions (USGS Bulletin 2032-B). \nWashington, DC: US Geological Survey. \nMachette, M.N. (2000) Active, capable, and potentially active faults – a paleoseismic perspective.\nJournal of Geodynamics, 29(3-5), 387–392. https://doi.org/10.1016/S0264-3707(99)00060-5 Magee, J.W. (2009)\nA Literature Review Palaeovalley Groundwater Resources in Arid and Semi-Arid Australia - A literature review.\nCanberra, Australia: Geoscience Australia. \nhttp://pid.geoscience.gov.au/dataset/ga/68542 McCalpin, J.P. (200\"},]\n",
         "[{'dataset_id': 'https://doi.org/10.1080/08120090802546977', 'type': 'Primary'}, {'dataset_id': 'https://doi.org/10.1038/nature04428', 'type': 'Secondary'}, {'dataset_id': 'https://doi.org/10.1016/S0264-3707', 'type': 'Primary'}, {'dataset_id': 'https://doi.org/10.1016/S0264-3707', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided an article with ID 10.1002_esp.5090 and a text content. They also have a list of potential_dataset_ids with their contexts. My task is to identify citations of datasets, classify each type (Primary or Secondary), and present the results in JSON.\n\nFirst, I need to look through the text of the paper to find any dataset references. The text mentions \"optically stimulated luminescence (OSL) age of sheetwash sediment\" which looks like a dataset. The DOI here is https://doi.org/10.1080/08120090802546977. That's a dataset_id. Then there's another DOI: https://doi.org/10.1038/nature04428. That seems like a citation for a different paper, but maybe a secondary dataset? Wait, the context says \"derived for samples < 5 km distance from the surface rupture on the hanging-wall and foot-wall.\" So that's a dataset from a different study. Then there's another DOI: https://doi.org/10.1016/S0264-3707. This seems like a dataset from USGS. Also, there's a URL for USGS Bulletin 2032-B. \n\nNow, I need to check if these are dataset_ids. The first two DOIs look like standard DOIs. The third one is from the USGS, which is a repository, so that's a dataset. The fourth one is from the USGS as well. The fifth one is from McCalpin's dataset. \n\nEach of these should be classified as Primary or Secondary. The first dataset (OSL age) is likely a primary dataset since it's directly related to the study. The second is a secondary because it's from another study. The USGS datasets are primary as well. So the JSON should include these three datasets. Let me make sure I didn't miss any. The text mentions two excavations, which might indicate data from the same study, so that's a primary dataset. The other two are from different studies, so secondary. \n\nSo the JSON would have each dataset with their type, and the list should include all three. I need to format it correctly, ensuring that if no datasets are found, the JSON is missing. In this case, there are three datasets, so the JSON should list them.\n</think>"
        ],
        [
         "27",
         "10.1002_mp.14424",
         "Article ID: 10.1002_mp.14424\nText Content: PleThora: Pleural effusion and thoracic cavity segmentations in diseased lungs for benchmarking chest CT processing pipelines Kendall J. Kisera) John P. and Kathrine G. McGovern Medical School, Houston, TX, USA Center for Precision Health, UTHealth School of Biomedical Informatics, Houston, TX, USA Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA Sara Ahmed and Sonja Stieb Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA Abdallah S. R. Mohamed Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA MD Anderson Cancer Center-UTHealth Graduate School of Biomedical Sciences, Houston, TX, USA Hesham Elhalawani Department of Radiation Oncology, Cleveland Clinic Taussig Cancer Center, Cleveland, OH, USA Peter Y. S. Park, Nathan S. Doyle, and Brandon J. Wang Department of Diagnostic and Interventional Imaging, John P. and Kathrine G. McGovern Medical School, Houston, TX, USA Arko Barman, Zhao Li, and W. Jim Zheng Center for Precision Health, UTHealth School of Biomedical Informatics, Houston, TX, USA Clifton D. Fullera) Department of Radiation Oncology, University of Texas MD Anderson Cancer Center, Houston, TX, USA MD Anderson Cancer Center-UTHealth Graduate School of Biomedical Sciences, Houston, TX, USA Luca Giancardoa) Center for Precision Health, UTHealth School of Biomedical Informatics, Houston, TX, USA Department of Radiation Oncology, Cleveland Clinic Taussig Cancer Center, Cleveland, OH, USA (Received 7 April 2020; revised 22 July 2020; accepted for publication 27 July 2020; published 28 August 2020) \nThis manuscript describes a dataset of thoracic cavity segmentations and discrete pleural effusion segmentations we have annotated on 402 computed tomography (CT) scans acquired from patients with non-small cell lung cancer.\nThe segmentation of these anatomic regions precedes fundamental tasks in image analysis pipelines such as lung structure segmentation, lesion detection, and radiomics feature extraction.\nBilateral thoracic cavity volumes and pleural effusion volumes were manually seg-men.\n potential_dataset_ids: [{\"dataset_ids\": ['https://doi.org/10.7937/tcia.2020.6c7y-gq39.51'], context: \"We describe PleThora, a dataset of 402 expert-vetted tho-racic cavity segmentations, 78 expert-vetted pleural effusion segmentations, and corresponding clinical and technical metadata made available to the public through TCIA at https://doi.org/10.7937/tcia.2020.6c7y-gq39.51 These seg-mentations have value for preprocessing steps in image analy-sis pipelines built for fundamental quantitative imaging tasks, including but not limited to pathologic lung segmentation, lesion detection, and radiomics feature extraction. \"},{\"dataset_ids\": ['https://doi.org/10.7937/K9/TCIA.2015.PF0M9REI'], context: \"ncer imaging archive (TCIA): maintaining and operating a public information repository.\nJ Digit Imag-ing.\n2013;26:1045–1057. \n22.\nAerts HJWL, Wee L, Rios Velazquez E, et al.\nData from NSCLC-Radio-mics\n[Dataset].\nIn: The Cancer Imaging Archive; 2019. https://doi.org/10.7937/K9/TCIA.2015.PF0M9REI 23.\nAerts HJ, Velazquez ER, Leijenaar RT, et al.\nDecoding tumour pheno-type by noninvasive imaging using a quantitative radiomics approach. \nNat Commun.\n2014;5:4006. \n24.\nLi X, Morgan PS, Ashburner J, Smith J, Rorden C. dcm2niix.exe [com-puter progr\"},{\"dataset_ids\": ['https://doi.org/10.7937/tcia.2020.6c7y-gq39'], context: \"51.\nKiser KJ, Ahmed S, Stieb SM, et al.\nData from the thoracic volume and pleural effusion segmentations in diseased lungs for benchmarking chest CT processing pipelines\n[Dataset].\nIn: The Cancer Imaging Archive; 2020.\nhttps://doi.org/10.7937/tcia.2020.6c7y-gq39 52.\nRonneberger O, Fischer P, Brox T. U-Net: Convolutional Networks for Biomedical Image Segmentation.\nPaper presented at: Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015; 5–9 October, 2015; Munich, Germany.\nhttps://doi.org/\"},]\n",
         "[{'dataset_id': 'https://doi.org/10.7937/tcia.2020.6c7y-gq39', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem step by step. The user provided an article ID and a text content. The task is to identify all citations of datasets, classify them as Primary or Secondary, and return the results in JSON format.\n\nFirst, I need to look through the text content to find any dataset IDs. The text mentions \"PleThora\" and \"Thoracic cavity segmentations and discrete pleural effusion segmentations.\" The DOI provided is https://doi.org/10.7937/tcia.2020.6c7y-gq39. That seems like a dataset ID.\n\nNext, I check the potential_dataset_ids list. The first entry has the dataset_id as 'https://doi.org/10.7937/tcia.2020.6c7y-gq39'. The context says it's a dataset for benchmarking CT processing pipelines. So that's a primary dataset because it's for the study's purpose.\n\nThe other dataset IDs in the list are not relevant here. The second part of the potential_dataset_ids has other DOIs, but they don't relate to the study. The third one is about a dataset from a conference, which is a different context.\n\nNow, I need to make sure there are no other dataset IDs mentioned. The text mentions 402 CT scans, but the DOI is the only one. The classification should be Primary for the PleThora dataset since it's the dataset used for the study. The other entries in the potential_dataset_ids are not relevant here.\n\nPutting it all together, the JSON should include the dataset_id and type for the found dataset. Since there's only one dataset found, the JSON array will have that object. I need to ensure that if there are multiple datasets, they are listed, but in this case, only one is present.\n</think>"
        ],
        [
         "28",
         "10.1002_nafm.10870",
         "Article ID: 10.1002_nafm.10870\nText Content: Abstract Objective: Silver Chub Macrhybopsis storeriana, a predominately riverine species  throughout its native range, exists within Lake Erie as the only known lake popu-lation.\nIts population declined in the 1950s and never fully recovered.\nCanada has  listed Silver Chub in the Great Lakes–­St.\nLawrence River as endangered and has  initiated a recovery plan that recognized the identification of spawning areas as a  critical component to inform Silver Chub's recovery potential. \nMethods: We investigated potential spawning locations of Silver Chub using cap-ture records, otolith microchemistry, and daily age analysis.\nLapillus otolith\nSr:Ca ra-tios from 27 age-­0 Silver Chub were used to identify potential spawning areas.\nDaily  ages estimated from lapilli were used to calculate hatch dates, which then were com-pared with capture data of adults and river flows to further inform potential spawn-ing areas. \nResult: The Detroit River (and its nearshore area) was all but ruled out as a potential  spawning location.\nThe Maumee, Portage, and Sandusky rivers or their nearshore  areas were all possible spawning locations.\nProjected hatch dates spanned the end  of May through the end of June and occurred across a wide range of flows, although  some peaks in hatch dates corresponded to flow peaks, indicating recruitment is po-tentially enhanced by high flows. \nConclusion: Silver Chub spawning period and hypothesized spawning rivers or  lacustuaries overlap those of invasive Grass Carp Ctenopharyngodon idella, creat-ing a need to jointly consider Grass Carp control efforts with conservation of Silver  Chub when assessing management alternatives.\nFurther research on spawning  guild and the use of rivers themselves or nearshore areas influenced by rivers as  spawning areas are required to maximize potential for conservation and recovery  of Silver Chub. .\n potential_dataset_ids: [{\"dataset_ids\": ['https://doi.org/10.5066/P9GTUMAY.'], context: \"associated species.\nDoctoral dissertation.\nThe Ohio State  University, Columbus. \nKočovský, P. M., and J. Farver. 2019.\nWater chemistry of Great Lakes  tributaries, 2017–­2018.\nU.S. Geological Survey data release.  \nAvailable: https://doi.org/10.5066/P9GTUMAY. \nKočovský, P. M., N. R. King, C. M. Merkes, and T. M. Schreier.  2018.\nDevelopmental stages of Grass Carp (Ctenopharyngodon  idella) eggs in the Sandusky River (version 3.0, July 2020).  \nU.S. Geological Survey data release.\nAvailable: https://doi. \"},{\"dataset_ids\": ['https://doi.org/10.5066/F75M63X0.'], context: \"earch.\nAmerican Fisheries Society, Bethesda, Maryland. \nUSGS (U.S. Geological Survey). 2019a.\nGreat Lakes Science Center  Great Lakes research vessel operations 1958–­2018.\n(version  3.0, April 2019).\nU.S. Geological Survey data release.\nAvailable:  https://doi.org/10.5066/F75M63X0. \nUSGS (U.S. Geological Survey). 2019b.\nNewly hatched invasive Grass  Carp found in Maumee River, OH.\nU.S. Geological Survey,  Office of Communications and Publishing, Reston, Virginia.  \nAvailable: www.usgs.gov/news/newly​-­hatch​ed-­invas​ive-­gras\"},]\n",
         "[{'dataset_id': 'https://doi.org/10.5066/P9GTUMAY.1', 'type': 'Primary'}, {'dataset_id': 'https://doi.org/10.5066/F75M63X0.1', 'type': 'Primary'}]",
         "<think>\nOkay, let's tackle this problem. The user provided an article ID and a text content, along with potential_dataset_ids that contain dataset IDs and their context. My task is to identify all citations of datasets, classify their types as Primary or Secondary, and present the results in JSON.\n\nFirst, I need to look through the text and see where dataset IDs are mentioned. The first thing I notice is the DOI in the abstract. The text mentions \"Silver Chub Macrhybopsis storeriana\" as the species, and then references a doctoral dissertation. The DOI is https://doi.org/10.5066/P9GTUMAY. The context here is the dissertation, which is a dataset. Since the DOI is a dataset identifier, I can classify that as a Primary dataset because it's the data used for the study.\n\nNext, there's another DOI: https://doi.org/10.5066/F75M63X0. The context here is about the research on Grass Carp eggs in the Sandusky River. The dataset is the U.S. Geological Survey data release, so that's also a Primary dataset.\n\nWait, but the user's potential_dataset_ids list includes both of these. I need to check if there are any other dataset IDs mentioned. The text also mentions a PhD dissertation, but the DOI is the dataset. Then, the second DOI refers to a study on newly hatched Grass Carp, which again is a dataset. \n\nSo, each of these two DOIs is a dataset. The first one is for the dissertation, the second for the Grass Carp research. Both are Primary datasets because they are raw or processed data generated for the study. The user's task is to return the JSON with these datasets. \n\nI need to ensure that each dataset is correctly identified and classified. Since there are two citations, each contributing to a dataset, the JSON should include both entries. Let me structure the JSON accordingly.\n</think>"
        ],
        [
         "29",
         "10.1007_jhep07(2018)134",
         "Article ID: 10.1007_jhep07(2018)134\nText Content: Abstract: The production cross-sections of Υ(1S), Υ(2S) and Υ(3S) mesons in proton-proton collisions at √s = 13 TeV are measured with a data sample corresponding to an integrated luminosity of 277 ± 11 pb−1 recorded by the LHCb experiment in 2015.\nThe Υ mesons are reconstructed in the decay mode Υ →µ+µ−.\nThe differential production cross-sections times the dimuon\nbranching fractions are measured as a function of the Υ transverse momentum, pT, and rapidity, y, over the range 0 < pT < 30 GeV/c and 2.0 < y < 4.5.\nThe ratios of the cross-sections with respect to the LHCb measurement at √s = 8 TeV are also determined.\nThe measurements are compared with theoretical predictions based on NRQCD. .\n potential_dataset_ids: []\n",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]",
         "<think>\nOkay, let's tackle this problem. The user provided an article ID and a text content. They also have a list of potential dataset_ids and their context within the paper. My job is to identify citations of datasets, classify their types (Primary or Secondary), and return them in JSON.\n\nFirst, looking at the article ID: 10.1007_jhep07(2018)134. The text mentions measurements of Υ mesons' production cross-sections in proton-proton collisions at 13 TeV. The data sample is from LHCb at 2015. The abstract talks about measuring cross-sections using a dataset. But the potential_dataset_ids list is empty. Wait, maybe there's no dataset mentioned here. The text doesn't mention any dataset IDs like DOIs or Accession IDs. \n\nSo, according to task 1, I need to check if there are any dataset citations. The text doesn't use any dataset terms like \"dataset\", \"repository\", \"data release\", etc. It's just about measuring cross-sections. Therefore, no dataset citations are found. \n\nTask 2: Since there are no dataset IDs mentioned, all datasets are \"Missing\" in both type. The third task requires a JSON object if no datasets are found. The example shows a single object with both dataset_id and type as \"Missing\". \n\nWait, but maybe I should check again. The text says \"the data sample corresponding to an integrated luminosity...\" which could imply data, but the potential_dataset_ids list is empty. So no dataset IDs are cited here. \n\nTherefore, the correct JSON would have a single object with dataset_id and type both missing. That's the answer.\n</think>"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 30
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>llm_input</th>\n",
       "      <th>llm_response</th>\n",
       "      <th>llm_thinking_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>Article ID: 10.1002_2017jc013030\\nText Content...</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.17882/4938...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this step by step....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>Article ID: 10.1002_anie.201916483\\nText Conte...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>Article ID: 10.1002_anie.202005531\\nText Conte...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1002_anie.202007717</td>\n",
       "      <td>Article ID: 10.1002_anie.202007717\\nText Conte...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1002_chem.201902131</td>\n",
       "      <td>Article ID: 10.1002_chem.201902131\\nText Conte...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1002_chem.201903120</td>\n",
       "      <td>Article ID: 10.1002_chem.201903120\\nText Conte...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.1002_chem.202000235</td>\n",
       "      <td>Article ID: 10.1002_chem.202000235\\nText Conte...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's see. The user provided an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.1002_chem.202001412</td>\n",
       "      <td>Article ID: 10.1002_chem.202001412\\nText Conte...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.1002_chem.202001668</td>\n",
       "      <td>Article ID: 10.1002_chem.202001668\\nText Conte...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.1002_chem.202003167</td>\n",
       "      <td>Article ID: 10.1002_chem.202003167\\nText Conte...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.1002_cssc.202201821</td>\n",
       "      <td>Article ID: 10.1002_cssc.202201821\\nText Conte...</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5281/zenod...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.1002_ece3.3985</td>\n",
       "      <td>Article ID: 10.1002_ece3.3985\\nText Content: E...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.1002_ece3.4466</td>\n",
       "      <td>Article ID: 10.1002_ece3.4466\\nText Content: E...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.1002_ece3.5260</td>\n",
       "      <td>Article ID: 10.1002_ece3.5260\\nText Content: E...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.1002_ece3.5395</td>\n",
       "      <td>Article ID: 10.1002_ece3.5395\\nText Content: E...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.1002_ece3.6144</td>\n",
       "      <td>Article ID: 10.1002_ece3.6144\\nText Content: E...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.1002_ece3.6303</td>\n",
       "      <td>Article ID: 10.1002_ece3.6303\\nText Content: E...</td>\n",
       "      <td>[{'dataset_id': 'MN107084', 'type': 'Primary'}...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.1002_ece3.6784</td>\n",
       "      <td>Article ID: 10.1002_ece3.6784\\nText Content: E...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.1002_ece3.961</td>\n",
       "      <td>Article ID: 10.1002_ece3.961\\nText Content: Th...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.1002_ece3.9627</td>\n",
       "      <td>Article ID: 10.1002_ece3.9627\\nText Content: E...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.1002_ecs2.1280</td>\n",
       "      <td>Article ID: 10.1002_ecs2.1280\\nText Content: A...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.1002_ecs2.4619</td>\n",
       "      <td>Article ID: 10.1002_ecs2.4619\\nText Content: A...</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.25349/D9QW...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.1002_ejic.201900904</td>\n",
       "      <td>Article ID: 10.1002_ejic.201900904\\nText Conte...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.1002_ejoc.202000139</td>\n",
       "      <td>Article ID: 10.1002_ejoc.202000139\\nText Conte...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.1002_ejoc.202000916</td>\n",
       "      <td>Article ID: 10.1002_ejoc.202000916\\nText Conte...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's start by looking at the a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.1002_esp.5058</td>\n",
       "      <td>Article ID: 10.1002_esp.5058\\nText Content: R ...</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.1002/esp.4...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.1002_esp.5090</td>\n",
       "      <td>Article ID: 10.1002_esp.5090\\nText Content: R ...</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.1080/08120...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10.1002_mp.14424</td>\n",
       "      <td>Article ID: 10.1002_mp.14424\\nText Content: Pl...</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.7937/tcia....</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.1002_nafm.10870</td>\n",
       "      <td>Article ID: 10.1002_nafm.10870\\nText Content: ...</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5066/P9GTU...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.1007_jhep07(2018)134</td>\n",
       "      <td>Article ID: 10.1007_jhep07(2018)134\\nText Cont...</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem. The ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 article_id  \\\n",
       "0      10.1002_2017jc013030   \n",
       "1    10.1002_anie.201916483   \n",
       "2    10.1002_anie.202005531   \n",
       "3    10.1002_anie.202007717   \n",
       "4    10.1002_chem.201902131   \n",
       "5    10.1002_chem.201903120   \n",
       "6    10.1002_chem.202000235   \n",
       "7    10.1002_chem.202001412   \n",
       "8    10.1002_chem.202001668   \n",
       "9    10.1002_chem.202003167   \n",
       "10   10.1002_cssc.202201821   \n",
       "11        10.1002_ece3.3985   \n",
       "12        10.1002_ece3.4466   \n",
       "13        10.1002_ece3.5260   \n",
       "14        10.1002_ece3.5395   \n",
       "15        10.1002_ece3.6144   \n",
       "16        10.1002_ece3.6303   \n",
       "17        10.1002_ece3.6784   \n",
       "18         10.1002_ece3.961   \n",
       "19        10.1002_ece3.9627   \n",
       "20        10.1002_ecs2.1280   \n",
       "21        10.1002_ecs2.4619   \n",
       "22   10.1002_ejic.201900904   \n",
       "23   10.1002_ejoc.202000139   \n",
       "24   10.1002_ejoc.202000916   \n",
       "25         10.1002_esp.5058   \n",
       "26         10.1002_esp.5090   \n",
       "27         10.1002_mp.14424   \n",
       "28       10.1002_nafm.10870   \n",
       "29  10.1007_jhep07(2018)134   \n",
       "\n",
       "                                            llm_input  \\\n",
       "0   Article ID: 10.1002_2017jc013030\\nText Content...   \n",
       "1   Article ID: 10.1002_anie.201916483\\nText Conte...   \n",
       "2   Article ID: 10.1002_anie.202005531\\nText Conte...   \n",
       "3   Article ID: 10.1002_anie.202007717\\nText Conte...   \n",
       "4   Article ID: 10.1002_chem.201902131\\nText Conte...   \n",
       "5   Article ID: 10.1002_chem.201903120\\nText Conte...   \n",
       "6   Article ID: 10.1002_chem.202000235\\nText Conte...   \n",
       "7   Article ID: 10.1002_chem.202001412\\nText Conte...   \n",
       "8   Article ID: 10.1002_chem.202001668\\nText Conte...   \n",
       "9   Article ID: 10.1002_chem.202003167\\nText Conte...   \n",
       "10  Article ID: 10.1002_cssc.202201821\\nText Conte...   \n",
       "11  Article ID: 10.1002_ece3.3985\\nText Content: E...   \n",
       "12  Article ID: 10.1002_ece3.4466\\nText Content: E...   \n",
       "13  Article ID: 10.1002_ece3.5260\\nText Content: E...   \n",
       "14  Article ID: 10.1002_ece3.5395\\nText Content: E...   \n",
       "15  Article ID: 10.1002_ece3.6144\\nText Content: E...   \n",
       "16  Article ID: 10.1002_ece3.6303\\nText Content: E...   \n",
       "17  Article ID: 10.1002_ece3.6784\\nText Content: E...   \n",
       "18  Article ID: 10.1002_ece3.961\\nText Content: Th...   \n",
       "19  Article ID: 10.1002_ece3.9627\\nText Content: E...   \n",
       "20  Article ID: 10.1002_ecs2.1280\\nText Content: A...   \n",
       "21  Article ID: 10.1002_ecs2.4619\\nText Content: A...   \n",
       "22  Article ID: 10.1002_ejic.201900904\\nText Conte...   \n",
       "23  Article ID: 10.1002_ejoc.202000139\\nText Conte...   \n",
       "24  Article ID: 10.1002_ejoc.202000916\\nText Conte...   \n",
       "25  Article ID: 10.1002_esp.5058\\nText Content: R ...   \n",
       "26  Article ID: 10.1002_esp.5090\\nText Content: R ...   \n",
       "27  Article ID: 10.1002_mp.14424\\nText Content: Pl...   \n",
       "28  Article ID: 10.1002_nafm.10870\\nText Content: ...   \n",
       "29  Article ID: 10.1007_jhep07(2018)134\\nText Cont...   \n",
       "\n",
       "                                         llm_response  \\\n",
       "0   [{'dataset_id': 'https://doi.org/10.17882/4938...   \n",
       "1      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "2      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "3      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "4      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "5      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "6      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "7      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "8      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "9      [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "10  [{'dataset_id': 'https://doi.org/10.5281/zenod...   \n",
       "11     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "12     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "13     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "14     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "15     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "16  [{'dataset_id': 'MN107084', 'type': 'Primary'}...   \n",
       "17     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "18     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "19     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "20     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "21  [{'dataset_id': 'https://doi.org/10.25349/D9QW...   \n",
       "22     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "23     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "24     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "25  [{'dataset_id': 'https://doi.org/10.1002/esp.4...   \n",
       "26  [{'dataset_id': 'https://doi.org/10.1080/08120...   \n",
       "27  [{'dataset_id': 'https://doi.org/10.7937/tcia....   \n",
       "28  [{'dataset_id': 'https://doi.org/10.5066/P9GTU...   \n",
       "29     [{'dataset_id': 'Missing', 'type': 'Missing'}]   \n",
       "\n",
       "                                 llm_thinking_content  \n",
       "0   <think>\\nOkay, let's tackle this step by step....  \n",
       "1   <think>\\nOkay, let's start by looking at the a...  \n",
       "2   <think>\\nOkay, let's start by looking at the a...  \n",
       "3   <think>\\nOkay, let's start by looking at the a...  \n",
       "4   <think>\\nOkay, let's start by looking at the a...  \n",
       "5   <think>\\nOkay, let's tackle this problem. The ...  \n",
       "6   <think>\\nOkay, let's see. The user provided an...  \n",
       "7   <think>\\nOkay, let's start by looking at the a...  \n",
       "8   <think>\\nOkay, let's start by looking at the a...  \n",
       "9   <think>\\nOkay, let's start by looking at the a...  \n",
       "10  <think>\\nOkay, let's start by looking at the a...  \n",
       "11  <think>\\nOkay, let's tackle this problem step ...  \n",
       "12  <think>\\nOkay, let's start by looking at the a...  \n",
       "13  <think>\\nOkay, let's start by looking at the a...  \n",
       "14  <think>\\nOkay, let's tackle this problem step ...  \n",
       "15  <think>\\nOkay, let's tackle this problem step ...  \n",
       "16  <think>\\nOkay, let's tackle this problem step ...  \n",
       "17  <think>\\nOkay, let's start by looking at the a...  \n",
       "18  <think>\\nOkay, let's tackle this problem step ...  \n",
       "19  <think>\\nOkay, let's tackle this problem step ...  \n",
       "20  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "21  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "22  <think>\\nOkay, let's start by looking at the p...  \n",
       "23  <think>\\nOkay, let's start by looking at the a...  \n",
       "24  <think>\\nOkay, let's start by looking at the a...  \n",
       "25  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "26  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "27  <think>\\nOkay, let's tackle this problem step ...  \n",
       "28  <think>\\nOkay, let's tackle this problem. The ...  \n",
       "29  <think>\\nOkay, let's tackle this problem. The ...  "
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_processed_articles_df = process_articles(test_file_paths_df, inference_model)\n",
    "test_set_processed_articles_df.to_csv(\"test_set_processed_articles.csv\", index=False)\n",
    "test_set_processed_articles_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f2525aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Qwen model and tokenizer from: C:\\Users\\jim\\.cache\\kagglehub\\models\\qwen-lm\\qwen-3\\transformers\\0.6b\\1\n"
     ]
    }
   ],
   "source": [
    "eval_model = QwenModelEval(QWEN_BASE_MODEL_PATH, sys_prompt=\"You are a chatbot.\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8bee422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: How many r's in strawberries?\n",
      "Preparing input for user: How many r's in strawberries?\n",
      "Generating response for user input:\n",
      "Parsing response from generated IDs...\n",
      "Bot: ('There are 2 \\'r\\'s in the word \"strawberries\".', '<think>\\nOkay, the user is asking how many \\'r\\'s are in \"strawberries\". Let me start by breaking down the word. \"Strawberries\" is spelled S-T-R-A-W-B-E-R-R-I-N-G-S. Let me count each \\'r\\' here. \\n\\nFirst letter: S, no \\'r\\'. Second: T, no. Third: R, one \\'r\\'. Fourth: A, no. Fifth: W, no. Sixth: B, no. Seventh: E, no. Eighth: R, another \\'r\\'. Ninth: I, no. Tenth: N, no. Eleventh: G, no. So that\\'s two \\'r\\'s in total. \\n\\nWait, maybe I should double-check. Let me write it out again: S-T-R-A-W-B-E-R-R-I-N-G-S. Yes, the third and eighth letters are both \\'r\\'s. So two \\'r\\'s. The user might be testing if I can count them correctly. I should make sure there\\'s no other \\'r\\'s I missed. No, I think that\\'s all. So the answer is two.\\n</think>')\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Example Usage of eval_model ---\n",
    "\n",
    "# First input (without /think or /no_think tags, thinking mode is enabled by default)\n",
    "user_input_1 = \"How many r's in strawberries?\"\n",
    "print(f\"User: {user_input_1}\")\n",
    "response_1 = eval_model.generate_response(user_input_1)\n",
    "print(f\"Bot: {response_1}\")\n",
    "print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 2. Data Preparation for LLM Training (Revised for Combined Task) ---\n",
    "\n",
    "def load_base_llm_for_training():\n",
    "    \"\"\"Loads the base Qwen model and tokenizer for fine-tuning.\"\"\"\n",
    "    global llm_tokenizer, llm_model\n",
    "    if not AutoModelForCausalLM or not QWEN_BASE_MODEL_PATH:\n",
    "        print(\"LLM components not available or base model path not set. Skipping LLM loading.\")\n",
    "        return False\n",
    "    try:\n",
    "        print(f\"Loading Qwen tokenizer from: {QWEN_BASE_MODEL_PATH}\")\n",
    "        llm_tokenizer = AutoTokenizer.from_pretrained(QWEN_BASE_MODEL_PATH, trust_remote_code=True)\n",
    "        if llm_tokenizer.pad_token is None:\n",
    "            llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "            print(\"Set tokenizer.pad_token to tokenizer.eos_token\")\n",
    "\n",
    "        print(f\"Loading Qwen model from: {QWEN_BASE_MODEL_PATH}\")\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "            QWEN_BASE_MODEL_PATH,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32,\n",
    "            device_map=\"auto\", # Automatically uses GPU if available\n",
    "            trust_remote_code=True,\n",
    "            # load_in_8bit=True if bnb else False # Uncomment if bitsandbytes is used\n",
    "        )\n",
    "        print(f\"Base LLM loaded successfully on {llm_model.device}.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading base LLM for training: {e}\")\n",
    "        llm_tokenizer, llm_model = None, None # Reset to None on failure\n",
    "        return False\n",
    "\n",
    "def prepare_training_data_for_llm(\n",
    "    training_df: pd.DataFrame,\n",
    "    all_article_texts: dict[str, str],\n",
    "    tokenizer_max_length: int\n",
    ") -> Dataset:\n",
    "    \"\"\"\n",
    "    Prepares training data for LLM fine-tuning, aggregating dataset IDs and classifications\n",
    "    per article and formatting into ChatML JSON output.\n",
    "    \"\"\"\n",
    "    formatted_examples = []\n",
    "\n",
    "    # Group training data by article_id to get all datasets for each article\n",
    "    # This creates a dictionary where keys are article_ids and values are lists of dataset dicts\n",
    "    grouped_training_data = training_df.groupby('article_id').apply(\n",
    "        lambda x: [{\"dataset_id\": row['dataset_id'], \"classification\": row['label']} for _, row in x]\n",
    "    ).to_dict()\n",
    "\n",
    "    # Get all article IDs for which we have text content\n",
    "    all_article_ids_with_text = set(all_article_texts.keys())\n",
    "    \n",
    "    # Iterate through all articles for which we have text (these are our potential training examples)\n",
    "    for article_id in all_article_ids_with_text:\n",
    "        article_text = all_article_texts.get(article_id, \"\")\n",
    "        if not article_text:\n",
    "            print(f\"Warning: Article text for {article_id} not found. Skipping training example.\")\n",
    "            continue\n",
    "\n",
    "        # Truncate article text to fit within the model's context window\n",
    "        # Reserve tokens for the prompt and the expected JSON response.\n",
    "        # A typical Qwen 1.5 model has 32768 max_seq_length.\n",
    "        # 512 tokens for prompt/response is a safe estimate.\n",
    "        truncated_article_text = article_text[:tokenizer_max_length - 512] \n",
    "\n",
    "        # Determine the ground truth output for this article\n",
    "        if article_id in grouped_training_data:\n",
    "            # Article has datasets, format them as JSON\n",
    "            ground_truth_datasets = grouped_training_data[article_id]\n",
    "            assistant_response_json = json.dumps(ground_truth_datasets, ensure_ascii=False)\n",
    "        else:\n",
    "            # Article has no datasets in training data, so the model should output an empty list.\n",
    "            # This explicitly trains the model to output '[]' for \"Missing\" cases.\n",
    "            assistant_response_json = \"[]\"\n",
    "            # print(f\"Info: Article {article_id} has no datasets in training data. Training to output '[]'.\")\n",
    "\n",
    "        # Construct the user message for the LLM\n",
    "        user_message = f\"\"\"\n",
    "Article Text:\n",
    "{truncated_article_text}\n",
    "\n",
    "Task: Identify all datasets or databases used in this research article and classify each as \"Primary\" (if created by the authors for this research) or \"Secondary\" (if an existing dataset used in this research).\n",
    "\n",
    "Output Format: Provide a JSON list of objects. Each object should have \"dataset_id\" and \"classification\" keys. If no datasets are identified, return an empty JSON list: [].\n",
    "\"\"\"\n",
    "        # Construct the full ChatML formatted string for SFTTrainer\n",
    "        # The trainer will use this entire string as the 'text' field.\n",
    "        chatml_formatted_string = f\"<|im_start|>system\\nYou are an expert research assistant. Your task is to extract and classify datasets from scientific articles.<|im_end|>\\n<|im_start|>user\\n{user_message.strip()}<|im_end|>\\n<|im_start|>assistant\\n{assistant_response_json}<|im_end|>\"\n",
    "        \n",
    "        formatted_examples.append({\"text\": chatml_formatted_string})\n",
    "\n",
    "    if not formatted_examples:\n",
    "        raise ValueError(\"No training examples could be prepared. Check your data and article texts.\")\n",
    "\n",
    "    return Dataset.from_list(formatted_examples)\n",
    "\n",
    "# --- 3. LLM Model Training (Fine-tuning) ---\n",
    "\n",
    "# Attempt to load tokenizer and model if not already loaded (e.g., if previous training failed or was skipped)\n",
    "if llm_model is None:\n",
    "    load_base_llm_for_training()\n",
    "\n",
    "if llm_model and not training_df.empty and Dataset: # Ensure Dataset is imported\n",
    "    print(\"\\n--- Preparing data for Fine-tuning (Combined Task) ---\")\n",
    "    # Use the model's max_length for context, or a reasonable default if tokenizer isn't loaded\n",
    "    max_len = llm_tokenizer.model_max_length if llm_tokenizer else 4096 \n",
    "    train_dataset = prepare_training_data_for_llm(training_df, all_article_texts, max_len)\n",
    "    \n",
    "    print(f\"Prepared {len(train_dataset)} examples for fine-tuning.\")\n",
    "    print(\"Example formatted training instance (first 500 chars):\")\n",
    "    print(train_dataset[0]['text'][:500])\n",
    "\n",
    "    print(\"\\n--- Starting Fine-tuning (Combined Task) ---\")\n",
    "    try:\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"{FINE_TUNED_MODEL_OUTPUT_DIR}/checkpoints\",\n",
    "            num_train_epochs=1,  # Start with 1 epoch, adjust as needed\n",
    "            per_device_train_batch_size=1, # Adjust based on VRAM\n",
    "            gradient_accumulation_steps=4, # Effective batch size = 1 * 4 = 4\n",
    "            learning_rate=2e-5,\n",
    "            logging_steps=10,\n",
    "            save_steps=50, # Save checkpoints periodically\n",
    "            fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
    "            bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "            optim=\"paged_adamw_8bit\", # Good for memory efficiency if bitsandbytes is installed\n",
    "            # report_to=\"none\", # Disable logging to external services\n",
    "            # max_steps=100, # For quick testing\n",
    "        )\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model=llm_model,\n",
    "            tokenizer=llm_tokenizer,\n",
    "            train_dataset=train_dataset,\n",
    "            dataset_text_field=\"text\", # This field contains the full ChatML string\n",
    "            args=training_args,\n",
    "            max_seq_length=max_len, # Use the model's full context length\n",
    "            packing=False, # Set to True if your inputs are much shorter than max_seq_length\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        print(\"Fine-tuning completed.\")\n",
    "\n",
    "        print(f\"Saving fine-tuned model to: {FINE_TUNED_MODEL_OUTPUT_DIR}\")\n",
    "        trainer.save_model(FINE_TUNED_MODEL_OUTPUT_DIR)\n",
    "        print(\"Model and tokenizer saved.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during fine-tuning: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        llm_model = None # Mark model as failed to load/train\n",
    "else:\n",
    "    print(\"Skipping LLM fine-tuning due to missing training data or LLM components.\")\n",
    "\n",
    "\n",
    "# --- 4. LLM-based Extraction & Classification (Inference) ---\n",
    "\n",
    "# Load the fine-tuned model for inference (if training was successful)\n",
    "# If training was skipped or failed, this will attempt to load from the base path or fail.\n",
    "if inference_model is None: # Only load if not already loaded\n",
    "    if AutoModelForCausalLM: # Check if transformers is available\n",
    "        if os.path.exists(FINE_TUNED_MODEL_OUTPUT_DIR) and os.path.isdir(FINE_TUNED_MODEL_OUTPUT_DIR):\n",
    "            MODEL_TO_LOAD = FINE_TUNED_MODEL_OUTPUT_DIR\n",
    "            print(f\"Loading fine-tuned model for inference from: {MODEL_TO_LOAD}\")\n",
    "        else:\n",
    "            MODEL_TO_LOAD = QWEN_BASE_MODEL_PATH\n",
    "            print(f\"Fine-tuned model not found. Loading base model for inference from: {MODEL_TO_LOAD}\")\n",
    "\n",
    "        try:\n",
    "            inference_tokenizer = AutoTokenizer.from_pretrained(MODEL_TO_LOAD, trust_remote_code=True)\n",
    "            if inference_tokenizer.pad_token is None:\n",
    "                inference_tokenizer.pad_token = inference_tokenizer.eos_token\n",
    "            inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "                MODEL_TO_LOAD,\n",
    "                torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            ).eval() # Set to evaluation mode\n",
    "            print(f\"Inference LLM loaded successfully on {inference_model.device}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading inference LLM from {MODEL_TO_LOAD}: {e}\")\n",
    "            inference_model, inference_tokenizer = None, None\n",
    "    else:\n",
    "        print(\"Transformers library not available. Cannot load LLM for inference.\")\n",
    "\n",
    "\n",
    "def extract_and_classify_with_llm(article_text: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Uses the loaded LLM to extract dataset IDs and classify them.\n",
    "    Returns a list of dictionaries like [{\"dataset_id\": \"...\", \"classification\": \"...\"}].\n",
    "    Returns an empty list if LLM is unavailable or parsing fails.\n",
    "    \"\"\"\n",
    "    if not inference_model or not inference_tokenizer:\n",
    "        print(\"  LLM unavailable for extraction/classification.\")\n",
    "        return [] # Return empty list if LLM is not loaded\n",
    "\n",
    "    # Truncate article text for inference if it exceeds model's context window\n",
    "    # Use the same max_length as during training for consistency\n",
    "    max_inference_context_length = inference_tokenizer.model_max_length - 256 # Reserve tokens for prompt and response\n",
    "    truncated_article_text = article_text[:max_inference_context_length]\n",
    "\n",
    "    user_message = f\"\"\"\n",
    "Article Text:\n",
    "{truncated_article_text}\n",
    "\n",
    "Task: Identify all datasets or databases used in this research article and classify each as \"Primary\" (if created by the authors for this research) or \"Secondary\" (if an existing dataset used in this research).\n",
    "\n",
    "Output Format: Provide a JSON list of objects. Each object should have \"dataset_id\" and \"classification\" keys. If no datasets are identified, return an empty JSON list: [].\n",
    "\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert research assistant. Your task is to extract and classify datasets from scientific articles.\"},\n",
    "        {\"role\": \"user\", \"content\": user_message.strip()}\n",
    "    ]\n",
    "    \n",
    "    input_ids = inference_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(inference_model.device)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=512, # Allow more tokens for multiple dataset outputs\n",
    "                pad_token_id=inference_tokenizer.eos_token_id,\n",
    "                eos_token_id=inference_tokenizer.convert_tokens_to_ids(\"<|im_end|>\")\n",
    "            )\n",
    "        \n",
    "        response_text = inference_tokenizer.decode(\n",
    "            outputs[0][input_ids.shape[1]:],\n",
    "            skip_special_tokens=False # Keep special tokens to remove <|im_end|> explicitly\n",
    "        ).strip()\n",
    "        response_text = response_text.replace(\"<|im_end|>\", \"\").strip()\n",
    "        \n",
    "        print(f\"  LLM raw response: '{response_text}'\")\n",
    "\n",
    "        # Attempt to parse the JSON output\n",
    "        try:\n",
    "            parsed_data = json.loads(response_text)\n",
    "            if isinstance(parsed_data, list):\n",
    "                # Validate structure: each item should be a dict with 'dataset_id' and 'classification'\n",
    "                valid_datasets = []\n",
    "                for item in parsed_data:\n",
    "                    if isinstance(item, dict) and 'dataset_id' in item and 'classification' in item:\n",
    "                        # Basic validation for classification label\n",
    "                        if item['classification'] in [\"Primary\", \"Secondary\"]:\n",
    "                            valid_datasets.append(item)\n",
    "                        else:\n",
    "                            print(f\"  Warning: Invalid classification '{item['classification']}' for dataset '{item.get('dataset_id', 'N/A')}'. Skipping.\")\n",
    "                    else:\n",
    "                        print(f\"  Warning: Malformed JSON object: {item}. Skipping.\")\n",
    "                return valid_datasets\n",
    "            else:\n",
    "                print(f\"  Warning: LLM did not return a JSON list: {response_text}\")\n",
    "                return []\n",
    "        except json.JSONDecodeError as jde:\n",
    "            print(f\"  Error decoding JSON from LLM response: {jde}. Raw response: '{response_text}'\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error during LLM generation: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Main Processing Loop for all articles (Revised) ---\n",
    "print(\"\\n--- Starting Article Processing and Classification (LLM-driven) ---\")\n",
    "final_results = []\n",
    "\n",
    "for article_id, article_text in all_article_texts.items():\n",
    "    print(f\"\\nProcessing article: {article_id}\")\n",
    "    \n",
    "    # LLM directly extracts and classifies\n",
    "    identified_datasets = extract_and_classify_with_llm(article_text)\n",
    "    \n",
    "    if not identified_datasets:\n",
    "        # If LLM returns an empty list, classify the article as \"Missing\"\n",
    "        print(f\"  LLM identified no datasets for {article_id}. Classifying as 'Missing'.\")\n",
    "        final_results.append({\n",
    "            \"article_id\": article_id,\n",
    "            \"dataset_id\": \"N/A\", # Indicate no specific dataset ID\n",
    "            \"classification_label\": \"Missing\"\n",
    "        })\n",
    "    else:\n",
    "        print(f\"  LLM identified {len(identified_datasets)} dataset(s) for {article_id}.\")\n",
    "        for item in identified_datasets:\n",
    "            final_results.append({\n",
    "                \"article_id\": article_id,\n",
    "                \"dataset_id\": item.get(\"dataset_id\", \"Unknown\"), # Use .get() for safety\n",
    "                \"classification_label\": item.get(\"classification\", \"Uncertain_LLM\")\n",
    "            })\n",
    "\n",
    "\n",
    "# --- 5. Results & Output ---\n",
    "\n",
    "print(\"\\n--- Final Results ---\")\n",
    "if final_results:\n",
    "    results_df = pd.DataFrame(final_results)\n",
    "    print(results_df.head(10)) # Print first 10 rows\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(FINAL_RESULTS_CSV_PATH, index=False)\n",
    "    print(f\"\\nResults saved to: {FINAL_RESULTS_CSV_PATH}\")\n",
    "else:\n",
    "    print(\"No results generated.\")\n",
    "\n",
    "print(\"\\nProcessing complete, Jim!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
