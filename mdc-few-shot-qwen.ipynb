{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3fa441",
   "metadata": {},
   "source": [
    "## 0. Kaggle Environment and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "663565a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import collections\n",
    "import xml.etree.ElementTree as ET\n",
    "import PyPDF2\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5660a7",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00f362c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Constants\n",
    "MAX_TOKENS = 4096  # Adjust based on your model's capabilities\n",
    "QWEN_MODEL_NAME_OR_PATH = \"/kaggle/input/qwen-model-files/qwen-7b-chat\" # Example\n",
    "OUTPUT_CSV_PATH = \"/kaggle/working/article_dataset_classification.csv\"\n",
    "ARTICLES_BASE_DIR = './kaggle/input/make-data-count-finding-data-references/'\n",
    "ARTICLES_TRAIN_DIR = ARTICLES_BASE_DIR + 'train/'\n",
    "ARTICLES_TEST_DIR = ARTICLES_BASE_DIR + 'test/'\n",
    "ARTICLE_FORMATS = [{'format':'PDF', 'ext': '.pdf'}, {'format': 'XML', 'ext': '.xml'}]\n",
    "train_labels_file_path = ARTICLES_BASE_DIR+'train_labels.csv'\n",
    "sample_submission_file_path = ARTICLES_BASE_DIR+'sample_submission.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d3006",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71410613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read text from PDF files using PyPDF2\n",
    "def read_pdf_text(pdf_file_path) -> str:\n",
    "    \"\"\"Extracts all text from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    # Ensure the file path is a string and not NaN or empty\n",
    "    if pd.isna(pdf_file_path) or not pdf_file_path:\n",
    "        return text\n",
    "    \n",
    "    # Convert to string and strip whitespace\n",
    "    pdf_file_path = str(pdf_file_path).strip()\n",
    "\n",
    "    try:\n",
    "        with open(pdf_file_path, 'rb') as pdf_file_obj:\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file_obj)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            for page_num in range(num_pages):\n",
    "                page_obj = pdf_reader.pages[page_num]\n",
    "                text += page_obj.extract_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_file_path} with PyPDF2: {e}\")\n",
    "        \n",
    "    return text\n",
    "\n",
    "def read_xml_text(xml_file_path) -> str:\n",
    "    \"\"\"Reads and concatenates all text content from an XML file.\"\"\"\n",
    "    # Using your previously developed function\n",
    "    all_text_parts = []\n",
    "    try:\n",
    "        tree = ET.parse(xml_file_path)\n",
    "        root = tree.getroot()\n",
    "        for element in root.iter():\n",
    "            if element.text:\n",
    "                cleaned_text = element.text.strip()\n",
    "                if cleaned_text:\n",
    "                    all_text_parts.append(cleaned_text)\n",
    "            if element.tail:\n",
    "                cleaned_tail = element.tail.strip()\n",
    "                if cleaned_tail:\n",
    "                    all_text_parts.append(cleaned_tail)\n",
    "        return \" \".join(all_text_parts) if all_text_parts else \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading XML {xml_file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb0fa7c",
   "metadata": {},
   "source": [
    "## 3. Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53d215c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize dataset IDs\n",
    "# This function takes a dataset ID as input and normalizes it by removing the \"doi.org/\" prefix if it exists.\n",
    "def get_dataset_id_regex(id: str) -> str:\n",
    "    # Regex to capture the DOI part after \"doi.org/\"\n",
    "    # It handles optional \"https://\" and \"www.\"\n",
    "    regex_id = id\n",
    "    dryad_marker = \"/dryad.\"\n",
    "    regex = r\"(?:https://)?(?:www\\.)?doi\\.org/(.+)\"\n",
    "    match = re.search(regex, str(id).lower())\n",
    "    if match:\n",
    "        # The DOI is in the first capturing group\n",
    "        full_doi_candidate = match.group(1)\n",
    "        dryad_index = full_doi_candidate.find(dryad_marker)\n",
    "        if dryad_index != -1:\n",
    "            # Calculate the starting point of the suffix (right after \"/dryad.\" + 5 characters)\n",
    "            start_of_suffix = dryad_index + len(dryad_marker) + 5\n",
    "            # \"/dryad.\" is found in the DOI candidate\n",
    "            prefix = full_doi_candidate[:start_of_suffix]\n",
    "            \n",
    "            # Get the remaining characters for the suffix\n",
    "            suffix = full_doi_candidate[start_of_suffix : ]\n",
    "            \n",
    "            # Construct the regex ID\n",
    "            regex_id = prefix + '\\\\s?' + suffix\n",
    "        else:\n",
    "            # Otherwise, return the full DOI\n",
    "            regex_id = full_doi_candidate\n",
    "\n",
    "    return regex_id.replace('.', '\\\\.\\\\s?').replace('/', '\\\\/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68c4d24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_regex_with_context(main_string: str, search_regex: str, context_chars: int = 200) -> list[str]:\n",
    "    \"\"\"\n",
    "    Finds all occurrences of search_regex within main_string and returns\n",
    "    a context window for each. The context window includes the matching search_string\n",
    "    itself, surrounded by up to 'context_chars' characters from before and\n",
    "    after its occurrence in the main_string.\n",
    "\n",
    "    Args:\n",
    "        main_string (str): The string to search within.\n",
    "        search_regex (str): The regular expression to search for.\n",
    "        context_chars (int): The number of characters to include before and after\n",
    "                             the search_string in the context window. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings, where each string is an occurrence of\n",
    "                   search_string surrounded by its context. Returns an empty\n",
    "                   list if search_string is not found, or if either\n",
    "                   main_string or search_string is empty.\n",
    "    \"\"\"\n",
    "    # Ensure the main_string and search_regex are valid\n",
    "    if not main_string or not search_regex:\n",
    "        return []\n",
    "\n",
    "    re_doi = re.compile(search_regex, re.IGNORECASE)\n",
    "    occurrences_with_context: list[str] = []\n",
    "    len_search: int = len(search_regex)\n",
    "\n",
    "    doi_matches = re_doi.finditer(main_string, re.IGNORECASE)\n",
    "    for match in doi_matches:\n",
    "        extracted_snippet = main_string[max(0, match.start() - context_chars): match.start() + len_search ]\n",
    "        occurrences_with_context.append(extracted_snippet.lower())\n",
    "            \n",
    "    return occurrences_with_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c3c4ead5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unmatched_parentheses(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes non-matching '(' and ')' characters from a string.\n",
    "    A parenthesis is considered matching if it forms a valid pair.\n",
    "\n",
    "    Args:\n",
    "        s (str): The input string.\n",
    "\n",
    "    Returns:\n",
    "        str: The string with all non-matching parentheses removed.\n",
    "    \"\"\"\n",
    "    if not s:\n",
    "        return \"\"\n",
    "\n",
    "    # Use a deque as a stack to store indices of opening parentheses.\n",
    "    # When we find a '(', we push its index. When we find a ')', we pop an index.\n",
    "    open_paren_indices_stack = collections.deque()\n",
    "    \n",
    "    # A boolean list to mark characters that should be kept in the final string.\n",
    "    # Initially, assume all characters are kept. We'll mark unmatched parentheses as False.\n",
    "    keep_char = [True] * len(s)\n",
    "\n",
    "    for i, char in enumerate(s):\n",
    "        if char == '(':\n",
    "            # This is a potential opening parenthesis. Store its index.\n",
    "            open_paren_indices_stack.append(i)\n",
    "        elif char == ')':\n",
    "            if open_paren_indices_stack:\n",
    "                # Found a matching opening parenthesis for this closing one.\n",
    "                # Pop the index of the matched opening parenthesis from the stack.\n",
    "                open_paren_indices_stack.pop()\n",
    "            else:\n",
    "                # This closing parenthesis has no matching opening parenthesis.\n",
    "                # It is unmatched and should be removed.\n",
    "                keep_char[i] = False\n",
    "        # For non-parenthesis characters, keep_char[i] remains True (its default value).\n",
    "    \n",
    "    # After iterating through the entire string, any opening parentheses\n",
    "    # remaining in the stack are unmatched because they never found a closing pair.\n",
    "    # Mark these for removal.\n",
    "    while open_paren_indices_stack:\n",
    "        unmatched_open_idx = open_paren_indices_stack.pop()\n",
    "        keep_char[unmatched_open_idx] = False\n",
    "            \n",
    "    # Construct the final string by iterating through the original string\n",
    "    # and appending only the characters marked to be kept.\n",
    "    final_chars = [s[i] for i, should_keep in enumerate(keep_char) if should_keep]\n",
    "            \n",
    "    return \"\".join(final_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8ca0c8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_doi(doi: str) -> str:\n",
    "    doi = doi.strip()\n",
    "    # Remove non-matching \"(\" and \")\" characters\n",
    "    doi = remove_unmatched_parentheses(doi)\n",
    "    # Remove trailing periods and commas\n",
    "    doi = re.sub(r'[.,]$', '', doi)\n",
    "    # Remove any leading or trailing whitespace\n",
    "    doi = doi.strip()\n",
    "    return doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6d206774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dois_from_text(text) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts DOIs from text using a flexible regex.\n",
    "    (This would use your modified extract_doi_flexible logic,\n",
    "     but here we'll just use a general DOI regex for simplicity in this plan)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    # A common DOI regex pattern (simplified for this example)\n",
    "    # For example, to find any string starting with 10. followed by numbers/dots/slashes\n",
    "    #doi_pattern = r'\\b10\\.\\d{4,9}/[-._;()/:A-Za-z0-9]+\\b'\n",
    "    #doi_pattern = r'\\b10\\.\\s?\\d{4,9}\\/[-._()<>;\\/:A-Za-z0-9\\s]+[-._()<>;\\/:0-9]+'\n",
    "    doi_pattern = r'\\b10\\.\\s?\\d{4,9}\\/[-._()<>;\\/:A-Za-z0-9]+\\s?(?![A-Z]+)+[-._()<>;\\/:A-Za-z0-9]+'\n",
    "    found_dois = set(re.findall(doi_pattern, text))\n",
    "    found_dois = [\"\".join(scrub_doi(doi).split()) for doi in found_dois]  # Clean up whitespace\n",
    "    return list(set(found_dois))  # Return unique DOIs)\n",
    "\n",
    "\n",
    "def extract_dataset_ids(article_id: str, text: str, extracted_dois: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Identifies dataset IDs. For now, let's assume dataset_ids are primarily the DOIs found.\n",
    "    This function can be expanded with more regexes for other ID types.\n",
    "    \"\"\"\n",
    "    article_id_slashed = article_id.replace('_', '/')  # Replace dashes with slashes to match DOI format\n",
    "\n",
    "    # For this version, we'll consider all found DOIs as potential dataset_ids\n",
    "    # You might want to add more sophisticated logic here to find non-DOI dataset IDs\n",
    "    data_related_dois = []\n",
    "    for doi in extracted_dois:\n",
    "        # print(f\"Processing DOI: {doi}\")\n",
    "        # Skip if DOI is empty, NaN, or matches the article_id_slashed\n",
    "        if pd.isna(doi) or not doi or doi.lower() == article_id_slashed.lower():\n",
    "            continue\n",
    "        # Normalize the DOI to a regex format\n",
    "        regex_id = get_dataset_id_regex(doi)\n",
    "        # Find occurrences of this DOI in the text with context\n",
    "        occurrences = find_regex_with_context(text, regex_id)\n",
    "        # Check if any string in occurrences contains 'data' (case-insensitive)\n",
    "        if occurrences and any(('dataset' in s or 'database' in s) for s in occurrences):\n",
    "            data_related_dois.append(doi)\n",
    "    return data_related_dois \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b80f995",
   "metadata": {},
   "source": [
    "## 4. LLM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bccf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global LLM model and tokenizer (load once)\n",
    "llm_model = None\n",
    "llm_tokenizer = None\n",
    "device = \"cuda\" if torch and torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def load_llm():\n",
    "    global llm_model, llm_tokenizer\n",
    "    if not AutoModelForCausalLM or not QWEN_MODEL_NAME_OR_PATH:\n",
    "        print(\"LLM components not available or path not set. Skipping LLM loading.\")\n",
    "        return False\n",
    "    try:\n",
    "        print(f\"Loading Qwen tokenizer from: {QWEN_MODEL_NAME_OR_PATH}\")\n",
    "        llm_tokenizer = AutoTokenizer.from_pretrained(QWEN_MODEL_NAME_OR_PATH, trust_remote_code=True)\n",
    "        print(f\"Loading Qwen model from: {QWEN_MODEL_NAME_OR_PATH}\")\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "            QWEN_MODEL_NAME_OR_PATH,\n",
    "            device_map=\"auto\", # Automatically uses GPU if available\n",
    "            trust_remote_code=True\n",
    "        ).eval() # Set to evaluation mode\n",
    "        print(f\"LLM loaded successfully on {llm_model.device}.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading LLM: {e}\")\n",
    "        return False\n",
    "\n",
    "def generate_llm_classification(article_text_snippet, dataset_id):\n",
    "    \"\"\"\n",
    "    Uses the LLM to classify dataset usage.\n",
    "    article_text_snippet: A relevant portion of article text, or full text if manageable.\n",
    "    \"\"\"\n",
    "    if not llm_model or not llm_tokenizer:\n",
    "        print(\"LLM not loaded. Cannot classify.\")\n",
    "        return \"Error: LLM not loaded\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert research assistant. Your task is to determine how a dataset was used in a research article.\n",
    "    Read the following article context and the dataset identifier carefully.\n",
    "\n",
    "    Article Context (excerpt):\n",
    "    \"{article_text_snippet[:4000]}\"  # Truncate for context window, adjust as needed\n",
    "\n",
    "    Dataset Identifier: \"{dataset_id}\"\n",
    "\n",
    "    Question: Based on the provided article context, was the dataset (identified as \"{dataset_id}\"):\n",
    "    1. Created by the authors primarily for the research described in THIS article? (If so, it's \"Primary\")\n",
    "    2. An existing dataset that the authors obtained and used for their research in THIS article? (If so, it's \"Secondary\")\n",
    "\n",
    "    Please respond with only one word: \"Primary\" or \"Secondary\".\n",
    "    \"\"\"\n",
    "    \n",
    "    # This is a simplified generation example for Qwen.\n",
    "    # You'll need to adapt this to the specific chat/completion format Qwen expects.\n",
    "    # For many chat models, it's a list of messages:\n",
    "    # messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    # inputs = llm_tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Simpler non-chat completion style (check Qwen docs for best practice):\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=llm_tokenizer.model_max_length - 50).to(llm_model.device) # Reserve some tokens for generation\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad(): # Important for inference\n",
    "            outputs = llm_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10, # We only expect \"Primary\" or \"Secondary\"\n",
    "                pad_token_id=llm_tokenizer.eos_token_id # Important for some models\n",
    "            )\n",
    "        \n",
    "        # Decode the generated tokens, skipping special tokens and the prompt\n",
    "        response_text = llm_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
    "        \n",
    "        print(f\"LLM raw response for {dataset_id}: '{response_text}'\")\n",
    "\n",
    "        if \"Primary\" in response_text:\n",
    "            return \"Primary\"\n",
    "        elif \"Secondary\" in response_text:\n",
    "            return \"Secondary\"\n",
    "        else:\n",
    "            print(f\"Warning: LLM response for {dataset_id} not clearly Primary/Secondary: '{response_text}'\")\n",
    "            return \"Uncertain\" # Or handle as per your logic\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM generation for {dataset_id}: {e}\")\n",
    "        return \"Error: LLM generation failed\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50dd97b",
   "metadata": {},
   "source": [
    "## 5. Main Processing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1771c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_article_files(base_directory):\n",
    "    \"\"\"\n",
    "    Returns a list of all article files (PDF and XML) in the specified base_directory.\n",
    "    \"\"\"\n",
    "    # Collect files from all formats and flatten the list\n",
    "    all_article_files = [glob.glob(os.path.join(base_directory, fmt['format'], f\"*{fmt['ext']}\")) for fmt in ARTICLE_FORMATS]  \n",
    "    return [item for sublist in all_article_files for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b2e1e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 5. Main Processing Logic ---\n",
    "def process_articles(articles_directory):\n",
    "    results = []\n",
    "    \n",
    "    # if not load_llm(): # Attempt to load LLM once\n",
    "    #     print(\"Proceeding without LLM classification.\")\n",
    "\n",
    "    article_files = get_all_article_files(articles_directory)\n",
    "\n",
    "    for i, filepath in enumerate(article_files):\n",
    "        print(f\"\\nProcessing article {i+1}/{len(article_files)}: {os.path.basename(filepath)}\")\n",
    "        article_id = os.path.splitext(os.path.basename(filepath))[0]\n",
    "        article_format = os.path.splitext(filepath)[1].lower()\n",
    "\n",
    "        # Read the text content from the file\n",
    "        text_content = None\n",
    "        if filepath.endswith(\".pdf\"):\n",
    "            text_content = read_pdf_text(filepath)\n",
    "        elif filepath.endswith(\".xml\"):\n",
    "            text_content = read_xml_text(filepath)\n",
    "\n",
    "        if not text_content:\n",
    "            print(f\"Could not extract text from {filepath}. Skipping.\")\n",
    "            results.append({\n",
    "                \"article_id\": article_id,\n",
    "                \"dataset_id\": \"N/A\",\n",
    "                \"dataset_id_raw\": \"N/A\",\n",
    "                \"article_format\": article_format,\n",
    "                \"classification_label\": \"Error: No text\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Extract DOIs and other potential dataset IDs\n",
    "        extracted_dois = extract_dois_from_text(text_content)\n",
    "        dataset_ids_found = extract_dataset_ids(article_id, text_content, extracted_dois)\n",
    "\n",
    "        if not dataset_ids_found:\n",
    "            print(f\"No dataset IDs/DOIs found in {article_id}.\")\n",
    "            results.append({\n",
    "                \"article_id\": article_id,\n",
    "                \"dataset_id\": \"Missing\",\n",
    "                \"dataset_id_raw\": \"Missing\",\n",
    "                \"article_format\": article_format,\n",
    "                \"classification_label\": \"Missing\"\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Found {len(dataset_ids_found)} potential dataset(s)/DOI(s) in {article_id}: {dataset_ids_found}\")\n",
    "            for ds_id in dataset_ids_found:\n",
    "                # For LLM classification, provide context.\n",
    "                # You might want to be more selective about the text snippet.\n",
    "                # For now, using the beginning of the text.\n",
    "                classification = \"LLM_Disabled\"\n",
    "                # if llm_model and llm_tokenizer: # Check if LLM is loaded\n",
    "                #     classification = generate_llm_classification(text_content, ds_id)\n",
    "\n",
    "                results.append({\n",
    "                    \"article_id\": article_id,\n",
    "                    \"dataset_id\": f\"https://doi.org/{ds_id}\",\n",
    "                    \"dataset_id_raw\": ds_id,\n",
    "                    \"article_format\": article_format,\n",
    "                    \"classification_label\": classification\n",
    "                })\n",
    "\n",
    "    # Convert results to a DataFrame and sort by article_id, dataset_id, and article_format\n",
    "    return pd.DataFrame(results).sort_values(by=[\"article_id\", \"dataset_id\", \"article_format\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5f008cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing article 1/55: 10.1002_2017jc013030.pdf\n",
      "Found 3 potential dataset(s)/DOI(s) in 10.1002_2017jc013030: ['10.17882/47142', '10.17882/49388', '10.5194/essd-9-861-2017']\n",
      "\n",
      "Processing article 2/55: 10.1002_anie.201916483.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_anie.201916483.\n",
      "\n",
      "Processing article 3/55: 10.1002_anie.202005531.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_anie.202005531.\n",
      "\n",
      "Processing article 4/55: 10.1002_anie.202007717.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_anie.202007717.\n",
      "\n",
      "Processing article 5/55: 10.1002_chem.201902131.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_chem.201902131.\n",
      "\n",
      "Processing article 6/55: 10.1002_chem.201903120.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_chem.201903120.\n",
      "\n",
      "Processing article 7/55: 10.1002_chem.202000235.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_chem.202000235.\n",
      "\n",
      "Processing article 8/55: 10.1002_chem.202001412.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_chem.202001412.\n",
      "\n",
      "Processing article 9/55: 10.1002_chem.202001668.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_chem.202001668.\n",
      "\n",
      "Processing article 10/55: 10.1002_chem.202003167.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_chem.202003167.\n",
      "\n",
      "Processing article 11/55: 10.1002_cssc.202201821.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_cssc.202201821.\n",
      "\n",
      "Processing article 12/55: 10.1002_ece3.3985.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.3985.\n",
      "\n",
      "Processing article 13/55: 10.1002_ece3.4466.pdf\n",
      "Found 1 potential dataset(s)/DOI(s) in 10.1002_ece3.4466: ['10.5061/dryad.r6nq870']\n",
      "\n",
      "Processing article 14/55: 10.1002_ece3.5260.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.5260.\n",
      "\n",
      "Processing article 15/55: 10.1002_ece3.5395.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.5395.\n",
      "\n",
      "Processing article 16/55: 10.1002_ece3.6144.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.6144.\n",
      "\n",
      "Processing article 17/55: 10.1002_ece3.6303.pdf\n",
      "Found 1 potential dataset(s)/DOI(s) in 10.1002_ece3.6303: ['10.5061/dryad.37pvm']\n",
      "\n",
      "Processing article 18/55: 10.1002_ece3.6784.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.6784.\n",
      "\n",
      "Processing article 19/55: 10.1002_ece3.961.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.961.\n",
      "\n",
      "Processing article 20/55: 10.1002_ece3.9627.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.9627.\n",
      "\n",
      "Processing article 21/55: 10.1002_ecs2.1280.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_ecs2.1280.\n",
      "\n",
      "Processing article 22/55: 10.1002_ecs2.4619.pdf\n",
      "Found 1 potential dataset(s)/DOI(s) in 10.1002_ecs2.4619: ['10.25349/D9QW5X']\n",
      "\n",
      "Processing article 23/55: 10.1002_ejic.201900904.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_ejic.201900904.\n",
      "\n",
      "Processing article 24/55: 10.1002_ejoc.202000139.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_ejoc.202000139.\n",
      "\n",
      "Processing article 25/55: 10.1002_ejoc.202000916.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_ejoc.202000916.\n",
      "\n",
      "Processing article 26/55: 10.1002_esp.5058.pdf\n",
      "Found 2 potential dataset(s)/DOI(s) in 10.1002_esp.5058: ['10.1002/esp.4637', '10.5061/dryad.jh9w0vt9t']\n",
      "\n",
      "Processing article 27/55: 10.1002_esp.5090.pdf\n",
      "Found 1 potential dataset(s)/DOI(s) in 10.1002_esp.5090: ['10.1080/08120090802546977']\n",
      "\n",
      "Processing article 28/55: 10.1002_mp.14424.pdf\n",
      "Found 1 potential dataset(s)/DOI(s) in 10.1002_mp.14424: ['10.7937/tcia.2020.6c7y-gq39']\n",
      "\n",
      "Processing article 29/55: 10.1002_nafm.10870.pdf\n",
      "No dataset IDs/DOIs found in 10.1002_nafm.10870.\n",
      "\n",
      "Processing article 30/55: 10.1007_jhep07(2018)134.pdf\n",
      "No dataset IDs/DOIs found in 10.1007_jhep07(2018)134.\n",
      "\n",
      "Processing article 31/55: 10.1002_2017jc013030.xml\n",
      "Found 3 potential dataset(s)/DOI(s) in 10.1002_2017jc013030: ['10.5194/essd-2017-58', '10.17882/47142', '10.17882/49388']\n",
      "\n",
      "Processing article 32/55: 10.1002_anie.201916483.xml\n",
      "No dataset IDs/DOIs found in 10.1002_anie.201916483.\n",
      "\n",
      "Processing article 33/55: 10.1002_anie.202005531.xml\n",
      "No dataset IDs/DOIs found in 10.1002_anie.202005531.\n",
      "\n",
      "Processing article 34/55: 10.1002_anie.202007717.xml\n",
      "No dataset IDs/DOIs found in 10.1002_anie.202007717.\n",
      "\n",
      "Processing article 35/55: 10.1002_chem.201902131.xml\n",
      "No dataset IDs/DOIs found in 10.1002_chem.201902131.\n",
      "\n",
      "Processing article 36/55: 10.1002_chem.201903120.xml\n",
      "No dataset IDs/DOIs found in 10.1002_chem.201903120.\n",
      "\n",
      "Processing article 37/55: 10.1002_chem.202000235.xml\n",
      "No dataset IDs/DOIs found in 10.1002_chem.202000235.\n",
      "\n",
      "Processing article 38/55: 10.1002_chem.202001412.xml\n",
      "No dataset IDs/DOIs found in 10.1002_chem.202001412.\n",
      "\n",
      "Processing article 39/55: 10.1002_chem.202001668.xml\n",
      "No dataset IDs/DOIs found in 10.1002_chem.202001668.\n",
      "\n",
      "Processing article 40/55: 10.1002_chem.202003167.xml\n",
      "No dataset IDs/DOIs found in 10.1002_chem.202003167.\n",
      "\n",
      "Processing article 41/55: 10.1002_cssc.202201821.xml\n",
      "No dataset IDs/DOIs found in 10.1002_cssc.202201821.\n",
      "\n",
      "Processing article 42/55: 10.1002_ece3.3985.xml\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.3985.\n",
      "\n",
      "Processing article 43/55: 10.1002_ece3.4466.xml\n",
      "Found 1 potential dataset(s)/DOI(s) in 10.1002_ece3.4466: ['10.5061/dryad.r6nq870']\n",
      "\n",
      "Processing article 44/55: 10.1002_ece3.5260.xml\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.5260.\n",
      "\n",
      "Processing article 45/55: 10.1002_ece3.5395.xml\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.5395.\n",
      "\n",
      "Processing article 46/55: 10.1002_ece3.6144.xml\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.6144.\n",
      "\n",
      "Processing article 47/55: 10.1002_ece3.6303.xml\n",
      "Found 1 potential dataset(s)/DOI(s) in 10.1002_ece3.6303: ['10.5061/dryad.37pvmcvgb']\n",
      "\n",
      "Processing article 48/55: 10.1002_ece3.6784.xml\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.6784.\n",
      "\n",
      "Processing article 49/55: 10.1002_ece3.961.xml\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.961.\n",
      "\n",
      "Processing article 50/55: 10.1002_ece3.9627.xml\n",
      "No dataset IDs/DOIs found in 10.1002_ece3.9627.\n",
      "\n",
      "Processing article 51/55: 10.1002_ecs2.4619.xml\n",
      "Found 1 potential dataset(s)/DOI(s) in 10.1002_ecs2.4619: ['10.25349/D9QW5X']\n",
      "\n",
      "Processing article 52/55: 10.1002_ejic.201900904.xml\n",
      "No dataset IDs/DOIs found in 10.1002_ejic.201900904.\n",
      "\n",
      "Processing article 53/55: 10.1002_ejoc.202000916.xml\n",
      "No dataset IDs/DOIs found in 10.1002_ejoc.202000916.\n",
      "\n",
      "Processing article 54/55: 10.1002_esp.5090.xml\n",
      "Found 1 potential dataset(s)/DOI(s) in 10.1002_esp.5090: ['10.1080/08120090802546977']\n",
      "\n",
      "Processing article 55/55: 10.1002_mp.14424.xml\n",
      "Found 1 potential dataset(s)/DOI(s) in 10.1002_mp.14424: ['10.7937/tcia.2020.6c7y']\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_id_raw",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "article_format",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "classification_label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "19825ed9-15cc-4145-9dfd-51b29ed28d2d",
       "rows": [
        [
         "0",
         "10.1002_2017jc013030",
         "https://doi.org/10.17882/47142",
         "10.17882/47142",
         ".pdf",
         "LLM_Disabled"
        ],
        [
         "1",
         "10.1002_2017jc013030",
         "https://doi.org/10.17882/47142",
         "10.17882/47142",
         ".xml",
         "LLM_Disabled"
        ],
        [
         "2",
         "10.1002_2017jc013030",
         "https://doi.org/10.17882/49388",
         "10.17882/49388",
         ".pdf",
         "LLM_Disabled"
        ],
        [
         "3",
         "10.1002_2017jc013030",
         "https://doi.org/10.17882/49388",
         "10.17882/49388",
         ".xml",
         "LLM_Disabled"
        ],
        [
         "4",
         "10.1002_2017jc013030",
         "https://doi.org/10.5194/essd-2017-58",
         "10.5194/essd-2017-58",
         ".xml",
         "LLM_Disabled"
        ],
        [
         "5",
         "10.1002_2017jc013030",
         "https://doi.org/10.5194/essd-9-861-2017",
         "10.5194/essd-9-861-2017",
         ".pdf",
         "LLM_Disabled"
        ],
        [
         "6",
         "10.1002_anie.201916483",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "7",
         "10.1002_anie.201916483",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "8",
         "10.1002_anie.202005531",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "9",
         "10.1002_anie.202005531",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "10",
         "10.1002_anie.202007717",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "11",
         "10.1002_anie.202007717",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "12",
         "10.1002_chem.201902131",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "13",
         "10.1002_chem.201902131",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "14",
         "10.1002_chem.201903120",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "15",
         "10.1002_chem.201903120",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "16",
         "10.1002_chem.202000235",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "17",
         "10.1002_chem.202000235",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "18",
         "10.1002_chem.202001412",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "19",
         "10.1002_chem.202001412",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "20",
         "10.1002_chem.202001668",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "21",
         "10.1002_chem.202001668",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "22",
         "10.1002_chem.202003167",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "23",
         "10.1002_chem.202003167",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "24",
         "10.1002_cssc.202201821",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "25",
         "10.1002_cssc.202201821",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "26",
         "10.1002_ece3.3985",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "27",
         "10.1002_ece3.3985",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "28",
         "10.1002_ece3.4466",
         "https://doi.org/10.5061/dryad.r6nq870",
         "10.5061/dryad.r6nq870",
         ".pdf",
         "LLM_Disabled"
        ],
        [
         "29",
         "10.1002_ece3.4466",
         "https://doi.org/10.5061/dryad.r6nq870",
         "10.5061/dryad.r6nq870",
         ".xml",
         "LLM_Disabled"
        ],
        [
         "30",
         "10.1002_ece3.5260",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "31",
         "10.1002_ece3.5260",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "32",
         "10.1002_ece3.5395",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "33",
         "10.1002_ece3.5395",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "34",
         "10.1002_ece3.6144",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "35",
         "10.1002_ece3.6144",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "36",
         "10.1002_ece3.6303",
         "https://doi.org/10.5061/dryad.37pvm",
         "10.5061/dryad.37pvm",
         ".pdf",
         "LLM_Disabled"
        ],
        [
         "37",
         "10.1002_ece3.6303",
         "https://doi.org/10.5061/dryad.37pvmcvgb",
         "10.5061/dryad.37pvmcvgb",
         ".xml",
         "LLM_Disabled"
        ],
        [
         "38",
         "10.1002_ece3.6784",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "39",
         "10.1002_ece3.6784",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "40",
         "10.1002_ece3.961",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "41",
         "10.1002_ece3.961",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "42",
         "10.1002_ece3.9627",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "43",
         "10.1002_ece3.9627",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "44",
         "10.1002_ecs2.1280",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "45",
         "10.1002_ecs2.4619",
         "https://doi.org/10.25349/D9QW5X",
         "10.25349/D9QW5X",
         ".pdf",
         "LLM_Disabled"
        ],
        [
         "46",
         "10.1002_ecs2.4619",
         "https://doi.org/10.25349/D9QW5X",
         "10.25349/D9QW5X",
         ".xml",
         "LLM_Disabled"
        ],
        [
         "47",
         "10.1002_ejic.201900904",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ],
        [
         "48",
         "10.1002_ejic.201900904",
         "Missing",
         "Missing",
         ".xml",
         "Missing"
        ],
        [
         "49",
         "10.1002_ejoc.202000139",
         "Missing",
         "Missing",
         ".pdf",
         "Missing"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 60
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>dataset_id</th>\n",
       "      <th>dataset_id_raw</th>\n",
       "      <th>article_format</th>\n",
       "      <th>classification_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/47142</td>\n",
       "      <td>10.17882/47142</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/47142</td>\n",
       "      <td>10.17882/47142</td>\n",
       "      <td>.xml</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/49388</td>\n",
       "      <td>10.17882/49388</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.17882/49388</td>\n",
       "      <td>10.17882/49388</td>\n",
       "      <td>.xml</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.5194/essd-2017-58</td>\n",
       "      <td>10.5194/essd-2017-58</td>\n",
       "      <td>.xml</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1002_2017jc013030</td>\n",
       "      <td>https://doi.org/10.5194/essd-9-861-2017</td>\n",
       "      <td>10.5194/essd-9-861-2017</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.1002_anie.202005531</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.1002_anie.202007717</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.1002_anie.202007717</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.1002_chem.201902131</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.1002_chem.201902131</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.1002_chem.201903120</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.1002_chem.201903120</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.1002_chem.202000235</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.1002_chem.202000235</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.1002_chem.202001412</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.1002_chem.202001412</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.1002_chem.202001668</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.1002_chem.202001668</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.1002_chem.202003167</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.1002_chem.202003167</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.1002_cssc.202201821</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.1002_cssc.202201821</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.1002_ece3.3985</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10.1002_ece3.3985</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.1002_ece3.4466</td>\n",
       "      <td>https://doi.org/10.5061/dryad.r6nq870</td>\n",
       "      <td>10.5061/dryad.r6nq870</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.1002_ece3.4466</td>\n",
       "      <td>https://doi.org/10.5061/dryad.r6nq870</td>\n",
       "      <td>10.5061/dryad.r6nq870</td>\n",
       "      <td>.xml</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10.1002_ece3.5260</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10.1002_ece3.5260</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10.1002_ece3.5395</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10.1002_ece3.5395</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10.1002_ece3.6144</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10.1002_ece3.6144</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10.1002_ece3.6303</td>\n",
       "      <td>https://doi.org/10.5061/dryad.37pvm</td>\n",
       "      <td>10.5061/dryad.37pvm</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10.1002_ece3.6303</td>\n",
       "      <td>https://doi.org/10.5061/dryad.37pvmcvgb</td>\n",
       "      <td>10.5061/dryad.37pvmcvgb</td>\n",
       "      <td>.xml</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10.1002_ece3.6784</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10.1002_ece3.6784</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10.1002_ece3.961</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10.1002_ece3.961</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10.1002_ece3.9627</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>10.1002_ece3.9627</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>10.1002_ecs2.1280</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>10.1002_ecs2.4619</td>\n",
       "      <td>https://doi.org/10.25349/D9QW5X</td>\n",
       "      <td>10.25349/D9QW5X</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10.1002_ecs2.4619</td>\n",
       "      <td>https://doi.org/10.25349/D9QW5X</td>\n",
       "      <td>10.25349/D9QW5X</td>\n",
       "      <td>.xml</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>10.1002_ejic.201900904</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>10.1002_ejic.201900904</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>10.1002_ejoc.202000139</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>10.1002_ejoc.202000916</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>10.1002_ejoc.202000916</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.xml</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>10.1002_esp.5058</td>\n",
       "      <td>https://doi.org/10.1002/esp.4637</td>\n",
       "      <td>10.1002/esp.4637</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>10.1002_esp.5058</td>\n",
       "      <td>https://doi.org/10.5061/dryad.jh9w0vt9t</td>\n",
       "      <td>10.5061/dryad.jh9w0vt9t</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>10.1002_esp.5090</td>\n",
       "      <td>https://doi.org/10.1080/08120090802546977</td>\n",
       "      <td>10.1080/08120090802546977</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>10.1002_esp.5090</td>\n",
       "      <td>https://doi.org/10.1080/08120090802546977</td>\n",
       "      <td>10.1080/08120090802546977</td>\n",
       "      <td>.xml</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>10.1002_mp.14424</td>\n",
       "      <td>https://doi.org/10.7937/tcia.2020.6c7y</td>\n",
       "      <td>10.7937/tcia.2020.6c7y</td>\n",
       "      <td>.xml</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>10.1002_mp.14424</td>\n",
       "      <td>https://doi.org/10.7937/tcia.2020.6c7y-gq39</td>\n",
       "      <td>10.7937/tcia.2020.6c7y-gq39</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>LLM_Disabled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>10.1002_nafm.10870</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>10.1007_jhep07(2018)134</td>\n",
       "      <td>Missing</td>\n",
       "      <td>Missing</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>Missing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 article_id                                   dataset_id  \\\n",
       "0      10.1002_2017jc013030               https://doi.org/10.17882/47142   \n",
       "1      10.1002_2017jc013030               https://doi.org/10.17882/47142   \n",
       "2      10.1002_2017jc013030               https://doi.org/10.17882/49388   \n",
       "3      10.1002_2017jc013030               https://doi.org/10.17882/49388   \n",
       "4      10.1002_2017jc013030         https://doi.org/10.5194/essd-2017-58   \n",
       "5      10.1002_2017jc013030      https://doi.org/10.5194/essd-9-861-2017   \n",
       "6    10.1002_anie.201916483                                      Missing   \n",
       "7    10.1002_anie.201916483                                      Missing   \n",
       "8    10.1002_anie.202005531                                      Missing   \n",
       "9    10.1002_anie.202005531                                      Missing   \n",
       "10   10.1002_anie.202007717                                      Missing   \n",
       "11   10.1002_anie.202007717                                      Missing   \n",
       "12   10.1002_chem.201902131                                      Missing   \n",
       "13   10.1002_chem.201902131                                      Missing   \n",
       "14   10.1002_chem.201903120                                      Missing   \n",
       "15   10.1002_chem.201903120                                      Missing   \n",
       "16   10.1002_chem.202000235                                      Missing   \n",
       "17   10.1002_chem.202000235                                      Missing   \n",
       "18   10.1002_chem.202001412                                      Missing   \n",
       "19   10.1002_chem.202001412                                      Missing   \n",
       "20   10.1002_chem.202001668                                      Missing   \n",
       "21   10.1002_chem.202001668                                      Missing   \n",
       "22   10.1002_chem.202003167                                      Missing   \n",
       "23   10.1002_chem.202003167                                      Missing   \n",
       "24   10.1002_cssc.202201821                                      Missing   \n",
       "25   10.1002_cssc.202201821                                      Missing   \n",
       "26        10.1002_ece3.3985                                      Missing   \n",
       "27        10.1002_ece3.3985                                      Missing   \n",
       "28        10.1002_ece3.4466        https://doi.org/10.5061/dryad.r6nq870   \n",
       "29        10.1002_ece3.4466        https://doi.org/10.5061/dryad.r6nq870   \n",
       "30        10.1002_ece3.5260                                      Missing   \n",
       "31        10.1002_ece3.5260                                      Missing   \n",
       "32        10.1002_ece3.5395                                      Missing   \n",
       "33        10.1002_ece3.5395                                      Missing   \n",
       "34        10.1002_ece3.6144                                      Missing   \n",
       "35        10.1002_ece3.6144                                      Missing   \n",
       "36        10.1002_ece3.6303          https://doi.org/10.5061/dryad.37pvm   \n",
       "37        10.1002_ece3.6303      https://doi.org/10.5061/dryad.37pvmcvgb   \n",
       "38        10.1002_ece3.6784                                      Missing   \n",
       "39        10.1002_ece3.6784                                      Missing   \n",
       "40         10.1002_ece3.961                                      Missing   \n",
       "41         10.1002_ece3.961                                      Missing   \n",
       "42        10.1002_ece3.9627                                      Missing   \n",
       "43        10.1002_ece3.9627                                      Missing   \n",
       "44        10.1002_ecs2.1280                                      Missing   \n",
       "45        10.1002_ecs2.4619              https://doi.org/10.25349/D9QW5X   \n",
       "46        10.1002_ecs2.4619              https://doi.org/10.25349/D9QW5X   \n",
       "47   10.1002_ejic.201900904                                      Missing   \n",
       "48   10.1002_ejic.201900904                                      Missing   \n",
       "49   10.1002_ejoc.202000139                                      Missing   \n",
       "50   10.1002_ejoc.202000916                                      Missing   \n",
       "51   10.1002_ejoc.202000916                                      Missing   \n",
       "52         10.1002_esp.5058             https://doi.org/10.1002/esp.4637   \n",
       "53         10.1002_esp.5058      https://doi.org/10.5061/dryad.jh9w0vt9t   \n",
       "54         10.1002_esp.5090    https://doi.org/10.1080/08120090802546977   \n",
       "55         10.1002_esp.5090    https://doi.org/10.1080/08120090802546977   \n",
       "56         10.1002_mp.14424       https://doi.org/10.7937/tcia.2020.6c7y   \n",
       "57         10.1002_mp.14424  https://doi.org/10.7937/tcia.2020.6c7y-gq39   \n",
       "58       10.1002_nafm.10870                                      Missing   \n",
       "59  10.1007_jhep07(2018)134                                      Missing   \n",
       "\n",
       "                 dataset_id_raw article_format classification_label  \n",
       "0                10.17882/47142           .pdf         LLM_Disabled  \n",
       "1                10.17882/47142           .xml         LLM_Disabled  \n",
       "2                10.17882/49388           .pdf         LLM_Disabled  \n",
       "3                10.17882/49388           .xml         LLM_Disabled  \n",
       "4          10.5194/essd-2017-58           .xml         LLM_Disabled  \n",
       "5       10.5194/essd-9-861-2017           .pdf         LLM_Disabled  \n",
       "6                       Missing           .pdf              Missing  \n",
       "7                       Missing           .xml              Missing  \n",
       "8                       Missing           .pdf              Missing  \n",
       "9                       Missing           .xml              Missing  \n",
       "10                      Missing           .pdf              Missing  \n",
       "11                      Missing           .xml              Missing  \n",
       "12                      Missing           .pdf              Missing  \n",
       "13                      Missing           .xml              Missing  \n",
       "14                      Missing           .pdf              Missing  \n",
       "15                      Missing           .xml              Missing  \n",
       "16                      Missing           .pdf              Missing  \n",
       "17                      Missing           .xml              Missing  \n",
       "18                      Missing           .pdf              Missing  \n",
       "19                      Missing           .xml              Missing  \n",
       "20                      Missing           .pdf              Missing  \n",
       "21                      Missing           .xml              Missing  \n",
       "22                      Missing           .pdf              Missing  \n",
       "23                      Missing           .xml              Missing  \n",
       "24                      Missing           .pdf              Missing  \n",
       "25                      Missing           .xml              Missing  \n",
       "26                      Missing           .pdf              Missing  \n",
       "27                      Missing           .xml              Missing  \n",
       "28        10.5061/dryad.r6nq870           .pdf         LLM_Disabled  \n",
       "29        10.5061/dryad.r6nq870           .xml         LLM_Disabled  \n",
       "30                      Missing           .pdf              Missing  \n",
       "31                      Missing           .xml              Missing  \n",
       "32                      Missing           .pdf              Missing  \n",
       "33                      Missing           .xml              Missing  \n",
       "34                      Missing           .pdf              Missing  \n",
       "35                      Missing           .xml              Missing  \n",
       "36          10.5061/dryad.37pvm           .pdf         LLM_Disabled  \n",
       "37      10.5061/dryad.37pvmcvgb           .xml         LLM_Disabled  \n",
       "38                      Missing           .pdf              Missing  \n",
       "39                      Missing           .xml              Missing  \n",
       "40                      Missing           .pdf              Missing  \n",
       "41                      Missing           .xml              Missing  \n",
       "42                      Missing           .pdf              Missing  \n",
       "43                      Missing           .xml              Missing  \n",
       "44                      Missing           .pdf              Missing  \n",
       "45              10.25349/D9QW5X           .pdf         LLM_Disabled  \n",
       "46              10.25349/D9QW5X           .xml         LLM_Disabled  \n",
       "47                      Missing           .pdf              Missing  \n",
       "48                      Missing           .xml              Missing  \n",
       "49                      Missing           .pdf              Missing  \n",
       "50                      Missing           .pdf              Missing  \n",
       "51                      Missing           .xml              Missing  \n",
       "52             10.1002/esp.4637           .pdf         LLM_Disabled  \n",
       "53      10.5061/dryad.jh9w0vt9t           .pdf         LLM_Disabled  \n",
       "54    10.1080/08120090802546977           .pdf         LLM_Disabled  \n",
       "55    10.1080/08120090802546977           .xml         LLM_Disabled  \n",
       "56       10.7937/tcia.2020.6c7y           .xml         LLM_Disabled  \n",
       "57  10.7937/tcia.2020.6c7y-gq39           .pdf         LLM_Disabled  \n",
       "58                      Missing           .pdf              Missing  \n",
       "59                      Missing           .pdf              Missing  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = process_articles(ARTICLES_TEST_DIR)\n",
    "result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ad1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 6. Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure the ARTICLES_DIR exists or adjust path\n",
    "    if not os.path.isdir(ARTICLES_DIR):\n",
    "        print(f\"Articles directory not found: {ARTICLES_DIR}\")\n",
    "        print(\"Please create dummy files or point to a valid directory for testing.\")\n",
    "        # Create dummy files for a quick test if ARTICLES_DIR is missing\n",
    "        # This part is for local testing, remove or adapt for Kaggle\n",
    "        if ARTICLES_DIR == \"/kaggle/input/research-articles-dataset/articles/\": # Be careful with this\n",
    "             print(\"Cannot create dummy files in /kaggle/input. Please provide data via Kaggle Datasets.\")\n",
    "        else: # Local testing\n",
    "            os.makedirs(ARTICLES_DIR, exist_ok=True)\n",
    "            with open(os.path.join(ARTICLES_DIR, \"article1.pdf\"), \"w\") as f: f.write(\"Dummy PDF with DOI 10.1234/foo.bar and dataset created by us.\") # Needs actual PDF content\n",
    "            with open(os.path.join(ARTICLES_DIR, \"article2.xml\"), \"w\") as f: f.write(\"<root><text>Used dataset 10.5678/baz.qux from another study.</text></root>\")\n",
    "    \n",
    "    print(\"Starting article processing...\")\n",
    "    df_results = process_articles(ARTICLES_DIR)\n",
    "    \n",
    "    print(\"\\n--- Results ---\")\n",
    "    print(df_results.head())\n",
    "    \n",
    "    df_results.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "    print(f\"\\nResults saved to {OUTPUT_CSV_PATH}\")\n",
    "\n",
    "    # If you have training data, you can load it here and compare/evaluate\n",
    "    # Example:\n",
    "    # if os.path.exists(TRAINING_DATA_PATH):\n",
    "    #     df_train = pd.read_csv(TRAINING_DATA_PATH)\n",
    "    #     print(\"\\nTraining Data Head:\")\n",
    "    #     print(df_train.head())\n",
    "    #     # ... further evaluation logic ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
