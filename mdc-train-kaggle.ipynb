{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3195093d",
   "metadata": {},
   "source": [
    "#### 1. Setup and Dependencies\n",
    "\n",
    "First, we'll ensure all necessary libraries are installed. Given your previous work with `lxml`, `PyMuPDF`, and `spaCy`, we'll include those for text extraction and potentially more advanced NLP preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ca2607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Using GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# 1.1. Install necessary libraries\n",
    "# Use !pip install for notebook environment\n",
    "# !pip install transformers trl accelerate bitsandbytes sentencepiece lxml PyMuPDF spacy peft\n",
    "# !python -m spacy download en_core_web_sm # Download a small spaCy model\n",
    "\n",
    "# 1.2. Import Libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Set, List, Optional, Dict, Any\n",
    "\n",
    "import fitz # PyMuPDF\n",
    "from lxml import etree # For XML parsing\n",
    "import spacy\n",
    "import kagglehub\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset #, load_metric\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For KaggleHub integration (assuming it's set up or models are downloaded)\n",
    "# You might need to install kagglehub if you plan to use it directly for model download\n",
    "# !pip install kagglehub\n",
    "\n",
    "# 1.3. Configure CUDA for local GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available! Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache() # Clear GPU memory\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b02acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants for file paths and model configurations\n",
    "BASE_INPUT_DIR = './kaggle/input/make-data-count-finding-data-references'\n",
    "BASE_OUTPUT_DIR = \"./kaggle/working\"\n",
    "\n",
    "# Define directories for articles in train and test sets\n",
    "TRAIN_DATA_DIR = os.path.join(BASE_INPUT_DIR, 'train')\n",
    "TEST_DATA_DIR = os.path.join(BASE_INPUT_DIR, 'test')\n",
    "TRAIN_LABELS_PATH = os.path.join(BASE_INPUT_DIR, 'train_labels.csv')\n",
    "\n",
    "# Define the base model path\n",
    "QWEN_BASE_MODEL_PATH = kagglehub.model_download(\"qwen-lm/qwen-3/transformers/0.6b\")\n",
    "\n",
    "# Output directory for the fine-tuned model and results\n",
    "FINE_TUNED_MODEL_OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, \"qwen_finetuned_dataset_classifier\")\n",
    "SAMPLE_SUBMISSION_PATH = os.path.join(BASE_OUTPUT_DIR, \"submission.csv\")\n",
    "\n",
    "# Load spaCy model for sentence segmentation and potentially other NLP tasks\n",
    "# python -m spacy download en_core_web_sm \n",
    "NLP_SPACY = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b9daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Extraction (IE) - Dataset Identification ---\n",
    "NON_STD_UNICODE_DASHES = re.compile(r'[\\u2010\\u2011\\u2012\\u2013\\u2014]')\n",
    "NON_STD_UNICODE_TICKS = re.compile(r'[\\u201c\\u201d]')\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean the input text by removing non-standard unicode dashes and extra whitespace.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to clean.\n",
    "        \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Replace all non-standard unicode dashes with '-'\n",
    "    text = text.replace('\\u200b', '').replace('-\\n', '-').replace('_\\n', '_').replace('/\\n', '/').replace('dryad.\\n', 'dryad.').replace('doi.\\norg', 'doi.org')\n",
    "    text = NON_STD_UNICODE_DASHES.sub('-', text)\n",
    "    text = NON_STD_UNICODE_TICKS.sub(\"'\", text)\n",
    "    # Remove extra whitespace\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Information Extraction (IE) - Dataset Identification\n",
    "# Regex patterns for common dataset identifiers\n",
    "#DOI_PATTERN = r'\\b10\\.\\d{4,5}\\/[-._\\/:A-Za-z0-9]+'\n",
    "DOI_PATTERN = r\"(?:doi:|https?://(?:dx\\.)?doi\\.org/)(10\\.\\d{4,9}/[-._;()/:A-Z0-9]+)\"\n",
    "EPI_PATTERN = r'\\bEPI[-_A-Z0-9]{2,}'\n",
    "SAM_PATTERN = r'\\bSAMN[0-9]{2,}'          # SAMN07159041\n",
    "IPR_PATTERN = r'\\bIPR[0-9]{2,}'\n",
    "CHE_PATTERN = r'\\bCHEMBL[0-9]{2,}'\n",
    "PRJ_PATTERN = r'\\bPRJ[A-Z0-9]{2,}'\n",
    "E_G_PATTERN = r'\\bE-[A-Z]{4}-[0-9]{2,}'   # E-GEOD-19722 or E-PROT-100\n",
    "ENS_PATTERN = r'\\bENS[A-Z]{4}[0-9]{2,}'\n",
    "CVC_PATTERN = r'\\bCVCL_[A-Z0-9]{2,}'\n",
    "EMP_PATTERN = r'\\bEMPIAR-[0-9]{2,}'\n",
    "PXD_PATTERN = r'\\bPXD[0-9]{2,}'\n",
    "HPA_PATTERN = r'\\bHPA[0-9]{2,}'\n",
    "SRR_PATTERN = r'\\bSRR[0-9]{2,}'\n",
    "GSE_PATTERN = r'\\b(GSE|GSM|GDS|GPL)\\d{4,6}\\b' # Example for GEO accession numbers (e.g., GSE12345, GSM12345)\n",
    "GNB_PATTERN = r'\\b[A-Z]{1,2}\\d{5,6}\\b' # GenBank accession numbers (e.g., AB123456, AF000001)\n",
    "CAB_PATTERN = r'\\bCAB[0-9]{2,}'\n",
    "PDB_PATTERN = r\"\\bpdb\\s*\\d[A-Za-z0-9]{3}\" # Example: pdb 5yfp\n",
    "\n",
    "# Combine all patterns into a list\n",
    "DATASET_ID_PATTERNS = [\n",
    "    DOI_PATTERN,\n",
    "    EPI_PATTERN,\n",
    "    SAM_PATTERN,\n",
    "    IPR_PATTERN,\n",
    "    CHE_PATTERN,\n",
    "    PRJ_PATTERN,\n",
    "    E_G_PATTERN,\n",
    "    ENS_PATTERN,\n",
    "    CVC_PATTERN,\n",
    "    EMP_PATTERN,\n",
    "    PXD_PATTERN,\n",
    "    HPA_PATTERN,\n",
    "    SRR_PATTERN,\n",
    "    GSE_PATTERN,\n",
    "    GNB_PATTERN,\n",
    "    CAB_PATTERN,\n",
    "    PDB_PATTERN\n",
    "]\n",
    "\n",
    "# Compile all patterns for efficiency\n",
    "COMPILED_DATASET_ID_REGEXES = [re.compile(p) for p in DATASET_ID_PATTERNS]\n",
    "\n",
    "# Data related keywords to look for in the text\n",
    "# These keywords help to ensure that the text is relevant to datasets\n",
    "DATA_RELATED_KEYWORDS = ['data release', 'data associated', 'data availability', 'data access', 'download', 'program data', 'the data', 'dataset', 'database', 'repository', 'data source', 'data access', 'archive', 'arch.', 'digital']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914e15c",
   "metadata": {},
   "source": [
    "#### 2. Data Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "701fe0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1. DatasetCitation Class\n",
    "@dataclass\n",
    "class DatasetCitation:\n",
    "    dataset_ids: Set[str] = field(default_factory=set)  # Set to store unique dataset IDs\n",
    "    citation_context: str = \"\"\n",
    "    citation_type: Optional[str] = None # \"Primary\" or \"Secondary\" - for ground truth during training\n",
    "\n",
    "    def add_dataset_id(self, dataset_id: str):\n",
    "        self.dataset_ids.add(dataset_id)\n",
    "\n",
    "    def set_citation_context(self, context: str):\n",
    "        \"\"\"Sets the citation context, cleaning it.\"\"\"\n",
    "        if context:\n",
    "            # Replace newlines with spaces, remove brackets, and normalize whitespace\n",
    "            context = context.replace('\\n', ' ').replace('[', '').replace(']', '')\n",
    "            context = re.sub(r'\\s+', ' ', context.strip())\n",
    "            self.citation_context = context \n",
    "\n",
    "    def has_dataset(self) -> bool:\n",
    "        \"\"\"Returns True if there are both dataset IDs and citation context.\"\"\"\n",
    "        return bool(self.dataset_ids and self.citation_context.strip())\n",
    "\n",
    "    def to_dict(self):\n",
    "        d = asdict(self)\n",
    "        d[\"dataset_ids\"] = list(self.dataset_ids)\n",
    "        return d\n",
    "\n",
    "# 2.2. ArticleData Class\n",
    "@dataclass\n",
    "class ArticleData:\n",
    "    article_id: str = \"\"\n",
    "    article_doi: str = \"\"\n",
    "    title: str = \"\"\n",
    "    author: str = \"\"\n",
    "    abstract: str = \"\"\n",
    "    dataset_citations: List[DatasetCitation] = field(default_factory=list)\n",
    "    full_text: str = \"\"\n",
    "\n",
    "    def __post_init__(self):\n",
    "        # Custom initialization\n",
    "        if self.article_id and not self.article_doi:\n",
    "            # If article_id is provided but not article_doi, set article_doi\n",
    "            self.article_doi = self.article_id.replace(\"_\", \"/\").lower()\n",
    "\n",
    "    def add_dataset_citation(self, dataset_citation: DatasetCitation):\n",
    "        \"\"\"Adds a DatasetCitation object to the article.\"\"\"\n",
    "        if dataset_citation.has_dataset():\n",
    "            self.dataset_citations.append(dataset_citation)\n",
    "        \n",
    "    def to_dict(self):\n",
    "        d = asdict(self)\n",
    "        # Convert list of DatasetCitation objects to their dict representation\n",
    "        d[\"dataset_citations\"] = [dc.to_dict() for dc in self.dataset_citations]\n",
    "        # Remove full_text from the dictionary if it exists\n",
    "        if \"full_text\" in d:\n",
    "            del d[\"full_text\"]\n",
    "        return d\n",
    "\n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), separators=(',', ':'))\n",
    "\n",
    "    def has_data(self) -> bool:\n",
    "        \"\"\"Returns True if there are any dataset citations.\"\"\"\n",
    "        return bool(self.dataset_citations)\n",
    "    \n",
    "@dataclass\n",
    "class LlmTrainingData:\n",
    "    article_id: str = \"\"\n",
    "    article_doi: str = \"\"\n",
    "    article_title: str = \"\"\n",
    "    article_abstract: str = \"\"\n",
    "    citation_context: str = \"\"\n",
    "    dataset_id: str = \"\"\n",
    "    label: str = \"\"\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "    \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.to_dict(), separators=(',', ':'))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b4259",
   "metadata": {},
   "source": [
    "#### 3. Data Loading and Initial Preprocessing\n",
    "\n",
    "This section will cover how to load the raw competition data (full text articles and labels) and begin structuring it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a5d12b",
   "metadata": {},
   "source": [
    "#### Load Labeled Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "438f57fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labeled training data from: ./kaggle/input/make-data-count-finding-data-references\\train_labels.csv\n",
      "Training labels shape: (1028, 3)\n",
      "Example grouped training data for article_id '10.1002_2017jc013030': [{'dataset_id': 'https://doi.org/10.17882/49388', 'type': 'Primary'}]\n",
      "Files paths shape: (30, 5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ground_truth_dataset_info",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "c8ec6a46-bf20-48f9-af3d-4d9d445aa0b1",
       "rows": [
        [
         "1",
         "10.1002_anie.201916483",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.201916483.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_anie.201916483.xml",
         "test",
         "[{'dataset_id': 'Missing', 'type': 'Missing'}]"
        ],
        [
         "28",
         "10.1002_nafm.10870",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_nafm.10870.pdf",
         "",
         "test",
         "[{'dataset_id': 'https://doi.org/10.5066/p9gtumay', 'type': 'Primary'}]"
        ],
        [
         "16",
         "10.1002_ece3.6303",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6303.pdf",
         "./kaggle/input/make-data-count-finding-data-references\\test\\XML\\10.1002_ece3.6303.xml",
         "test",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.37pvmcvgb', 'type': 'Primary'}]"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>ground_truth_dataset_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1002_anie.201916483</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'Missing', 'type': 'Missing'}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.1002_nafm.10870</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5066/p9gtu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.1002_ece3.6303</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                article_id                                      pdf_file_path  \\\n",
       "1   10.1002_anie.201916483  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "28      10.1002_nafm.10870  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "16       10.1002_ece3.6303  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "                                        xml_file_path dataset_type  \\\n",
       "1   ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "28                                                            test   \n",
       "16  ./kaggle/input/make-data-count-finding-data-re...         test   \n",
       "\n",
       "                            ground_truth_dataset_info  \n",
       "1      [{'dataset_id': 'Missing', 'type': 'Missing'}]  \n",
       "28  [{'dataset_id': 'https://doi.org/10.5066/p9gtu...  \n",
       "16  [{'dataset_id': 'https://doi.org/10.5061/dryad...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_file_paths(dataset_type_dir: str) -> pd.DataFrame: \n",
    "    pdf_path = os.path.join(dataset_type_dir, 'PDF')\n",
    "    xml_path = os.path.join(dataset_type_dir, 'XML')\n",
    "    dataset_type = os.path.basename(dataset_type_dir)\n",
    "    pdf_files = [f for f in os.listdir(pdf_path) if f.endswith('.pdf')]\n",
    "    xml_files = [f for f in os.listdir(xml_path) if f.endswith('.xml')]\n",
    "    df_pdf = pd.DataFrame({\n",
    "        'article_id': [f.replace('.pdf', '') for f in pdf_files],\n",
    "        'pdf_file_path': [os.path.join(pdf_path, f) for f in pdf_files]\n",
    "    })\n",
    "    df_xml = pd.DataFrame({\n",
    "        'article_id': [f.replace('.xml', '') for f in xml_files],\n",
    "        'xml_file_path': [os.path.join(xml_path, f) for f in xml_files]\n",
    "    })\n",
    "    merge_df = pd.merge(df_pdf, df_xml, on='article_id', how='outer', suffixes=('_pdf', '_xml'), validate=\"one_to_many\")\n",
    "    merge_df['dataset_type'] = dataset_type\n",
    "    return merge_df\n",
    "\n",
    "# Load the labeled training data CSV file\n",
    "print(f\"Loading labeled training data from: {TRAIN_LABELS_PATH}\")\n",
    "train_labels_df = pd.read_csv(TRAIN_LABELS_PATH)\n",
    "print(f\"Training labels shape: {train_labels_df.shape}\")\n",
    "\n",
    "# Group training data by article_id to get all datasets for each article\n",
    "# This creates a dictionary where keys are article_ids and values are lists of dataset dicts\n",
    "grouped_training_data = {}\n",
    "for article_id, group_df in train_labels_df.groupby('article_id'):\n",
    "    grouped_training_data[article_id] = group_df[['dataset_id', 'type']].to_dict('records')\n",
    "\n",
    "# Example usage of grouped_training_data\n",
    "print(f\"Example grouped training data for article_id '10.1002_2017jc013030': {grouped_training_data['10.1002_2017jc013030']}\")\n",
    "\n",
    "# Set the base file dir for the articles to be processed\n",
    "base_file_dir = TEST_DATA_DIR \\\n",
    "    if os.getenv('KAGGLE_IS_COMPETITION_RERUN') \\\n",
    "    else TRAIN_DATA_DIR\n",
    "\n",
    "# Just for testing, always set to the TEST_DATA_DIR\n",
    "base_file_dir = TEST_DATA_DIR\n",
    "\n",
    "# Load file paths for base directory\n",
    "file_paths_df = load_file_paths(base_file_dir)\n",
    "file_paths_df['xml_file_path'] = file_paths_df['xml_file_path'].fillna('')\n",
    "\n",
    "# Merge the file paths with the grouped_training_data\n",
    "file_paths_df['ground_truth_dataset_info'] = file_paths_df['article_id'].map(grouped_training_data)\n",
    "file_paths_df['ground_truth_dataset_info'] = file_paths_df['ground_truth_dataset_info'].fillna('')\n",
    "\n",
    "print(f\"Files paths shape: {file_paths_df.shape}\")\n",
    "display(file_paths_df.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f0763",
   "metadata": {},
   "source": [
    "#### Define File Extract Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "971f8c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1. Helper function to extract text from various file types\n",
    "def extract_text_from_file(filepath: str) -> str:\n",
    "    \"\"\"Extracts text from XML, PDF, or TXT files.\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return \"\"\n",
    "    \n",
    "    print(f\"Extracting text from file: {filepath}\")\n",
    "    if filepath.endswith(\".xml\"):\n",
    "        parser = etree.XMLParser(resolve_entities=False, no_network=True)\n",
    "        try:\n",
    "            tree = etree.parse(filepath, parser)\n",
    "            # A common way to get all text from an XML scientific article\n",
    "            # This might need adjustment based on the specific XML schema\n",
    "            return clean_text(\" \".join(tree.xpath(\"//text()\")).strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing XML {filepath}: {e}\")\n",
    "            return \"\"\n",
    "    elif filepath.endswith(\".pdf\"):\n",
    "        try:\n",
    "            doc = fitz.open(filepath)\n",
    "            text = \"\"\n",
    "            for page in doc:\n",
    "                text += page.get_textpage().extractTEXT()+\"\\n\"\n",
    "            return clean_text(text.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing PDF {filepath}: {e}\")\n",
    "            return \"\"\n",
    "    elif filepath.endswith(\".txt\"):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return f.read().strip()\n",
    "    return \"\"\n",
    "\n",
    "def extract_first_few_sentences(text: str, num_sentences: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the first few sentences from the text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "        num_sentences (int): The number of sentences to extract.\n",
    "        \n",
    "    Returns:\n",
    "        str: The first few sentences from the text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    doc = NLP_SPACY(text)\n",
    "    sentences = list(doc.sents)\n",
    "    \n",
    "    # Join the first few sentences\n",
    "    return \" \".join([sent.text for sent in sentences[:num_sentences]]).strip()\n",
    "\n",
    "def extract_article_data_from_text(full_text: str, article_id: str) -> ArticleData:\n",
    "    \"\"\"\n",
    "    Extracts article data from the full text.\n",
    "    \n",
    "    Args:\n",
    "        full_text (str): The full text of the article.\n",
    "        article_id (str): The ID of the article.\n",
    "        \n",
    "    Returns:\n",
    "        ArticleData: An instance of ArticleData with extracted information.\n",
    "    \"\"\"\n",
    "    title = \"\"\n",
    "    author = \"\"\n",
    "\n",
    "    # Placeholder for extracting title, author, abstract\n",
    "    # This is highly dependent on the structure of your full text files.\n",
    "    # For now, we'll use simple regex or assume they are at the beginning.\n",
    "    # title_match = re.search(r\"Title:\\s*(.*)\", full_text, re.IGNORECASE)\n",
    "    # title = title_match.group(1).strip() if title_match else \"Unknown Title\"\n",
    "\n",
    "    # author_match = re.search(r\"Author(?:s)?:\\s*(.*)\", full_text, re.IGNORECASE)\n",
    "    # author = author_match.group(1).strip() if author_match else \"Unknown Author\"\n",
    "\n",
    "    abstract_match = re.search(r\"Abstract\\s*(.*?)(?=\\n\\n|\\Z)\", full_text, re.IGNORECASE | re.DOTALL)\n",
    "    abstract = abstract_match.group(1).strip() if abstract_match else \"No Abstract\"\n",
    "    abstract = extract_first_few_sentences(abstract[:400], num_sentences=3)  # Extract first few sentences for the abstract\n",
    "\n",
    "    return ArticleData(\n",
    "        article_id=article_id,\n",
    "        title=title,\n",
    "        author=author,\n",
    "        abstract=abstract\n",
    "    )\n",
    "\n",
    "# 4.2. Function to extract context around an ID\n",
    "def extract_context_around_id(sentences, dataset_id: str, window_size_sentences: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Extracts a window of sentences around a given dataset ID in the text.\n",
    "    Uses spaCy for sentence segmentation.\n",
    "    \"\"\"\n",
    "    if not sentences or not dataset_id or dataset_id == \"Missing\":\n",
    "        return \"\"\n",
    "        \n",
    "    # Find all occurrences of the dataset_id (case-insensitive)\n",
    "    matches = [(i, sent) for i, sent in enumerate(sentences) if dataset_id.lower() in sent.lower()]\n",
    "    if not matches:\n",
    "        return \"\"\n",
    "\n",
    "    # For simplicity, take the context around the first match.\n",
    "    # You might want to refine this to capture all relevant contexts or the most prominent one.\n",
    "    first_match_idx = matches[0][0]\n",
    "    \n",
    "    start_idx = max(0, first_match_idx - window_size_sentences)\n",
    "    end_idx = min(len(sentences), first_match_idx + 1)\n",
    "    \n",
    "    context_sentences = sentences[start_idx:end_idx]\n",
    "    return \" \".join(context_sentences)\n",
    "\n",
    "\n",
    "def extract_training_data_for_llm(file_paths_df: pd.DataFrame) -> list[dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Extracts article data for training set with ground truth.\n",
    "    \n",
    "    Args:\n",
    "        file_paths_df (pd.DataFrame): DataFrame containing file paths and ground truth info.\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, ArticleData]: Dictionary mapping article IDs to ArticleData objects.\n",
    "    \"\"\"\n",
    "    training_data_for_llm: list[dict[str, str]] = [] # This will be a list of LlmTrainingData for the LLM training dataset\n",
    "    for i, row in tqdm(file_paths_df.iterrows(), total=len(file_paths_df)):\n",
    "        article_id = row['article_id']\n",
    "        filepath = row['pdf_file_path'] if row['pdf_file_path'] else row['xml_file_path']\n",
    "        ground_truth_list = row['ground_truth_dataset_info'] if 'ground_truth_dataset_info' in row else []\n",
    "        \n",
    "        full_text = extract_text_from_file(filepath)\n",
    "        article_data = extract_article_data_from_text(full_text, article_id)\n",
    "\n",
    "        doc = NLP_SPACY(full_text)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "        if not ground_truth_list:\n",
    "            print(f\"No ground truth data found for article_id: {article_id}. Skipping this article.\")\n",
    "            continue\n",
    "        for gt in ground_truth_list:\n",
    "            dataset_id = gt['dataset_id'].replace(\"https://doi.org/\", \"\").replace(\"doi:\", \"\").strip()\n",
    "            citation_type = gt.get('type', 'Primary')\n",
    "            if dataset_id:\n",
    "                # Convert to dict for LLM training data\n",
    "                training_data_for_llm.append(\n",
    "                    {\n",
    "                        \"article_id\": article_data.article_id,\n",
    "                        \"article_doi\": article_data.article_doi,\n",
    "                        \"article_abstract\": article_data.abstract,\n",
    "                        \"citation_context\": extract_context_around_id(sentences, dataset_id),\n",
    "                        \"dataset_id\": dataset_id,\n",
    "                        \"label\": citation_type\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    print(f\"Loaded training data for {len(training_data_for_llm)} articles.\")\n",
    "    return training_data_for_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aeda081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "article_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pdf_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "xml_file_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataset_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "ground_truth_dataset_info",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "4284f493-5a7c-4afc-8ba7-53cbaa50bcf9",
       "rows": [
        [
         "25",
         "10.1002_esp.5058",
         "./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_esp.5058.pdf",
         "",
         "test",
         "[{'dataset_id': 'https://doi.org/10.5061/dryad.jh9w0vt9t', 'type': 'Primary'}]"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>pdf_file_path</th>\n",
       "      <th>xml_file_path</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>ground_truth_dataset_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.1002_esp.5058</td>\n",
       "      <td>./kaggle/input/make-data-count-finding-data-re...</td>\n",
       "      <td></td>\n",
       "      <td>test</td>\n",
       "      <td>[{'dataset_id': 'https://doi.org/10.5061/dryad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          article_id                                      pdf_file_path  \\\n",
       "25  10.1002_esp.5058  ./kaggle/input/make-data-count-finding-data-re...   \n",
       "\n",
       "   xml_file_path dataset_type  \\\n",
       "25                       test   \n",
       "\n",
       "                            ground_truth_dataset_info  \n",
       "25  [{'dataset_id': 'https://doi.org/10.5061/dryad...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing, let's extract training data for a specific article\n",
    "sample_file_paths_df = file_paths_df.loc[file_paths_df['article_id'] == '10.1002_esp.5058']\n",
    "sample_file_paths_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef756a4e",
   "metadata": {},
   "source": [
    "#### 4. Advanced Preprocessing: Extracting Dataset Mentions and Context (Training)\n",
    "\n",
    "Use regex to find the given dataset IDs from the training_labels and then use spaCy to extract surrounding sentences as context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f23263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd49ce6da5849a895927f88abb724f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_2017jc013030.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.201916483.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202005531.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_anie.202007717.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.201902131.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.201903120.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202000235.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202001412.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202001668.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_chem.202003167.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_cssc.202201821.pdf\n",
      "No ground truth data found for article_id: 10.1002_cssc.202201821. Skipping this article.\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.3985.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.4466.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.5260.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.5395.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6144.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6303.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.6784.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.961.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ece3.9627.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.1280.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ecs2.4619.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejic.201900904.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejoc.202000139.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_ejoc.202000916.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_esp.5058.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_esp.5090.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_mp.14424.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1002_nafm.10870.pdf\n",
      "Extracting text from file: ./kaggle/input/make-data-count-finding-data-references\\test\\PDF\\10.1007_jhep07(2018)134.pdf\n",
      "Loaded training data for 30 articles.\n",
      "Prepared 30 training examples for the LLM.\n"
     ]
    }
   ],
   "source": [
    "# 4.3. Populate ArticleData with DatasetCitation objects and ground truth\n",
    "training_data_for_llm = extract_training_data_for_llm(file_paths_df)\n",
    "print(f\"Prepared {len(training_data_for_llm)} training examples for the LLM.\")\n",
    "\n",
    "# Convert the list of LlmTrainingData to a DataFrame and save it\n",
    "training_data_for_llm_df = pd.DataFrame(training_data_for_llm)\n",
    "training_data_for_llm_df.to_csv(os.path.join(BASE_OUTPUT_DIR, \"training_data_for_llm.csv\"), index=False)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_list(training_data_for_llm)\n",
    "train_dataset = train_dataset.shuffle(seed=42) # Shuffle for good measure\n",
    "\n",
    "# Split into train/validation\n",
    "train_test_split = train_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "print(f\"Training set size: {len(train_dataset)} examples\")\n",
    "print(f\"Validation set size: {len(eval_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066f50c",
   "metadata": {},
   "source": [
    "#### 5. Model Selection and Configuration\n",
    "\n",
    "We'll use a Qwen model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e2982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model C:\\Users\\jim\\.cache\\kagglehub\\models\\qwen-lm\\qwen-3\\transformers\\0.6b\\1 loaded with 4-bit quantization.\n"
     ]
    }
   ],
   "source": [
    "# 5.1. Choose a Model from KaggleHub\n",
    "# Example: Qwen/Qwen1.5-0.5B-Chat (or 1.8B-Chat if 0.5B is too small/performs poorly)\n",
    "# You can find these on KaggleHub or Hugging Face Hub.\n",
    "model_name = QWEN_BASE_MODEL_PATH\n",
    "\n",
    "# 5.2. Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Qwen uses EOS for padding\n",
    "\n",
    "# 5.3. Load Model with Quantization (4-bit)\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16 # Or torch.float16 if bfloat16 is not supported by your GPU\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=nf4_config,\n",
    "    torch_dtype=torch.bfloat16, # Match compute_dtype\n",
    "    device_map=\"auto\", # Automatically maps model to available devices\n",
    "    trust_remote_code=True # Required for some models like Qwen\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training (LoRA compatible)\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"Model {model_name} loaded with 4-bit quantization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a83df8",
   "metadata": {},
   "source": [
    "#### 6. Dataset Preparation for Training\n",
    "\n",
    "Format the extracted data into instruction-tuning prompts using the ChatML format, which Qwen models are trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a008679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of formatted training data (string output):\n",
      "<|im_start|>system\n",
      "You are an expert assistant for classifying research data citations. /no_think<|im_end|>\n",
      "<|im_start|>user\n",
      "Given the following article context and a specific data citation, classify if the data was generated as 'Primary' (newly generated for this study), 'Secondary' (reused from existing records), or 'Missing' (no data citation context given).\n",
      "\n",
      "Article DOI: 10.1002/esp.5090\n",
      "Article Abstract: The 20 May 2016 MW 6.1 Petermann earthquake in central Australia generated a 21 km surface rupture with 0.1 to 1 m vertical displacements across a low-relief landscape. No paleo-scarps or potentially analogous topographic features are evident in pre-earthquake Worldview-1 and Worldview-2 satellite data. Two excavations across the surface rupture expose near-surface fault geometry and mixed aeolian\n",
      "Data Citation Context: Erosion has been locally enhanced by bed-rock shattering, rock fragment displacement (Figure 2), and rockfalls, particularly on the hanging-wall where coseismic shaking damage was more intense (King et al., 2018). 2 | METHODS 2.1 | Analysis of Worldview-1 imagery and elevation model For our topographic analysis we use the pre- and post-Petermann earthquake digital elevation models (DEMs) produced by Gold et al. (2019). These were derived using pre-event (March and June 2014) and post-event (October and November 2016) in-track stereo 0.5 m resolution panchromatic WorldView-1 and WorldView-2 images (©2019, DigitalGlobe) using the Surface Extraction from TIN-based Searchspace Minimization software (Noh & Howat, 2015) running on the University of Iowa Argon supercomputer. The DEMs are available at https://doi.org/10.5066/P9353101, and the processing techniques are described in the supplementary information provided by Gold et al. (2019), available at https://doi.org/10.1029/2019GL084926.\n",
      "Dataset ID: 10.5066/p9353101\n",
      "\n",
      "Classification:<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Secondary<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6.1. Define the formatting function for ChatML (Corrected for trl 0.19.1)\n",
    "def format_example(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert assistant for classifying research data citations. /no_think\"},\n",
    "        {\"role\": \"user\", \"content\": (\n",
    "            f\"Given the following article context and a specific data citation, classify if the data was generated as 'Primary' (newly generated for this study), 'Secondary' (reused from existing records), or 'Missing' (no data citation context given).\\n\\n\"\n",
    "            f\"Article DOI: {example['article_doi']}\\n\"\n",
    "            f\"Article Abstract: {example['article_abstract']}\\n\" \n",
    "            f\"Data Citation Context: {example['citation_context']}\\n\"\n",
    "            f\"Dataset ID: {example['dataset_id']}\\n\\n\"\n",
    "            f\"Classification:\"\n",
    "        )}\n",
    "    ]\n",
    "    # The target output for the model is just \"Primary\" or \"Secondary\"\n",
    "    messages.append({\"role\": \"assistant\", \"content\": example['label']})\n",
    "    \n",
    "    # Apply chat template and return the string directly\n",
    "    # <--- IMPORTANT CHANGE: Directly return the string, not a dictionary\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False, enable_thinking=False)\n",
    "\n",
    "# Apply the formatting to the dataset\n",
    "# IMPORTANT: When formatting_func returns a string directly, you typically don't\n",
    "# need to call .map() on the dataset beforehand if SFTTrainer handles it internally.\n",
    "# However, if you want to inspect the formatted text, you can still do this:\n",
    "# formatted_train_dataset = train_dataset.map(format_example)\n",
    "# But for SFTTrainer, you pass the original `train_dataset` and the `formatting_func`\n",
    "# and `dataset_text_field` (which will be ignored if formatting_func is used to generate the text).\n",
    "\n",
    "# Print an example to verify (you'll need to call format_example directly for this)\n",
    "print(\"\\nExample of formatted training data (string output):\")\n",
    "# You can't directly print from formatted_train_dataset if you don't map it first.\n",
    "# Let's print by calling the function on a sample:\n",
    "if len(train_dataset) > 0:\n",
    "    sample_formatted_text = format_example(train_dataset[0])\n",
    "    print(sample_formatted_text)\n",
    "else:\n",
    "    print(\"No training data to display example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8fdd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# This version uses the evaluation dataset directly in the SFTTrainer\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# First, import SFTConfig from trl\n",
    "from trl import SFTConfig\n",
    "\n",
    "# 7.1. Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=\"all-linear\", # Adjust based on model architecture if needed\n",
    ")\n",
    "\n",
    "# 7.2. Configure Training Arguments (now using SFTConfig)\n",
    "output_dir = \"./results\"\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[],\n",
    "    \n",
    "    # SFTTrainer-specific parameters moved into SFTConfig\n",
    "    max_seq_length=256,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\",\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant':False},\n",
    "\n",
    "    # --- NEW: Evaluation Parameters ---\n",
    "    evaluation_strategy=\"steps\", # Evaluate every 'eval_steps'\n",
    "    eval_steps=500,              # How often to run evaluation (e.g., every 500 steps)\n",
    "                                 # You can also use \"epoch\" for evaluation_strategy\n",
    "    save_strategy=\"steps\",       # How often to save checkpoints\n",
    "    save_total_limit=1,          # Only keep the best model checkpoint\n",
    "    load_best_model_at_end=True, # Load the model with the best validation metric at the end of training\n",
    "    metric_for_best_model=\"eval_loss\", # Metric to monitor for best model (default for CLM)\n",
    "    greater_is_better=False,     # For loss, lower is better\n",
    ")\n",
    "\n",
    "# 7.3. Initialize SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, # <--- Pass the evaluation dataset here\n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    "    formatting_func=format_example\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f90683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612c42297d1e41398bf21a46892eb71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdc41c736f5a42d08bd9f7c003ac7441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8068b3693b584593b7012fe602578e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00bac153dd446b9a53dbb795a245c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# This version works but does NOT use the evaluation dataset\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# # First, import SFTConfig from trl\n",
    "# from trl import SFTConfig\n",
    "\n",
    "# # 7.1. Configure LoRA\n",
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     r=64,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "#     target_modules=\"all-linear\", # Adjust based on model architecture if needed\n",
    "# )\n",
    "\n",
    "# # 7.2. Configure Training Arguments (now using SFTConfig)\n",
    "# # SFTConfig combines TrainingArguments with SFTTrainer-specific parameters\n",
    "# output_dir = \"./results\"\n",
    "# training_args = SFTConfig( # <--- IMPORTANT CHANGE: Use SFTConfig instead of TrainingArguments\n",
    "#     output_dir=output_dir,\n",
    "#     per_device_train_batch_size=1, # Adjust based on your GPU memory\n",
    "#     gradient_accumulation_steps=16,\n",
    "#     learning_rate=2e-4,\n",
    "#     num_train_epochs=3,\n",
    "#     logging_steps=10,\n",
    "#     save_steps=500,\n",
    "#     optim=\"paged_adamw_8bit\",\n",
    "#     fp16=True,  # <--- CHANGED: Try fp16 for broader compatibility and memory\n",
    "#     bf16=False, # <--- CHANGED: Disable bf16 if fp16 is used\n",
    "#     max_grad_norm=0.3,\n",
    "#     warmup_ratio=0.03,\n",
    "#     lr_scheduler_type=\"constant\",\n",
    "#     report_to=\"none\",\n",
    "#     disable_tqdm=False,\n",
    "#     remove_unused_columns=False, # Keep columns for formatting\n",
    "#     label_names=[], # Explicitly tell Trainer not to look for label columns in the dataset\n",
    "#     # Additional SFT-specific parameters    \n",
    "#     max_seq_length=512, # Max input sequence length (adjust based on context size)\n",
    "#     packing=False, # Set to True for more efficient training if your data is short\n",
    "#     dataset_text_field=\"text\", # The name of the column in your dataset containing the text\n",
    "#     # <--- NEW: Enable gradient checkpointing\n",
    "#     gradient_checkpointing=True,\n",
    "#     # This line is important for gradient checkpointing with PeftModel\n",
    "#     # It tells the model to use the Peft (LoRA) layers for checkpointing\n",
    "#     gradient_checkpointing_kwargs={'use_reentrant':False} # Recommended for newer PyTorch/Accelerate\n",
    "# )\n",
    "\n",
    "# # 7.3. Initialize SFTTrainer (Corrected for trl 0.19.1)\n",
    "# trainer = SFTTrainer(\n",
    "#     model=model,\n",
    "#     processing_class=tokenizer, \n",
    "#     train_dataset=train_dataset,\n",
    "#     peft_config=peft_config,\n",
    "#     args=training_args, # This is now an SFTConfig object\n",
    "#     formatting_func=format_example # This remains a direct argument to SFTTrainer\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45982673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:42, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n",
      "Fine-tuned model saved to ./results\\final_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7.4. Start Training\n",
    "print(\"\\nStarting model training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the fine-tuned model (LoRA adapters)\n",
    "trainer.save_model(os.path.join(output_dir, \"final_model\"))\n",
    "print(f\"Fine-tuned model saved to {os.path.join(output_dir, 'final_model')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6dba261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initiating GPU memory cleanup...\n",
      "GPU memory cleanup complete. Please check nvidia-smi to confirm.\n"
     ]
    }
   ],
   "source": [
    "import gc # Import the garbage collection module\n",
    "\n",
    "# --- Explicit GPU Memory Cleanup ---\n",
    "print(\"\\nInitiating GPU memory cleanup...\")\n",
    "\n",
    "# 1. Explicitly delete large objects that consume GPU memory\n",
    "#    This removes references, allowing Python's garbage collector to act.\n",
    "if 'trainer' in locals() and trainer is not None:\n",
    "    del trainer\n",
    "if 'model' in locals() and model is not None:\n",
    "    del model\n",
    "if 'tokenizer' in locals() and tokenizer is not None:\n",
    "    del tokenizer\n",
    "# If you had other large tensors or datasets explicitly moved to GPU,\n",
    "# you would delete them here too. For Hugging Face datasets, they are usually\n",
    "# on CPU unless you manually call .to('cuda').\n",
    "\n",
    "# 2. Force Python's garbage collection\n",
    "#    This helps ensure that deleted objects are immediately cleaned up.\n",
    "gc.collect()\n",
    "\n",
    "# 3. Clear PyTorch's CUDA memory cache\n",
    "#    This tells PyTorch to release any cached memory back to the OS/driver.\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"GPU memory cleanup complete. Please check nvidia-smi to confirm.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e3154",
   "metadata": {},
   "source": [
    "#### 8. Inference and Evaluation\n",
    "\n",
    "After training, load the best model (or the final one) and apply it to the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583fed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.1. Load the Trained Model (or merge LoRA adapters for full model)\n",
    "# If you saved LoRA adapters, you'll need to load the base model and then the adapters.\n",
    "# For inference, it's often easier to merge them.\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=nf4_config, # Use the same config as training\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "# model = PeftModel.from_pretrained(model, os.path.join(output_dir, \"final_model\"))\n",
    "# model = model.merge_and_unload() # Merge LoRA adapters into the base model\n",
    "\n",
    "# For simplicity, if you just want to test the last saved checkpoint:\n",
    "# You can also load the model directly from the checkpoint if it's a full save\n",
    "# model = AutoModelForCausalLM.from_pretrained(os.path.join(output_dir, \"final_model\"), device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(os.path.join(output_dir, \"final_model\"))\n",
    "\n",
    "# If you want to load the base model and then the adapters for inference:\n",
    "from peft import PeftModel\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=nf4_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, os.path.join(output_dir, \"final_model\"))\n",
    "model.eval() # Set to evaluation mode\n",
    "\n",
    "print(\"Model loaded for inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbddd2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8.2. Preprocess Test Data (similar to training data)\n",
    "test_articles_data: Dict[str, ArticleData] = {}\n",
    "# Assuming test data structure is similar to train data (full text files)\n",
    "for article_file in os.listdir(TEST_DATA_DIR):\n",
    "    article_id = os.path.splitext(article_file)[0]\n",
    "    filepath = os.path.join(TEST_DATA_DIR, article_file)\n",
    "    \n",
    "    full_text = extract_text_from_file(filepath)\n",
    "    \n",
    "    # Extract title, author, abstract (same as training)\n",
    "    title_match = re.search(r\"Title:\\s*(.*)\", full_text, re.IGNORECASE)\n",
    "    title = title_match.group(1).strip() if title_match else \"Unknown Title\"\n",
    "    author_match = re.search(r\"Author(?:s)?:\\s*(.*)\", full_text, re.IGNORECASE)\n",
    "    author = author_match.group(1).strip() if author_match else \"Unknown Author\"\n",
    "    abstract_match = re.search(r\"Abstract\\s*(.*?)(?=\\n\\n|\\Z)\", full_text, re.IGNORECASE | re.DOTALL)\n",
    "    abstract = abstract_match.group(1).strip() if abstract_match else \"No Abstract\"\n",
    "\n",
    "    article_data = ArticleData(\n",
    "        article_id=article_id,\n",
    "        title=title,\n",
    "        author=author,\n",
    "        abstract=abstract\n",
    "    )\n",
    "    # For test data, we need to find *all* potential dataset IDs, not just ground truth\n",
    "    # This is the \"finding datasets\" part of your goal.\n",
    "    \n",
    "    # Use regex to find all potential dataset IDs in the full text\n",
    "    found_dataset_mentions = []\n",
    "    for pattern in ALL_ID_PATTERNS:\n",
    "        for match in re.finditer(pattern, full_text, re.IGNORECASE):\n",
    "            dataset_id = match.group(1) if pattern == DOI_PATTERN else match.group(0)\n",
    "            span_text = match.group(0) # The full matched text\n",
    "            \n",
    "            context = extract_context_around_id(full_text, span_text, window_size_sentences=3)\n",
    "            \n",
    "            if context:\n",
    "                dc = DatasetCitation()\n",
    "                dc.add_dataset_id(dataset_id)\n",
    "                dc.set_citation_context(context)\n",
    "                found_dataset_mentions.append(dc)\n",
    "                \n",
    "    article_data.dataset_citations = found_dataset_mentions # Assign found mentions\n",
    "    test_articles_data[article_id] = article_data\n",
    "\n",
    "print(f\"Prepared {len(test_articles_data)} test articles for inference.\")\n",
    "\n",
    "# 8.3. Generate Predictions\n",
    "predictions = []\n",
    "true_labels = [] # Only if you have a test_labels.json for evaluation\n",
    "\n",
    "for article_id, article_data in test_articles_data.items():\n",
    "    for dc in article_data.dataset_citations:\n",
    "        # Create the prompt for inference\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert assistant for classifying research data citations.\"},\n",
    "            {\"role\": \"user\", \"content\": (\n",
    "                f\"Given the following article context and a specific data citation, classify if the data was generated as 'Primary' (newly generated for this study) or 'Secondary' (reused from existing records).\\n\\n\"\n",
    "                f\"Article Title: {article_data.title}\\n\"\n",
    "                f\"Article Abstract: {article_data.abstract}\\n\"\n",
    "                f\"Data Citation Context: {dc.citation_context}\\n\"\n",
    "                f\"Dataset ID: {list(dc.dataset_ids)[0]}\\n\\n\" # Assuming one ID per citation\n",
    "                f\"Classification:\"\n",
    "            )}\n",
    "        ]\n",
    "        \n",
    "        input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).input_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=10, # Expecting \"Primary\" or \"Secondary\"\n",
    "                do_sample=False, # Use greedy decoding as per your preference\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(output[0][input_ids.shape[1]:], skip_special_tokens=True).strip()\n",
    "        \n",
    "        # Post-process the generated text to get the classification\n",
    "        predicted_type = \"Unknown\"\n",
    "        if \"Primary\" in generated_text:\n",
    "            predicted_type = \"Primary\"\n",
    "        elif \"Secondary\" in generated_text:\n",
    "            predicted_type = \"Secondary\"\n",
    "        \n",
    "        predictions.append({\n",
    "            \"article_id\": article_id,\n",
    "            \"dataset_id\": list(dc.dataset_ids)[0],\n",
    "            \"predicted_type\": predicted_type\n",
    "        })\n",
    "\n",
    "        # If you have test labels, you can collect true_labels here for evaluation\n",
    "        # For Kaggle, you'll typically submit predictions without knowing test labels.\n",
    "\n",
    "print(f\"Generated {len(predictions)} predictions.\")\n",
    "\n",
    "# 8.4. Evaluation (if test labels are available)\n",
    "# If you have a separate test_labels.json for local evaluation:\n",
    "# test_labels = load_labels(TEST_LABELS_PATH) # Load test labels\n",
    "#\n",
    "# # Match predictions to true labels and calculate metrics\n",
    "# # This part requires careful matching of dataset_id within article_id\n",
    "# # and might involve fuzzy matching for context if exact span isn't available.\n",
    "# # For simplicity, assuming exact match on article_id and dataset_id.\n",
    "#\n",
    "# y_true = []\n",
    "# y_pred = []\n",
    "#\n",
    "# for pred_entry in predictions:\n",
    "#     article_id = pred_entry[\"article_id\"]\n",
    "#     dataset_id = pred_entry[\"dataset_id\"]\n",
    "#     predicted_type = pred_entry[\"predicted_type\"]\n",
    "#\n",
    "#     # Find the true label for this specific dataset_id in this article\n",
    "#     found_true_label = False\n",
    "#     if article_id in test_labels:\n",
    "#         for gt_info in test_labels[article_id]:\n",
    "#             if gt_info[\"dataset_id\"] == dataset_id: # Exact match on ID\n",
    "#                 y_true.append(gt_info[\"citation_type\"])\n",
    "#                 y_pred.append(predicted_type)\n",
    "#                 found_true_label = True\n",
    "#                 break\n",
    "#     if not found_true_label:\n",
    "#         # Handle cases where a predicted ID might not be in ground truth\n",
    "#         # or where the ID extraction was imperfect.\n",
    "#         # For competition, this means your ID extraction needs to be precise.\n",
    "#         pass\n",
    "#\n",
    "# if y_true and y_pred:\n",
    "#     from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "#     # Map \"Primary\" to 1, \"Secondary\" to 0 for sklearn metrics\n",
    "#     label_map = {\"Primary\": 1, \"Secondary\": 0}\n",
    "#     y_true_mapped = [label_map.get(l, -1) for l in y_true]\n",
    "#     y_pred_mapped = [label_map.get(l, -1) for l in y_pred]\n",
    "#\n",
    "#     # Filter out -1 if there were unknown labels\n",
    "#     valid_indices = [i for i, val in enumerate(y_true_mapped) if val != -1 and y_pred_mapped[i] != -1]\n",
    "#     y_true_mapped = [y_true_mapped[i] for i in valid_indices]\n",
    "#     y_pred_mapped = [y_pred_mapped[i] for i in valid_indices]\n",
    "#\n",
    "#     if y_true_mapped:\n",
    "#         print(\"\\nEvaluation Results:\")\n",
    "#         print(f\"Accuracy: {accuracy_score(y_true_mapped, y_pred_mapped):.4f}\")\n",
    "#         print(f\"F1 Score (weighted): {f1_score(y_true_mapped, y_pred_mapped, average='weighted'):.4f}\")\n",
    "#         print(f\"Precision (weighted): {precision_score(y_true_mapped, y_pred_mapped, average='weighted'):.4f}\")\n",
    "#         print(f\"Recall (weighted): {recall_score(y_true_mapped, y_pred_mapped, average='weighted'):.4f}\")\n",
    "#     else:\n",
    "#         print(\"No matching true labels found for evaluation.\")\n",
    "# else:\n",
    "#     print(\"Not enough data to perform evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef4754",
   "metadata": {},
   "source": [
    "#### 9. Submission File Generation (Kaggle Specific)\n",
    "\n",
    "Finally, format your predictions into the required `submission.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10b8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.1. Create Submission DataFrame\n",
    "\n",
    "submission_df = pd.DataFrame(predictions)\n",
    "# Rename columns to match Kaggle's expected format (e.g., 'id', 'class_label')\n",
    "# This will depend on the exact submission format specified by Kaggle.\n",
    "# Example:\n",
    "# submission_df = submission_df.rename(columns={\"article_id\": \"Id\", \"dataset_id\": \"DatasetId\", \"predicted_type\": \"Type\"})\n",
    "# submission_df[\"Id\"] = submission_df[\"Id\"] + \"_\" + submission_df[\"DatasetId\"] # If Id is a combination\n",
    "\n",
    "# Assuming the submission format is a list of dictionaries with 'article_id', 'dataset_id', 'citation_type'\n",
    "# You might need to adjust this based on the exact competition requirements.\n",
    "# For example, if it expects a single ID column like \"article_id_dataset_id\"\n",
    "final_submission_data = []\n",
    "for pred in predictions:\n",
    "    final_submission_data.append({\n",
    "        \"Id\": f\"{pred['article_id']}_{pred['dataset_id']}\", # Example: combine IDs\n",
    "        \"Type\": pred['predicted_type']\n",
    "    })\n",
    "\n",
    "final_submission_df = pd.DataFrame(final_submission_data)\n",
    "final_submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(\"Submission file 'submission.csv' created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
