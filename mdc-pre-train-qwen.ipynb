{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c0467a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- 0. Environment Setup & Offline Preparation ---\n",
    "# This cell primarily defines paths and ensures libraries are available.\n",
    "\n",
    "# Define paths to your Kaggle Datasets\n",
    "# IMPORTANT: Adjust these paths to match where your datasets are mounted in Kaggle\n",
    "QWEN_BASE_MODEL_PATH = \"/kaggle/input/qwen1-5-1-8b-chat-hf\" # Example path for Qwen model\n",
    "ARTICLES_DIR = \"/kaggle/input/your-articles-dataset/articles/\" # Path to your PDF/XML articles\n",
    "TRAINING_DATA_CSV_PATH = \"/kaggle/input/your-training-data/training_data.csv\" # Path to your labeled CSV\n",
    "\n",
    "# Output directory for the fine-tuned model and results\n",
    "FINE_TUNED_MODEL_OUTPUT_DIR = \"/kaggle/working/qwen_finetuned_dataset_classifier\"\n",
    "FINAL_RESULTS_CSV_PATH = \"/kaggle/working/article_dataset_classification.csv\"\n",
    "\n",
    "# Install necessary libraries from Kaggle Datasets if not pre-installed\n",
    "# Example for PyMuPDF (fitz) - you'd need to upload its wheel file\n",
    "# !pip install /kaggle/input/pymupdf-whl/*.whl --no-index --find-links /kaggle/input/pymupdf-whl/\n",
    "\n",
    "# For bitsandbytes (optional, for 8-bit quantization during training)\n",
    "# !pip install /kaggle/input/bitsandbytes-0-41-1-py3-none-any-whl/*.whl --no-index --find-links /kaggle/input/bitsandbytes-0-41-1-py3-none-any-whl/\n",
    "\n",
    "# For trl (Supervised Fine-Tuning)\n",
    "# !pip install /kaggle/input/trl-0-7-10-py3-none-any-whl/*.whl --no-index --find-links /kaggle/input/trl-0-7-10-py3-none-any-whl/\n",
    "\n",
    "# Standard Imports\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import collections # For deque in parenthesis removal\n",
    "import fitz # PyMuPDF for PDF processing\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "\n",
    "# Set device for PyTorch\n",
    "device = \"cuda\" if torch and torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072417f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Data Loading & Preprocessing ---\n",
    "\n",
    "def read_pdf_text(pdf_path: str) -> str:\n",
    "    \"\"\"Extracts all text from a PDF file using PyMuPDF.\"\"\"\n",
    "    text = \"\"\n",
    "    if not fitz:\n",
    "        return text  # Return empty string if fitz is not available\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_textpage().extractTEXT().replace('\\u200b', '').strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF {pdf_path}: {e}\")\n",
    "        \n",
    "    return text\n",
    "\n",
    "def read_xml_text(xml_file_path: str) -> str:\n",
    "    \"\"\"Reads and concatenates all text content from an XML file.\"\"\"\n",
    "    all_text_parts = []\n",
    "    try:\n",
    "        tree = ET.parse(xml_file_path)\n",
    "        root = tree.getroot()\n",
    "        for element in root.iter():\n",
    "            if element.text:\n",
    "                cleaned_text = element.text.strip()\n",
    "                if cleaned_text:\n",
    "                    all_text_parts.append(cleaned_text)\n",
    "            if element.tail:\n",
    "                cleaned_tail = element.tail.strip()\n",
    "                if cleaned_tail:\n",
    "                    all_text_parts.append(cleaned_tail)\n",
    "        return \" \".join(all_text_parts) if all_text_parts else \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading XML {xml_file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def load_article(filepath: str) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Loads text content from a single article file (PDF or XML).\n",
    "    Returns a tuple of (article_id, text_content).\n",
    "    \"\"\"\n",
    "    article_id = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    text_content = None\n",
    "\n",
    "    if filepath.endswith(\".pdf\"):\n",
    "        text_content = read_pdf_text(filepath)\n",
    "    elif filepath.endswith(\".xml\"):\n",
    "        text_content = read_xml_text(filepath)\n",
    "\n",
    "    if text_content is None:\n",
    "        print(f\"Warning: Could not extract text from {filepath}. Skipping.\")\n",
    "        return article_id, \"\"\n",
    "\n",
    "    return article_id, text_content\n",
    "\n",
    "def load_articles(articles_dir: str) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Loads text content from all PDF and XML files in the specified directory.\n",
    "    Returns a dictionary mapping article_id (filename without extension) to its text content.\n",
    "    \"\"\"\n",
    "    article_texts = {}\n",
    "    article_files = glob.glob(os.path.join(articles_dir, \"*.pdf\")) + \\\n",
    "                    glob.glob(os.path.join(articles_dir, \"*.xml\"))\n",
    "    \n",
    "    print(f\"Found {len(article_files)} article files.\")\n",
    "\n",
    "    for filepath in article_files:\n",
    "        article_id, text_content = load_article(filepath)\n",
    "        if not text_content:\n",
    "            continue\n",
    "        article_texts[article_id] = text_content\n",
    "            \n",
    "    print(f\"Successfully loaded text for {len(article_texts)} articles.\")\n",
    "    return article_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6a6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load all article texts once\n",
    "all_article_texts = load_articles(ARTICLES_DIR)\n",
    "\n",
    "# Load labeled training data\n",
    "try:\n",
    "    training_df = pd.read_csv(TRAINING_DATA_CSV_PATH)\n",
    "    print(f\"Loaded {len(training_df)} labeled training examples.\")\n",
    "    print(\"Training data head:\")\n",
    "    print(training_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Training data CSV not found at {TRAINING_DATA_CSV_PATH}. Skipping training phase.\")\n",
    "    training_df = pd.DataFrame() # Empty DataFrame if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Information Extraction (IE) - Dataset Identification ---\n",
    "\n",
    "# Regex patterns for common dataset identifiers\n",
    "# This list can be expanded based on the types of IDs you expect.\n",
    "# Using raw strings and including common variations for hyphens/dashes.\n",
    "DOI_PATTERN = r'\\b10\\.\\d{4,9}/[-._;()/:A-Za-z0-9\\u002D\\u2010\\u2011\\u2012\\u2013\\u2014\\u2015]+\\b'\n",
    "# Example for GenBank accession numbers (e.g., AB123456, AF000001)\n",
    "GENBANK_PATTERN = r'\\b[A-Z]{1,2}\\d{5,6}\\b'\n",
    "# Example for GEO accession numbers (e.g., GSE12345, GSM12345)\n",
    "GEO_PATTERN = r'\\b(GSE|GSM|GDS|GPL|GSE|GPL)\\d{4,6}\\b'\n",
    "# Example for Dryad DOIs (often follow a specific pattern)\n",
    "DRYAD_DOI_PATTERN = r'\\b10\\.5061/dryad\\.[a-zA-Z0-9]{5,}\\b' # e.g., 10.5061/dryad.2bs69\n",
    "\n",
    "# Combine all patterns into a list\n",
    "DATASET_ID_PATTERNS = [\n",
    "    DOI_PATTERN,\n",
    "    GENBANK_PATTERN,\n",
    "    GEO_PATTERN,\n",
    "    DRYAD_DOI_PATTERN,\n",
    "    # Add more as needed, e.g., for specific repositories like Figshare, Zenodo, etc.\n",
    "]\n",
    "\n",
    "# Compile all patterns for efficiency\n",
    "COMPILED_DATASET_ID_REGEXES = [re.compile(p) for p in DATASET_ID_PATTERNS]\n",
    "\n",
    "# Remove non-matching parentheses first, as they can interfere with DOI matching\n",
    "# (This uses the function you asked for previously)\n",
    "def remove_unmatched_parentheses_local(s: str) -> str:\n",
    "    open_paren_indices_stack = collections.deque()\n",
    "    keep_char = [True] * len(s)\n",
    "    for i, char in enumerate(s):\n",
    "        if char == '(':\n",
    "            open_paren_indices_stack.append(i)\n",
    "        elif char == ')':\n",
    "            if open_paren_indices_stack:\n",
    "                open_paren_indices_stack.pop()\n",
    "            else:\n",
    "                keep_char[i] = False\n",
    "    while open_paren_indices_stack:\n",
    "        unmatched_open_idx = open_paren_indices_stack.pop()\n",
    "        keep_char[unmatched_open_idx] = False\n",
    "    return \"\".join([s[i] for i, should_keep in enumerate(keep_char) if should_keep])\n",
    "\n",
    "def extract_dataset_ids_from_text(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extracts potential dataset identifiers from the given text using a list of regex patterns.\n",
    "    \"\"\"\n",
    "    found_ids = set() # Use a set to store unique IDs\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    for compiled_regex in COMPILED_DATASET_ID_REGEXES:\n",
    "        for match in compiled_regex.finditer(text):\n",
    "            found_ids.add(match.group(0))\n",
    "            \n",
    "    return list(found_ids)\n",
    "\n",
    "# Example of how to use it (will be integrated into main processing loop later)\n",
    "# article_id_example = list(all_article_texts.keys())[0]\n",
    "# text_example = all_article_texts[article_id_example]\n",
    "# extracted_ids = extract_dataset_ids_from_text(text_example)\n",
    "# print(f\"\\nExtracted IDs from '{article_id_example}': {extracted_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8cca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. LLM Model Training (Fine-tuning) ---\n",
    "\n",
    "# Global variables for LLM components\n",
    "llm_tokenizer = None\n",
    "llm_model = None\n",
    "\n",
    "def load_base_llm_for_training():\n",
    "    global llm_tokenizer, llm_model\n",
    "    if not AutoModelForCausalLM or not QWEN_BASE_MODEL_PATH:\n",
    "        print(\"LLM components not available or base model path not set. Skipping LLM loading.\")\n",
    "        return False\n",
    "    try:\n",
    "        print(f\"Loading Qwen tokenizer from: {QWEN_BASE_MODEL_PATH}\")\n",
    "        llm_tokenizer = AutoTokenizer.from_pretrained(QWEN_BASE_MODEL_PATH, trust_remote_code=True)\n",
    "        if llm_tokenizer.pad_token is None:\n",
    "            llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "            print(\"Set tokenizer.pad_token to tokenizer.eos_token\")\n",
    "\n",
    "        print(f\"Loading Qwen model from: {QWEN_BASE_MODEL_PATH}\")\n",
    "        llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "            QWEN_BASE_MODEL_PATH,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32,\n",
    "            device_map=\"auto\", # Automatically uses GPU if available\n",
    "            trust_remote_code=True,\n",
    "            # load_in_8bit=True if bnb else False # Uncomment if bitsandbytes is used\n",
    "        )\n",
    "        print(f\"Base LLM loaded successfully on {llm_model.device}.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading base LLM for training: {e}\")\n",
    "        llm_tokenizer, llm_model = None, None # Reset to None on failure\n",
    "        return False\n",
    "\n",
    "def create_finetuning_prompt_chatml(article_snippet: str, dataset_id: str, label: str) -> str:\n",
    "    \"\"\"\n",
    "    Creates a prompt in Qwen's ChatML format for fine-tuning.\n",
    "    The SFTTrainer will handle masking the prompt part for loss calculation.\n",
    "    \"\"\"\n",
    "    user_message = f\"\"\"\n",
    "Article Context (excerpt):\n",
    "\"{article_snippet}\"\n",
    "\n",
    "Dataset Identifier: \"{dataset_id}\"\n",
    "\n",
    "Question: Based on the provided article context, was the dataset (identified as \"{dataset_id}\"):\n",
    "1. Created by the authors primarily for the research described in THIS article? (If so, it's \"Primary\")\n",
    "2. An existing dataset that the authors obtained and used for their research in THIS article? (If so, it's \"Secondary\")\n",
    "\n",
    "Please respond with only one word: \"Primary\" or \"Secondary\".\n",
    "\"\"\"\n",
    "    # ChatML format: system message, user message, assistant response\n",
    "    return f\"<|im_start|>system\\nYou are an expert research assistant.<|im_end|>\\n<|im_start|>user\\n{user_message.strip()}<|im_end|>\\n<|im_start|>assistant\\n{label}<|im_end|>\"\n",
    "\n",
    "if not training_df.empty and llm_model is None: # Only attempt to load if training data exists and model not loaded\n",
    "    load_base_llm_for_training()\n",
    "\n",
    "if llm_model and not training_df.empty:\n",
    "    print(\"\\n--- Preparing data for Fine-tuning ---\")\n",
    "    formatted_texts = []\n",
    "    for _, row in training_df.iterrows():\n",
    "        article_id = row['article_id']\n",
    "        dataset_id = row['dataset_id']\n",
    "        label = row['label'] # \"Primary\" or \"Secondary\"\n",
    "        \n",
    "        # Get the full article text. Truncate if too long for context window.\n",
    "        # Qwen 1.5 models typically have 32k context, but for training, shorter is faster.\n",
    "        # Adjust max_length based on your model and GPU memory.\n",
    "        article_text = all_article_texts.get(article_id, \"\")\n",
    "        if not article_text:\n",
    "            print(f\"Warning: Article text for {article_id} not found. Skipping training example.\")\n",
    "            continue\n",
    "        \n",
    "        # Truncate article text to fit within context window for training\n",
    "        # A common practice is to take a snippet around the dataset mention if possible,\n",
    "        # but for simplicity here, we'll just take the beginning.\n",
    "        max_context_length = 2048 # Adjust based on model and VRAM\n",
    "        truncated_article_text = article_text[:max_context_length]\n",
    "            \n",
    "        formatted_texts.append({\"text\": create_finetuning_prompt_chatml(truncated_article_text, dataset_id, label)})\n",
    "\n",
    "    if formatted_texts:\n",
    "        from datasets import Dataset\n",
    "        train_dataset = Dataset.from_list(formatted_texts)\n",
    "        print(f\"Prepared {len(train_dataset)} examples for fine-tuning.\")\n",
    "        print(\"Example formatted training instance:\")\n",
    "        print(train_dataset[0]['text'])\n",
    "\n",
    "        print(\"\\n--- Starting Fine-tuning ---\")\n",
    "        try:\n",
    "            training_args = TrainingArguments(\n",
    "                output_dir=f\"{FINE_TUNED_MODEL_OUTPUT_DIR}/checkpoints\",\n",
    "                num_train_epochs=1,  # Start with 1 epoch, adjust as needed\n",
    "                per_device_train_batch_size=1, # Adjust based on VRAM\n",
    "                gradient_accumulation_steps=4, # Effective batch size = 1 * 4 = 4\n",
    "                learning_rate=2e-5,\n",
    "                logging_steps=10,\n",
    "                save_steps=50, # Save checkpoints periodically\n",
    "                fp16=torch.cuda.is_available() and not torch.cuda.is_bf16_supported(),\n",
    "                bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "                optim=\"paged_adamw_8bit\", # Good for memory efficiency if bitsandbytes is installed\n",
    "                # report_to=\"none\", # Disable logging to external services\n",
    "                # max_steps=100, # For quick testing\n",
    "            )\n",
    "\n",
    "            trainer = SFTTrainer(\n",
    "                model=llm_model,\n",
    "                tokenizer=llm_tokenizer,\n",
    "                train_dataset=train_dataset,\n",
    "                dataset_text_field=\"text\",\n",
    "                args=training_args,\n",
    "                max_seq_length=llm_tokenizer.model_max_length, # Use model's max length or a smaller value\n",
    "                packing=False, # Set to True if your inputs are much shorter than max_seq_length\n",
    "            )\n",
    "\n",
    "            trainer.train()\n",
    "            print(\"Fine-tuning completed.\")\n",
    "\n",
    "            # Save the fine-tuned model and tokenizer\n",
    "            print(f\"Saving fine-tuned model to: {FINE_TUNED_MODEL_OUTPUT_DIR}\")\n",
    "            trainer.save_model(FINE_TUNED_MODEL_OUTPUT_DIR)\n",
    "            print(\"Model and tokenizer saved.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during fine-tuning: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            llm_model = None # Mark model as failed to load/train\n",
    "    else:\n",
    "        print(\"No formatted training data available. Skipping fine-tuning.\")\n",
    "else:\n",
    "    print(\"Skipping LLM fine-tuning due to missing training data or LLM components.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bcd4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. LLM-based Classification (Inference) ---\n",
    "\n",
    "# Load the fine-tuned model for inference (if training was successful)\n",
    "# If training was skipped or failed, this will attempt to load from the base path or fail.\n",
    "inference_model = None\n",
    "inference_tokenizer = None\n",
    "\n",
    "# Determine which model path to use for inference\n",
    "if os.path.exists(FINE_TUNED_MODEL_OUTPUT_DIR) and os.path.isdir(FINE_TUNED_MODEL_OUTPUT_DIR):\n",
    "    MODEL_TO_LOAD = FINE_TUNED_MODEL_OUTPUT_DIR\n",
    "    print(f\"Loading fine-tuned model for inference from: {MODEL_TO_LOAD}\")\n",
    "else:\n",
    "    MODEL_TO_LOAD = QWEN_BASE_MODEL_PATH\n",
    "    print(f\"Fine-tuned model not found. Loading base model for inference from: {MODEL_TO_LOAD}\")\n",
    "\n",
    "if AutoModelForCausalLM and MODEL_TO_LOAD:\n",
    "    try:\n",
    "        inference_tokenizer = AutoTokenizer.from_pretrained(MODEL_TO_LOAD, trust_remote_code=True)\n",
    "        if inference_tokenizer.pad_token is None:\n",
    "            inference_tokenizer.pad_token = inference_tokenizer.eos_token\n",
    "        inference_model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_TO_LOAD,\n",
    "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        ).eval() # Set to evaluation mode\n",
    "        print(f\"Inference LLM loaded successfully on {inference_model.device}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading inference LLM from {MODEL_TO_LOAD}: {e}\")\n",
    "        inference_model, inference_tokenizer = None, None\n",
    "\n",
    "def classify_dataset_with_llm(article_text_snippet: str, dataset_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses the loaded LLM to classify dataset usage.\n",
    "    \"\"\"\n",
    "    if not inference_model or not inference_tokenizer:\n",
    "        return \"LLM_Unavailable\"\n",
    "\n",
    "    # Use the same prompt structure as for fine-tuning\n",
    "    user_message = f\"\"\"\n",
    "Article Context (excerpt):\n",
    "\"{article_text_snippet}\"\n",
    "\n",
    "Dataset Identifier: \"{dataset_id}\"\n",
    "\n",
    "Question: Based on the provided article context, was the dataset (identified as \"{dataset_id}\"):\n",
    "1. Created by the authors primarily for the research described in THIS article? (If so, it's \"Primary\")\n",
    "2. An existing dataset that the authors obtained and used for their research in THIS article? (If so, it's \"Secondary\")\n",
    "\n",
    "Please respond with only one word: \"Primary\" or \"Secondary\".\n",
    "\"\"\"\n",
    "    # Qwen ChatML format for inference\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert research assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_message.strip()}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template and tokenize\n",
    "    # `add_generation_prompt=True` adds the <|im_start|>assistant\\n token\n",
    "    input_ids = inference_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(inference_model.device)\n",
    "\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=10, # We expect a short answer\n",
    "                pad_token_id=inference_tokenizer.eos_token_id,\n",
    "                eos_token_id=inference_tokenizer.convert_tokens_to_ids(\"<|im_end|>\") # Stop at assistant end token\n",
    "            )\n",
    "        \n",
    "        # Decode the generated part, skipping the input prompt\n",
    "        response_text = inference_tokenizer.decode(\n",
    "            outputs[0][input_ids.shape[1]:],\n",
    "            skip_special_tokens=False # Keep special tokens to remove <|im_end|> explicitly\n",
    "        ).strip()\n",
    "        \n",
    "        # Clean up the response\n",
    "        response_text = response_text.replace(\"<|im_end|>\", \"\").strip()\n",
    "        \n",
    "        print(f\"  LLM raw response for {dataset_id}: '{response_text}'\")\n",
    "\n",
    "        if \"Primary\" in response_text:\n",
    "            return \"Primary\"\n",
    "        elif \"Secondary\" in response_text:\n",
    "            return \"Secondary\"\n",
    "        else:\n",
    "            print(f\"  Warning: LLM response for {dataset_id} not clear: '{response_text}'\")\n",
    "            return \"Uncertain_LLM\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error during LLM generation for {dataset_id}: {e}\")\n",
    "        return \"Error_LLM_Failed\"\n",
    "\n",
    "# --- Main Processing Loop for all articles ---\n",
    "print(\"\\n--- Starting Article Processing and Classification ---\")\n",
    "final_results = []\n",
    "\n",
    "for article_id, article_text in all_article_texts.items():\n",
    "    print(f\"\\nProcessing article: {article_id}\")\n",
    "    \n",
    "    # 1. Information Extraction (IE)\n",
    "    extracted_dataset_ids = extract_dataset_ids_from_text(article_text)\n",
    "    \n",
    "    if not extracted_dataset_ids:\n",
    "        # 2. Handle \"Missing\" classification\n",
    "        print(f\"  No dataset IDs found for {article_id}. Classifying as 'Missing'.\")\n",
    "        final_results.append({\n",
    "            \"article_id\": article_id,\n",
    "            \"dataset_id\": \"N/A\", # Or \"None\"\n",
    "            \"classification_label\": \"Missing\"\n",
    "        })\n",
    "    else:\n",
    "        print(f\"  Found {len(extracted_dataset_ids)} potential dataset(s): {extracted_dataset_ids}\")\n",
    "        for ds_id in extracted_dataset_ids:\n",
    "            # 3. LLM-based Classification\n",
    "            # Provide a relevant snippet for the LLM.\n",
    "            # For simplicity, using the beginning of the article.\n",
    "            # For better results, you might want to extract sentences/paragraphs\n",
    "            # around where 'ds_id' was mentioned.\n",
    "            llm_context_snippet = article_text[:4000] # Adjust context window size\n",
    "            \n",
    "            classification = classify_dataset_with_llm(llm_context_snippet, ds_id)\n",
    "            \n",
    "            final_results.append({\n",
    "                \"article_id\": article_id,\n",
    "                \"dataset_id\": ds_id,\n",
    "                \"classification_label\": classification\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25441828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Results & Output ---\n",
    "\n",
    "print(\"\\n--- Final Results ---\")\n",
    "if final_results:\n",
    "    results_df = pd.DataFrame(final_results)\n",
    "    print(results_df.head(10)) # Print first 10 rows\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(FINAL_RESULTS_CSV_PATH, index=False)\n",
    "    print(f\"\\nResults saved to: {FINAL_RESULTS_CSV_PATH}\")\n",
    "else:\n",
    "    print(\"No results generated.\")\n",
    "\n",
    "print(\"\\nProcessing complete, Jim!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
